## es单机部署

Centos7阿里yum源

```shell
curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo
curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo
```

 Java环境安装：Jdk1.8

```shell
yum install lrzsz vim -y
yum install java-1.8.0-openjdk java-1.8.0-openjdk-devel -y
java -version
```

 elk下载准备，版本采用7.6.2

```shell
https://www.elastic.co/cn/downloads/elasticsearch
采用rpm安装的方式
```

 注意事项

```shell
所有服务器基础环境都是这样装的
yum源、java环境，后面每台服务器都需要安装基础环境
```

 ES单机版安装部署

```shell
ES的安装：yum -y localinstall elasticsearch-7.6.2-x86_64.rpm
```

 JVM的内存限制更改/etc/elasticsearch/jvm.options，根据服务器内存情况来改

```shell
-Xms200M
-Xmx200M
```

 ES单实例配置/etc/elasticsearch/elasticsearch.yml，single-node代表单机运行

```shell
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
network.host: 0.0.0.0
http.port: 9200
xpack.security.enabled: true
discovery.type: single-node
```

 启动ES

```shell
systemctl enable elasticsearch
systemctl restart elasticsearch
观察日志，检查端口
```

 ES启动后第一步需要设置密码sjgpwd，采用自己设置密码

```shell
ES自己设置密码/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive
ES设置随机密码/usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto
```

 验证启动是否成功

```shell
curl -u elastic:sjgpwd http://xxx:9200
http://xxx:9200/_cat/nodes?v
http://xxx:9200/_cat/indices?v
curl -u elastic:sjgpwd -X POST http://192.168.237.50:9200/sjg/_doc -H 'Content-Type: application/json' -d '{"name": "sjg", "age": 30}'
http://xxx:9200/sjg/_search?q=*
```

## eflk集群部署

### 环境

| 主机名 | IP地址        | 系统版本  | 安装软件                                      | 配置要求 |
| ------ | ------------- | --------- | --------------------------------------------- | -------- |
| master | 192.168.1.190 | centos7.6 | Elasticsearch/zookeeper/kafka/Logstash/kibana | Mem>=4G  |
| node1  | 192.168.1.191 | centos7.6 | Elasticsearch/zookeeper/kafka                 | Mem>=4G  |
| node2  | 192.168.1.192 | centos7.6 | Elasticsearch/zookeeper/kafka                 | Mem>=4G  |
| node3  | 192.168.1.193 | centos7.6 | Filebeat                                      | Mem>=2G  |

### 版本

```text
jdk: 1.8
Elasticsearch: 6.5.4
Logstash: 6.5.4
Kibana: 6.5.4
Kafka: 2.11-1
Filebeat: 6.5.4
相应的版本最好下载对应的插件
```

### 架构

![image-20210221175729618](https://gitee.com/c_honghui/picture/raw/master/img/20210221175729.png)

### 部署

#### Elasticsearch集群部署

节 点 IP：192.168.1.190、192.168.1.191、192.168.1.192

软件版本：jdk-8u211-linux-x64.tar.gz、elasticsearch6.5.4.tar.gz

示例节点：192.168.1.190

1.1、安装配置jdk8

```shell
# tar zxvf /usr/local/src/jdk-8u211-linux-x64.tar.gz
-C /usr/local/
# mv /usr/local/jdk1.8.0_211 /usr/local/java
# echo '
JAVA_HOME=/usr/local/java
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME PATH
' >>/etc/profile.d/java.sh
# source /etc/profile.d/java.sh
# java -version
java version "1.8.0_211"
Java(TM) SE Runtime Environment (build 1.8.0_211-
b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.211-b12,
mixed mode)
```

1.2、安装配置ES

（1）创建运行ES的普通用户

```shell
# useradd ela
# echo "******" | passwd --stdin ela
```

（2）安装配置ES

```shell
# tar zxvf /usr/local/src/elasticsearch-6.5.4.tar.gz -C /usr/local/
# echo '
cluster.name: qf01-elk
node.name: elk01
node.master: true
node.data: true
path.data: /data/elasticsearch/data
path.logs: /data/elasticsearch/logs
bootstrap.memory_lock: true
bootstrap.system_call_filter: false
network.host: 0.0.0.0
http.port: 9200
#discovery.zen.ping.unicast.hosts: ["192.168.1.191","192.168.1.192"]
#discovery.zen.minimum_master_nodes: 2
#discovery.zen.ping_timeout: 150s
#discovery.zen.fd.ping_retries: 10
#client.transport.ping_timeout: 60s
http.cors.enabled: true
http.cors.allow-origin: "*"
' >>/usr/local/elasticsearch6.5.4/config/elasticsearch.yml
```

配置项含义

```text
cluster.name 集群名称，各节点配成相同的集群名称。
node.name 节点名称，各节点配置不同。 [!!各节点不同]
node.master 指示某个节点是否符合成为主节点的条件。
node.data 指示节点是否为数据节点。数据节点包含并管理索引的⼀部分。
path.data 数据存储⽬录。
path.logs ⽇志存储⽬录。
bootstrap.memory_lock 内存锁定，是否禁⽤交换。
bootstrap.system_call_filter 系统调⽤过滤器。
network.host 监听地址。
http.port 端⼝。
discovery.zen.ping.unicast.hosts 提供其他Elasticsearch 服务节点的单点⼴播发现功能。
discovery.zen.minimum_master_nodes 集群中可⼯作的具有Master节点资格的最⼩数量，官⽅的推荐值是(N/2)+1，其中N是具有master资格的节点的数量。
discovery.zen.ping_timeout 节点在发现过程中的等待时间。
discovery.zen.fd.ping_retries 节点发现重试次数。
http.cors.enabled 是否允许跨源 REST 请求，⽤于允许head插件访问ES。
http.cors.allow-origin 允许的源地址。
```

（3）设置JVM堆⼤⼩

```shell
sed -i 's/-Xms1g/-Xms2g/' /usr/local/elasticsearch6.5.4/config/jvm.options
sed -i 's/-Xmx1g/-Xmx2g/' /usr/local/elasticsearch6.5.4/config/jvm.options
注意：
1.确保堆内存最小值（Xms）与最大值（Xmx）的⼤⼩相同，防⽌
程序在运⾏时改变堆内存⼤⼩。
2.如果系统内存⾜够⼤，将堆内存最⼤和最⼩值设置为31G，因为
有⼀个32G性能瓶颈问题。
3.堆内存⼤⼩不要超过系统内存的50%
```

（4）创建ES数据及⽇志存储⽬录

```shell
mkdir -pv /data/elasticsearch/data
mkdir -pv /data/elasticsearch/logs
```

（5）修改安装⽬录及存储⽬录权限

```shell
chown -R ela:ela /data/elasticsearch
chown -R ela:ela /usr/local/elasticsearch-6.5.4
```

1.3、系统优化

（1）增加最⼤⽂件打开数

```shell
echo "* - nofile 65536" >> /etc/security/limits.conf
```

（2）增加最⼤进程数

```shell
echo "* soft nproc 31717" >> /etc/security/limits.conf
```

（3）增加最⼤内存映射数

```shell
echo "vm.max_map_count=262144" >> /etc/sysctl.conf
sysctl -p
```

(4)启动如果报下列错误

```text
memory locking requested for elasticsearch process
but memory is not locked
elasticsearch.yml⽂件
bootstrap.memory_lock : false
/etc/sysctl.conf⽂件
vm.swappiness=0
错误:
max file descriptors [4096] for elasticsearch
process is too low, increase to at least [65536]
意思是elasticsearch⽤户拥有的客串建⽂件描述的权限太低，知道
需要65536个
解决：
切换到root⽤户下⾯，
vim /etc/security/limits.conf
在最后添加
* hard nofile 65536
* hard nofile 65536
重新启动elasticsearch，还是⽆效？
必须重新登录启动elasticsearch的账户才可以，例如我的账户名是
elasticsearch，退出重新登录。
另外*也可以换为启动elasticsearch的账户也可以，* 代表所有，
其实⽐较不合适
启动还会遇到另外⼀个问题，就是
max virtual memory areas vm.max_map_count [65530] is
too low, increase to at least [262144]
意思是：elasticsearch⽤户拥有的内存权限太⼩了，⾄少需要
262114。这个⽐较简单，也不需要重启，直接执⾏
sysctl -w vm.max_map_count=262144
就可以了
```

1.4、启动ES

```shell
su - ela -c "cd /usr/local/elasticsearch-6.5.4 && nohup bin/elasticsearch &"
```

测试：浏览器访问http://192.168.1.190:9200

![image-20210221182745774](https://gitee.com/c_honghui/picture/raw/master/img/20210221182745.png)

查看集群状态：

```shell
curl -X GET ":9200/_cat/health?v"
curl -X GET ":9200/_cat/nodes?v"
```



1.5.安装配置head监控插件

（1）安装node

```shell
# wget https://npm.taobao.org/mirrors/node/latestv4.x/node-v4.4.7-linux-x64.tar.gz
# tar -zxf node-v4.4.7-linux-x64.tar.gz -C /usr/local
# echo '
NODE_HOME=/usr/local/node-v4.4.7-linux-x64
PATH=$NODE_HOME/bin:$PATH
export NODE_HOME PATH
' >>/etc/profile.d/node.sh
# source /etc/profile
# node --version #检查node版本号
 v4.4.7
```

（2）下载head插件

```shell
# wget https://github.com/mobz/elasticsearchhead/archive/master.zip
# unzip -d /usr/local master.zip
```

（3）安装grunt

```shell
# cd /usr/local/elasticsearch-head-master/
# npm install -g grunt-cli
# grunt --version #检查grunt版本号
 grunt-cli v1.3.2
```

（4）修改head源码

```shell
# vi /usr/local/elasticsearch-headmaster/Gruntfile.js （95左右）
connect: {
 server: {
 options: {
 port: 9100,
base: '.',
keepalive: true,
 hostname: '*'
 }
```

修改后图⽚如下:

![](https://gitee.com/c_honghui/picture/raw/master/img/20210221183038.png)

```shell
# vi /usr/local/elasticsearch-headmaster/_site/app.js (4374左右)
app.App = ui.AbstractWidget.extend({
 defaults: {
 base_uri: null
 },
 init: function(parent) {
 this._super();
this.prefs =
services.Preferences.instance();
 this.base_uri =
this.config.base_uri || this.prefs.get("appbase_uri") || "http://192.168.1.190:9200";
```

修改后图⽚如下:

![image-20210221183136813](https://gitee.com/c_honghui/picture/raw/master/img/20210221183136.png)

原本是http://localhost:9200 ，如果head和ES不在同⼀个节点， 注意修改成ES的IP地址,如果在同⼀台机器,可以不修改.

（5）下载head必要的⽂件

```shell
# wget
https://github.com/Medium/phantomjs/releases/download
/v2.1.1/phantomjs-2.1.1-linux-x86_64.tar.bz2
# mkdir /tmp/phantomjs/
# cp /usr/local/src/phantomjs-2.1.1-linuxx86_64.tar.bz2 /tmp/phantomjs/
```

（6）运⾏head

```shell
# cd /usr/local/elasticsearch-head-master/
# yum -y install bzip2
# npm install --
registry=https://registry.npm.taobao.org
# nohup grunt server &
```

（7）测试

访问http://192.168.1.190:9100

![image-20210221183245567](https://gitee.com/c_honghui/picture/raw/master/img/20210221183245.png)

```text
注意
1. Master 与 Data 节点分离，当 Data 节点⼤于 3 个的时候，建议责任分离，减轻压⼒
2. Data Node 内存不超过 32G ，建议设置成 31 G
3. discovery.zen.minimum_master_nodes 设置成 （ total / 2 + 1 ），避免脑裂情况
4. 最重要的⼀点，不要将 ES 暴露在公⽹中，建议都安装 X-PACK，来加强其安全性
```

#### Kibana部署

节 点 IP：192.168.1.190

软件版本：nginx-1.14.2、kibana-6.5.4-linux-x86_64.tar.gz

2.1. 安装配置Kibana

（1）安装

```shell
# tar zxf kibana-6.5.4-linux-x86_64.tar.gz -C /usr/local/
```

（2）配置

```shell
echo '
server.port: 5601
server.host: "192.168.1.190"
elasticsearch.url: "http://192.168.1.190:9200"
kibana.index: ".kibana"
'>>/usr/local/kibana-6.5.4-linuxx86_64/config/kibana.yml
```

配置项含义：

```text
server.port kibana服务端⼝，默认5601
server.host kibana主机IP地址，默认localhost
elasticsearch.url ⽤来做查询的ES节点的URL，默认
http://localhost:9200
kibana.index kibana在Elasticsearch中使⽤索引来存储保存的searches, visualizations和dashboards，默认.kibana
```

（3）启动

```shell
cd /usr/local/kibana-6.5.4-linux-x86_64/
nohup ./bin/kibana &
```

2.2. 安装配置Nginx反向代理

（1）配置YUM源

```shell
# rpm -ivh http://nginx.org/packages/centos/7/noarch/RPMS/nginxrelease-centos-7-0.el7.ngx.noarch.rpm
```

（2）安装

```shell
# yum install -y nginx httpd-tools
注意：httpd-tools⽤于⽣成nginx认证访问的⽤户密码⽂件
```

（3）配置反向代理

```shell
# cat /etc/nginx/nginx.conf
user nginx;
worker_processes 4;
error_log /var/log/nginx/error.log;
pid /var/run/nginx.pid;
worker_rlimit_nofile 65535;
events {
 worker_connections 65535;
 use epoll;
}
http {
 include mime.types;
 default_type application/octet-stream;
 log_format main '$remote_addr - $remote_user
[$time_local] "$request" '
 '$status $body_bytes_sent
"$http_referer" '
 '"$http_user_agent"
"$http_x_forwarded_for"';
 access_log /var/log/nginx/access.log main;
 server_names_hash_bucket_size 128;
 autoindex on;
 sendfile on;
 tcp_nopush on;
 tcp_nodelay on;
 keepalive_timeout 120;
 fastcgi_connect_timeout 300;
 fastcgi_send_timeout 300;
 fastcgi_read_timeout 300;
 fastcgi_buffer_size 64k;
 fastcgi_buffers 4 64k;
 fastcgi_busy_buffers_size 128k;
 fastcgi_temp_file_write_size 128k;
 #gzip模块设置
 gzip on; #开启gzip压缩输出
 gzip_min_length 1k; #最⼩压缩⽂件⼤⼩
 gzip_buffers 4 16k; #压缩缓冲区
 gzip_http_version 1.0; #压缩版本（默认1.1，前端
如果是squid2.5请使⽤1.0）
 gzip_comp_level 2; #压缩等级
 gzip_types text/plain application/x-javascript
text/css application/xml; #压缩类型，默认就已经包含
textml，所以下⾯就不⽤再写了，写上去也不会有问题，但是会有⼀个warn。
gzip_vary on;
 #开启限制IP连接数的时候需要使⽤
 #limit_zone crawler $binary_remote_addr 10m;
 #tips:
 #upstream bakend{#定义负载均衡设备的Ip及设备状态}{
 # ip_hash;
 # server 127.0.0.1:9090 down;
 # server 127.0.0.1:8080 weight=2;
 # server 127.0.0.1:6060;
 # server 127.0.0.1:7070 backup;
 #}
 #在需要使⽤负载均衡的server中增加 proxy_pass
http://bakend/;
 server {
 listen 80;
 server_name 192.168.1.190;
 #charset koi8-r;
 # access_log /var/log/nginx/host.access.log
main;
 access_log off;
 location / {
 auth_basic "Kibana"; #可以是string或
off，任意string表示开启认证，off表示关闭认证。
 auth_basic_user_file
/etc/nginx/passwd.db; #指定存储⽤户名和密码的认证⽂件。
 proxy_pass http://192.168.1.190:5601;
 proxy_set_header Host $host:5601;
 proxy_set_header X-Real-IP
$remote_addr;
 proxy_set_header X-Forwarded-For
$proxy_add_x_forwarded_for;
 proxy_set_header Via "nginx"; 
 }
 location /status {
 stub_status on; #开启⽹站监控状态
 access_log
/var/log/nginx/kibana_status.log; #监控⽇志
 auth_basic "NginxStatus"; }
 location /head/{
 auth_basic "head";
 auth_basic_user_file
/etc/nginx/passwd.db;
 proxy_pass http://192.168.1.190:9100;
 proxy_set_header Host $host:9100;
 proxy_set_header X-Real-IP
$remote_addr;
 proxy_set_header X-Forwarded-For
$proxy_add_x_forwarded_for;
 proxy_set_header Via "nginx";
 }
 # redirect server error pages to the static
page /50x.html
 error_page 500 502 503 504 /50x.html;
 location = /50x.html {
 root html;
 }
 }
}
```

（4）配置授权⽤户和密码

```shell
# htpasswd -cm /etc/nginx/passwd.db username
```

（5）启动nginx

```shell
# systemctl start nginx
```

浏览器访问http://192.168.1.190 刚开始没有任何数据，会提示你 创建新的索引。

![image-20210221215948190](https://gitee.com/c_honghui/picture/raw/master/img/20210221215948.png)

#### Kafka部署

节点IP：192.168.1.190、192.168.1.191、192.168.1.192

软件版本：jdk-8u211-linux-x64.tar.gz、kafka_2.11-2.1.0.tgz

示例节点：192.168.1.190

3.1、安装配置jdk8

（1）Kafka、Zookeeper（简称：ZK）运⾏依赖jdk8

```shell
# tar zxvf /usr/local/src/jdk-8u211-linux-x64.tar.gz
-C /usr/local/
echo '
JAVA_HOME=/usr/local/jdk1.8.0_121
PATH=$JAVA_HOME/bin:$PATH
export JAVA_HOME PATH
' >>/etc/profile
source /etc/profile
```

3.2、安装配置ZK

Kafka运⾏依赖ZK，Kafka官⽹提供的tar包中，已经包含了ZK，这 ⾥不再额下载ZK程序。

（1）安装

```shell
# tar xzvf /usr/local/src/kafka_2.11-2.0.0.tgz -C /usr/local
```

（2）配置

```shell
# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-
2.0.0/config/zookeeper.properties
echo '
dataDir=/opt/data/zookeeper/data
dataLogDir=/opt/data/zookeeper/logs
clientPort=2181
tickTime=2000
initLimit=20
syncLimit=10
server.1=192.168.1.190:2888:3888 //kafka集群IP:Port
server.2=192.168.1.191:2888:3888
server.3=192.168.1.192:2888:3888
'>>/usr/local/kafka_2.11-
2.0.0/config/zookeeper.properties
```

配置项含义

```text
dataDir ZK数据存放⽬录。
dataLogDir ZK⽇志存放⽬录。
clientPort 客户端连接ZK服务的端⼝。
tickTime ZK服务器之间或客户端与服务器之间维持⼼跳的时间间隔。
initLimit 允许follower(相对于Leaderer⾔的“客户端”)连接并同步到Leader的初始化连接时间，以tickTime为单位。当初始化连接时间超过该值，则表示连接失败。
syncLimit Leader与Follower之间发送消息时，请求和应答时间⻓度。如果follower在设置时间内不能与leader通信，那么此follower将会被丢弃。
server.1=172.16.244.31:2888:3888 2888是follower与leader交换信息的端⼝，3888是当leader挂了时⽤来执⾏选举时服务器相互通信的端⼝。
```

创建data、log⽬录

```shell
# mkdir -p /opt/data/zookeeper/{data,logs}
```

创建myid⽂件

```shell
# echo 1 > /opt/data/zookeeper/data/myid #每台kafka机器都要做成唯⼀的ID
```

3.3、配置Kafka

（1）配置

```shell
# sed -i 's/^[^#]/#&/' /usr/local/kafka_2.11-
2.0.0/config/server.properties
echo '
broker.id=1
listeners=PLAINTEXT://192.168.1.190:9092
num.network.threads=3
num.io.threads=8
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600
log.dirs=/opt/data/kafka/logs
num.partitions=6
num.recovery.threads.per.data.dir=1
offsets.topic.replication.factor=2
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1
log.retention.hours=168
log.segment.bytes=536870912
log.retention.check.interval.ms=300000
zookeeper.connect=192.168.1.190:2181,192.168.1.191:2
181,192.168.1.192:2181
zookeeper.connection.timeout.ms=6000
group.initial.rebalance.delay.ms=0
' >>/usr/local/kafka_2.11-2.0.0/config/server.properties
```

配置项含义

```text
broker.id 每个server需要单独配置broker id，如果不配置系
统会⾃动配置。需要和上⼀步ID⼀致
listeners 监听地址，格式PLAINTEXT://IP:端⼝。
num.network.threads 接收和发送⽹络信息的线程数。
num.io.threads 服务器⽤于处理请求的线程数，其中
可能包括磁盘I/O。
socket.send.buffer.bytes 套接字服务器使⽤的发送缓冲区
(SO_SNDBUF)
socket.receive.buffer.bytes 套接字服务器使⽤的接收缓冲区
(SO_RCVBUF)
socket.request.max.bytes 套接字服务器将接受的请求
的最⼤⼤⼩(防⽌OOM)
log.dirs ⽇志⽂件⽬录。
num.partitions partition数量。
num.recovery.threads.per.data.dir 在启动时恢复⽇
志、关闭时刷盘⽇志每个数据⽬录的线程的数量，默认1。
offsets.topic.replication.factor 偏移量话题的复
制因⼦（设置更⾼保证可⽤），为了保证有效的复制，偏移话题的复制
因⼦是可配置的，在偏移话题的第⼀次请求的时候可⽤的broker的数
量⾄少为复制因⼦的⼤⼩，否则要么话题创建失败，要么复制因⼦取可
⽤broker的数量和配置复制因⼦的最⼩值。
log.retention.hours ⽇志⽂件删除之前保留的时间（单位⼩
时），默认168
log.segment.bytes 单个⽇志⽂件的⼤⼩，默认1073741824
log.retention.check.interval.ms 检查⽇志段以查看是否可以
根据保留策略删除它们的时间间隔。
zookeeper.connect ZK主机地址，如果zookeeper是集群则以逗号隔开。
zookeeper.connection.timeout.ms 连接到Zookeeper的超时时间。
```

创建log⽬录

```shell
# mkdir -p /opt/data/kafka/logs
```

3.4、其他节点配置

只需把配置好的安装包直接分发到其他节点，然后修改ZK的 myid，Kafka的broker.id和listeners就可以了。

3.5、启动、验证ZK集群

（1）启动

在三个节点依次执⾏：

```shell
# cd /usr/local/kafka_2.11-2.0.0/
nohup bin/zookeeper-server-start.sh
config/zookeeper.properties &
```

（2）验证

查看ZK配置

```shell
# echo conf | nc 127.0.0.1 2181
clientPort=2181
dataDir=/opt/data/zookeeper/data/version-2
dataLogDir=/opt/data/zookeeper/logs/version-2
tickTime=2000
maxClientCnxns=60
minSessionTimeout=4000
maxSessionTimeout=40000
serverId=1
initLimit=20
syncLimit=10
electionAlg=3
electionPort=3888
quorumPort=2888
peerType=0
```

查看ZK状态

```shell
# echo stat|nc 127.0.0.1 2181
Zookeeper version: 3.4.13-
2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on
06/29/2018 00:39 GMT
Clients:
 /127.0.0.1:51876[0](queued=0,recved=1,sent=0)
Latency min/avg/max: 0/0/0
Received: 2
Sent: 1
Connections: 1
Outstanding: 0
Zxid: 0x0
Mode: follower
Node count: 4
```

查看端⼝

```shell
# lsof -i:2181
COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE
NAME
java 15002 root 98u IPv4 43385 0t0 TCP
*:eforward (LISTEN)
```

3.6、启动、验证Kafka

（1）启动

在三个节点依次执⾏：

```shell
# cd /usr/local/kafka_2.11-2.0.0/
nohup bin/kafka-server-start.sh
config/server.properties &
```

（2）验证

在192.168.1.190上创建topic

```shell
# bin/kafka-topics.sh --create --zookeeper
localhost:2181 --replication-factor 1 --partitions 1
--topic testtopic
Created topic "testtopic".
```

查询192.168.1.190上的topic

```shell
# bin/kafka-topics.sh --zookeeper 192.168.1.190:2181
--list
testtopic
```

查询192.168.1.191上的topic

```shell
# bin/kafka-topics.sh --zookeeper 192.168.1.191:2181
--list
testtopic
```

查询192.168.1.192上的topic

```shell
# bin/kafka-topics.sh --zookeeper 192.168.1.192:2181
--list
testtopic
```

模拟消息⽣产和消费 发送消息到192.168.1.190

```shell
# bin/kafka-console-producer.sh --broker-list
192.168.1.190:9092 --topic testtopic
>Hello World!
```

从192.168.1.191接受消息

```shell
# bin/kafka-console-consumer.sh --bootstrap-server
192.168.1.191:9092 --topic testtopic --from-beginning
Hello World!
```

####  Logstash部署

节点IP：192.168.1.190

软件版本：jdk-8u211-linux-x64.tar.gz、logstash-6.5.4.tar.gz

本次为节省资源，故将Logstash安装 在了kafka190节点

（1）安装

```shell
# tar xzvf /usr/local/src/logstash-6.5.4.tar.gz -C /usr/local/
```

（2）配置

创建⽬录，我们将所有input、filter、output配置⽂件全部放到该 ⽬录中。

```shell
# mkdir -p /usr/local/logstash-6.5.4/etc/conf.d
vi /usr/local/logstash-6.5.4/etc/conf.d/input.conf
input {
kafka {
 type => "nginx_kafka"
 codec => "json"
 topics => "nginx"
 decorate_events => true
 bootstrap_servers => "192.168.1.190:9092,192.168.1.191:9092, 192.168.1.192:9092"
 }
}
# vi /usr/local/logstash6.5.4/etc/conf.d/output.conf
output {
 if [type] == "nginx_kafka" {
 elasticsearch {
 hosts => ["192.168.1.190","192.168.1.191","192.168.1.192"]
 index => 'logstash-nginx-%{+YYYY-MM-dd}'
 }
 }
 }
```

（3）启动

```shell
# cd /usr/local/logstash-6.5.4
nohup bin/logstash -f etc/conf.d/ --
config.reload.automatic &
```

#### Filebeat 部署

Filebeat 需要部署在每台应⽤服务器上，可以通过 Ansible 来推送 并安装配置。

（1）下载

```shell
$ wget https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-6.5.4-linux-x86_64.tar.gz
```

（2）解压

```shell
# tar xzvf filebeat-6.5.4-linux-x86_64.tar.gz -C
/usr/local/
# cd /usr/local
# mv filebeat-6.5.4-linux-x86_64 filebeat
# cd filebeat/
```

（3）修改配置

修改 Filebeat 配置，⽀持收集本地⽬录⽇志，并输出⽇志到 Kafka 集群中

```shell
$ vim fileat.yml
filebeat.prospectors:
- input_type: log
 paths:
 - /opt/logs/server/nginx.log
 json.keys_under_root: true
 json.add_error_key: true
 json.message_key: log
output.kafka:
 hosts: [
"192.168.1.190:9092","192.168.1.191:9092","192.168.1
.192:9092" ]
 topic: 'nginx'
```

Filebeat 6.0 之后⼀些配置参数变动⽐较⼤，⽐如 document_type 就不⽀持，需要⽤ fields 来代替等等。

```shell
#keys_under_root可以让字段位于根节点，默认为false
json.keys_under_root: true
#对于同名的key，覆盖原有key值
json.overwrite_keys: true
#message_key是⽤来合并多⾏json⽇志使⽤的，如果配置该项还需要
配置multiline的设置，后⾯会讲
json.message_key: message
#将解析错误的消息记录储存在error.message字段中
json.add_error_key: true
```

（4）启动

```shell
$ nohup ./filebeat -e -c filebeat.yml &
```

