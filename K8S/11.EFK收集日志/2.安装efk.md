网络上各路大神对 FileBeat 在 K8S 中的部署方式基本上形成了一致的默契，就是`DaemonSet + Sidecar`。

前者用来处理一般的日志收集需求，在 K8S 环境的每个`Node`上都部署一个 FileBeat 的`POD`，给它赋权可以对`namespace`和`POD`有读的权限，应用的部署对其无感知，只要配置好相应的 topic 即可，这种方式对应用日志的采集有一定的局限性，即应用的一个`Pod`里的每个`Container`中只有一个将日志打印到标准输出的进程。因为 FileBeat 本质上是去读`Node`本地`Container`目录中的`json.log`日志，如果有多份标准输出同时打印到一份日志文件里，它们是混杂在一起的，没有可读性的日志采集出来也是没有意义的。

所以如果单个`Container`中有多个需要打印日志的进程，就需要采用`Sidecar`的部署方式，将应用日志输出到不同的日志文件在本地或 PVC 保存，FileBeat 共享日志目录进行获取。由于需要在应用的 POD 中单独启用 FileBeat Container，会带来一定的资源消耗，这样的情况越多整体的资源消耗越大，有时可能需要纳入到整体资源规划的范畴。总体来看这两种方式是互补的关系，基本上可以满足所有应用对于日志的采集需求。



es.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch-logging
  namespace: logging
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Elasticsearch"
spec:
  ports:
  - port: 9200
    protocol: TCP
    targetPort: db
  selector:
    k8s-app: elasticsearch-logging
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: elasticsearch-logging
  namespace: logging
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
rules:
- apiGroups:
  - ""
  resources:
  - "services"
  - "namespaces"
  - "endpoints"
  - "deployments"
  - "daemonsets"
  - "statefulsets"
  - "pods"
  verbs:
  - "get"
  - "watch"
  - "list"
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: logging
  name: elasticsearch-logging
  labels:
    k8s-app: elasticsearch-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
subjects:
- kind: ServiceAccount
  name: elasticsearch-logging
  namespace: logging
  apiGroup: ""
roleRef:
  kind: ClusterRole
  name: elasticsearch-logging
  apiGroup: ""
---
# Elasticsearch deployment itself
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch-logging
  namespace: logging
  labels:
    k8s-app: elasticsearch-logging
    version: 6.5.4
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  serviceName: elasticsearch-logging
  replicas: 2
  selector:
    matchLabels:
      k8s-app: elasticsearch-logging
      version: 6.5.4
  template:
    metadata:
      labels:
        k8s-app: elasticsearch-logging
        version: 6.5.4
        kubernetes.io/cluster-service: "true"
    spec:
      serviceAccountName: elasticsearch-logging
      containers:
      - image: 172.168.200.50/cfss/elasticsearch:6.5.4
        name: elasticsearch-logging
        imagePullPolicy: IfNotPresent
        resources:
          # need more cpu upon initialization, therefore burstable class
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        ports:
        - containerPort: 9200
          name: db
          protocol: TCP
        - containerPort: 9300
          name: transport
          protocol: TCP
        volumeMounts:
        - name: elasticsearch-logging
          mountPath: /usr/share/elasticsearch/data/
        env:
        - name: "NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
      volumes:
      - name: elasticsearch-logging
        #emptyDir: {}
        hostPath:
          path: "/data/efk"
          type: Directory
      # Elasticsearch requires vm.max_map_count to be at least 262144.
      # If your OS already sets up this number to a higher value, feel free
      # to remove this init container.
      initContainers:
      - image: alpine:3.6
        command: ["/sbin/sysctl", "-w", "vm.max_map_count=262144"]
        name: elasticsearch-logging-init
        securityContext:
          privileged: true
```

kibana.yaml

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kibana-logging
  namespace: logging
  labels:
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
    kubernetes.io/name: "Kibana"
spec:
  type: NodePort
  ports:
  - port: 5601
    protocol: TCP
    targetPort: ui
    nodePort: 30205
  selector:
    k8s-app: kibana-logging
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana-logging
  namespace: logging
  labels:
    k8s-app: kibana-logging
    kubernetes.io/cluster-service: "true"
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: kibana-logging
  template:
    metadata:
      labels:
        k8s-app: kibana-logging
      annotations:
        seccomp.security.alpha.kubernetes.io/pod: 'docker/default'
    spec:
      containers:
      - name: kibana-logging
       #image: 172.168.200.50/cfss/kibana:6.5.4
        image: bitnami/kibana:latest
        imagePullPolicy: IfNotPresent
        resources:
          # need more cpu upon initialization, therefore burstable class
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        env:
          - name: ELASTICSEARCH_URL
            value: http://elasticsearch-logging:9200
          - name: SERVER_NAME
            value: kibana-logging
#          - name: SERVER_BASEPATH
#            value: /api/v1/namespaces/kube-system/services/kibana-logging/proxy
 #         - name: SERVER_REWRITEBASEPATH
 #           value: "false"
        ports:
        - containerPort: 5601
          name: ui
          protocol: TCP
#        livenessProbe:
#          httpGet:
#            path: /api/status
#            port: ui
#          initialDelaySeconds: 60
#          timeoutSeconds: 10
#        readinessProbe:
#          httpGet:
#            path: /api/status
#            port: ui
#          initialDelaySeconds: 60
#          timeoutSeconds: 10
```

## 统一集中式

这种方式是默认推荐大家使用的日志采集方式，即FileBeat用`DaemonSet`方式部署（此时 k8s 要同时创建 `ClusterRole`、`ClusterRoleBinding`、`ServiceAccount`来让 FileBeat对`namespace`和 `POD` 有 `get`、`watch`、`list` 的权限）

对于大部分架构简单的容器应用，例如每个 POD 中只有一份日志输出到 `stdout`，选择统一集中式采集日志是最合适也是最简单的。

filebeat.yaml

```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: logging
  labels:
    k8s-app: filebeat
    kubernetes.io/cluster-service: "true"
data:
  filebeat.yml: |-
    filebeat.config:
      prospectors:
        # Mounted `filebeat-prospectors` configmap:
        path: ${path.config}/prospectors.d/*.yml
        # Reload prospectors configs as they change:
        reload.enabled: false
      modules:
        path: ${path.config}/modules.d/*.yml
        # Reload module configs as they change:
        reload.enabled: false
      multiline.pattern: '^\['
      multiline.negate: true
      multiline.match: after
   
    output.elasticsearch:
      hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}']
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-prospectors
  namespace: logging
  labels:
    k8s-app: filebeat
    kubernetes.io/cluster-service: "true"
data:
  kubernetes.yml: |-
    - type: docker
      containers.ids:
      - "*"
      processors:
        - add_kubernetes_metadata:
            in_cluster: true
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: filebeat
  namespace: logging
  labels:
    k8s-app: filebeat
    kubernetes.io/cluster-service: "true"
spec:
  template:
    metadata:
      labels:
        k8s-app: filebeat
        kubernetes.io/cluster-service: "true"
    spec:
      serviceAccountName: filebeat
      terminationGracePeriodSeconds: 30
      containers:
      - name: filebeat
        image: 172.168.200.50/cfss/filebeat:6.5.4
        args: [
          "-c", "/etc/filebeat.yml",
          "-e",
        ]
        env:
        - name: ELASTICSEARCH_HOST
          value: elasticsearch-logging
        - name: ELASTICSEARCH_PORT
          value: "9200"
        securityContext:
          runAsUser: 0
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: config
          mountPath: /etc/filebeat.yml
          readOnly: true
          subPath: filebeat.yml
        - name: prospectors
          mountPath: /usr/share/filebeat/prospectors.d
          readOnly: true
        - name: data
          mountPath: /usr/share/filebeat/data
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      volumes:
      - name: config
        configMap:
          defaultMode: 0600
          name: filebeat-config
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: prospectors
        configMap:
          defaultMode: 0600
          name: filebeat-prospectors
      - name: data
        emptyDir: {}
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: filebeat
subjects:
- kind: ServiceAccount
  name: filebeat
  namespace: logging
roleRef:
  kind: ClusterRole
  name: filebeat
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: filebeat
  labels:
    k8s-app: filebeat
rules:
- apiGroups: [""] # "" indicates the core API group
  resources:
  - namespaces
  - pods
  verbs:
  - get
  - watch
  - list
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: filebeat
  namespace: logging
  labels:
    k8s-app: filebeat
---
```

## Sidecar + PVC 方式

当遇到容器中存在多份日需要同时采集时，就需要用 `Sidecar` 的方式了。同时因为不建议在生产环境中使用 `ReadWriteMany` 的 `PVC`，所以使用 `Sidecar + PVC（ReadWriteOnce）`的方式部署 FileBeat 采集日志时，应用需要使用 `StatefulSet` 方式部署。

与应用合并部署的时候建议将 `input` 部分通过 `ConfigMap` 从配置中拆分出来，打开 Filebeat 的 `Live reloading`，以方便开发同事可以随时调整日志的采集配置，而不必每次因为调整日志采集而重启整个 `POD`。

```yaml
filebeat-inputs(ConfigMap)
apiVersion:v1
kind:ConfigMap
metadata:
  name:filebeat-inputs
  namespace:beats#根据应用实际修改命名空间
data:
  inputs.yml:|
    - type:log#根据需求可以增加多项-type
      paths:
        -/nlog/*log#根据应用实际修改日志目录
      harvester_buffer_size:32768
      close_inactive:3m
      encoding:gb2312#根据应用实际修改字符编码

---
filebeat-config(ConfigMap)
apiVersion:v1
kind:ConfigMap
metadata:
  name:filebeat-config
  namespace:beats#根据应用修改命名空间
data:
  filebeat.yml:|
    logging.level: warning		#FileBeat在调试完毕后可以适当提高日志记录级别
    filebeat.config:
      inputs:
        path:/usr/share/filebeat/inputs.d/*.yml
        enabled:true
        queue.mem.event:128
        reload.enabled:true
        reload.period:10s
    output.kafka:
      enabled:true
      partition.round_robin:
        reachable_only:true
      hosts:["10.0.0.1:9092"]
      worker:5
      topic:beats#根据应用修改topic
```

在应用部署模板的 `Container` 部分添加如下内容：

```yaml
      containers:
      - name:filebeat
        image:10.0.0.2/admin/filebeat:6.4.3
        args:[
          "-c","/etc/filebeat.yml",
          "-e",
        ]
        securityContext:
          runAsUser:0
        resources:
          requests:
            memory:"200Mi"
            cpu:"100m"
          limits:
            memory:"500Mi"
            cpu:"1"
        livenessProbe:
          exec:
            command:
            -/bin/sh
            --c
            -ps-ef|grep"filebeat"|grep-v"grep"
          failureThreshold:3
          initialDelaySeconds:5
          periodSeconds:5
          successThreshold:1
          timeoutSeconds:1
        volumeMounts:
        - name:pvc-default#根据应用实际修改，使用日志目录存放data数据
          mountPath:/usr/share/filebeat/data
        - name:filebeat-config
          mountPath:/etc/filebeat.yml
          readOnly:true
          subPath:filebeat.yml
        - name:filebeat-inputs
          mountPath:/usr/share/filebeat/inputs.d/
          readOnly:true
        - name:pvc-default#根据应用实际修改，为应用存放日志的PVC
          mountPath:/nlog/#根据应用实际修改日志目录
      volumes
      - name:filebeat-config
        configMap:
          defaultMode:0600
          name:filebeat-config
      - name:filebeat-inputs
        configMap:
          defaultMode:0600
          name:filebeat-inputs
```

## 总结

### 本地化 FileBeat 的数据目录

FileBeat 的数据文件默认存放在`/usr/share/filebeat/data/`中，只有`meta.json`和`registry`两个文件，FileBeat 会为每个日志文件启动一个`harvester`来逐行读取日志内容并将他们发送到`output`。`registry`文件中就保存着`harvester`对日志文件的读取状态，例如日志文件的完整路径、日志文件的 inode 号、日志采集位置等等信息。

如果 FileBeat 由于维护重启后没有找到`registry`文件，也没有配置`ignore_older`这样的参数，会将 input 配置中的所有日志重新发送，这会给下游系统带来一定影响和压力。所以无论是 DaemonSet 还是 Sidecar 方式部署的 FileBeat，都建议将 registry 文件保存在 hostPath 或者 PVC 中。

### 根据实际情况调整个别参数

`harvester_buffer_size` 是单次读取日志 `harvester` 可以用的缓存大小，在 FileBeat 中默认的配置是 `16384 bytes`，在绝大多数情况下 16KB 是够用的。

但是在实际环境发生过由于应用的日志量比较大，每秒的新增日志量大概 1w+，发现 FileBeat 进程的 CPU 使用率很高，调整 `harvester_buffer_size` 可以让 FileBeat 在 CPU 和内存使用量之间取得一定的平衡效果，降低系统影响。

`queue_mem_event` 的默认值是 4096，`max_bytes`（单条日志大小上限，超出部分丢弃）的默认值是 10MB，所以极限情况下 FileBeat 默认可以占用系统内存的大小是 `queue_mem_event * max_bytes = 40GB`，这给操作系统带来的影响简直是灾难性的。但是在容器中我们一般都会配置 `resource limit`，所以容器中部署 FileBeat 有关内存使用的部分配置，要结合 resource limit 的情况整体考量以达到一个平衡的最佳效果。

`close_renamed`，`close_timeout` 等等一些与 `harvester` 关闭有关的参数可能会导致日志漏传，这些参数的说明在官网都有非常详细的描述，需要在明确自身需求后再确认是否需要开启。

### 旧版本 FileBeat 对接 KAFKA 默认版本不同

FileBeat 版本更迭速度比较快，从 6.4 版本之后默认的 KAFKA 版本是 1.0.0，FileBeat 对接 KAFKA 默认版本的不同可能会对 FileBeat 运行造成一些影响，部署的时候还需要留心注意一下环境中的 KAFKA 版本号。