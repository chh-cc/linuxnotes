日志是定位问题的重要手段

监控、日志：企业必备



日志级别通常有四种，从高到低：ERROR、WARNING、INFO、DEBUG

日志级别中的优先级是什么意思？在你的系统中如果开启了某一级别的日志后，就不会打印比它级别低的日志。例如，程序如果开启了INFO级别日志，DEBUG日志就不会打印，通常在生产环境中开启INFO日志。

1、DEBUG：DEBU可以打印出最详细的日志信息，主要用于开发过程中打印一些运行信息。

2、INFO：INFO可以打印一些你感兴趣的或者重要的信息，这个可以用于生产环境中输出程序运行的一些重要信息，但是不能滥用，避免打印过多的日志。

3、WARNING：WARNING表明发生了一些暂时不影响运行的错误，会出现潜在错误的情形，有些信息不是错误信息，但是也要给程序员的一些提示

4、ERROR：ERROR可以打印错误和异常信息，如果不想输出太多的日志，可以使用这个级别，这一级就是比较重要的错误了，软件的某些功能已经不能继续执行了。

日志所面对的主体对象是软件开发人员(包括测试测试、维护人员)。



日志采集方案：

filebeat负责**采集数据**，发送给logstash负责**聚合、过滤**，再发送给elasticsearch**存储**，kibana接入elasticsearch**展示UI**界面

如果规模比较大，可以考虑在filebeat和logstash中间加一个消息队列如kafka

## filebeat

filebeat监视你指定的日志文件或位置，收集日志事件，并将它们转发到es或logstash



工作方式：

启动一个或多个输入，这些输入将在为日志数据指定的位置中查找，filebeat会启动收集器，每个收集器读取单个日志以获取新内容，并将日志数据发送到libeat，libeat将聚集事件，并发送到为filebeat配置的输出



两个主要组件：

harvester：一个harvester负责读取一个单个文件的内容。harvester逐行读取每个文件，并把这些内容发送到输出。每个文件启动一个harvester。

Input：一个input负责管理harvesters，并找到所有要读取的源。如果input类型是log，则input查找驱动器上与已定义的log日志路径匹配的所有文件，并为每个文件启动一个harvester。



传输方案：

1、output.elasticsearch

如果你希望使用filebeat直接向elasticsearch输出数据，需要配置output.elasticsearch

```yaml
output.elasticsearch:
  hosts: ["192.168.40.180:9200"]
```

2、output.logstash

如果使用filebeat向logstash输出数据，然后由logstash再向elasticsearch输出数据，需要配置output.logstash。logstash和filebeat一起工作时，如果logstash忙于处理数据，会通知FileBeat放慢读取速度。一旦拥塞得到解决，FileBeat将恢复到原来的速度并继续传播。这样，可以减少管道超负荷的情况。

```yaml
output.logstash:
  hosts: ["192.168.40.180:5044"]
```

3、output.kafka

如果使用filebeat向kafka输出数据，然后由logstash作为消费者拉取kafka中的日志，并再向elasticsearch输出数据，需要配置output.kafka

```yaml
output.kafka:
  enabled: true
  hosts: ["192.168.40.180:9092"]
  topic: elfk8stest
```

## logstash

Logstash主要对日志进行**过滤处理**，也能用来做日志收集。**但日志采集一般不用**logstash



三个组件：

input：输入，采集各种来源的数据

filter：过滤，可选，将日志格式化。有丰富的过滤插件：Grok正则捕获、Date时间处理、Json编解码、Mutate数据修改等。 

output：输出，选择存储，导出数据



常见input模块：

file：从文件系统上的文件读取

syslog：在众所周知的端口514上侦听系统日志消息，并根据RFC3164格式进行解析

redis：从redis服务器读取，使用redis通道和redis列表。

beats：处理由Filebeat发送的事件。

常见的filter模块：

grok：解析和结构任意文本。Grok目前是Logstash中将非结构化日志数据解析为结构化和可查询的最佳方法。

mutate：对事件字段执行一般转换。可以重命名，删除，替换和修改事件中的字段。

drop：完全放弃一个事件，例如调试事件。

clone：完全放弃一个事件，例如调试事件。

geoip：添加有关IP地址的地理位置的信息

常见的output模块：

elasticsearch：将事件数据发送给Elasticsearch（推荐模式）。

file：将事件数据写入文件或磁盘。

graphite：将事件数据发送给graphite（一个流行的开源工具，存储和绘制指标，http://graphite.readthedocs.io/en/latest/）。

statsd：将事件数据发送到statsd（这是一种侦听统计数据的服务，如计数器和定时器，通过UDP发送并将聚合发送到一个或多个可插入的后端服务）。