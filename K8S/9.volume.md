# Volume

https://kubernetes.io/docs/concepts/storage/volumes/

pod是有生命周期的，重启它的文件会丢失、一个pod中运行多个容器可能需要共享一些文件、不同pod共享文件、日志收集

存储卷类型：

-    emptyDIR存储卷
-    hostPath存储卷 
-    SAN（iscsi）或NAS（nfs、cifs）： 网络存储设备
-    分布式存储（ceph,glusterfs,cephfs） 
-    云存储（亚马逊的EBS，Azure Disk，阿里云）： 这种是网络存储的，一般只有k8s在云上部署才会用到。

## hostpath

不推荐使用，将节点上的文件或目录挂载到pod上

## emptyDir

emptyDir 是最基础的 Volume 类型。正如其名字所示，一个 emptyDir Volume 是 Host 上的一个空目录。

如果删除pod，emptydir卷中的数据也会被删除，一般用于pod中不同容器**临时共享数据**

示例：

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache #挂载到容器的/cache目录
      name: cache-volume
  volumes:
  - name: cache-volume #定义了一个emptyDir卷
    emptyDir: {}
```

## PV&PVC

PersistentVolume (PV)是外部存储系统中的一块存储空间，它是对底层共享存储的抽象，将共享存储作为用户可申请使用的资源。与 Volume 一样，PV 具有持久性，生命周期独立于 Pod，它是集群中的资源

PersistentVolumeClaim (PVC) 是对PV的申请，它向PV申请特定大小的空间及访问模式（如rw或ro）

PV没有namespace概念，PVC有namespace隔离

![img](https://gitee.com/c_honghui/picture/raw/master/img/20220122111121.png)

### 静态PV

nfs搭建

```
yum install nfs-utils
 
vim /etc/exports
/data/k8s/ 172.16.1.0/24(sync,rw,no_root_squash)
 
systemctl start nfs; systemctl start rpcbind 
systemctl enable nfs

测试：
yum install nfs-utils
showmount -e 172.16.1.131
```

创建PV：

```yaml
vim nfs-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 #pv名称
spec:
  capacity:
    storage: 2Gi #pv容量
  volumeMode: Filesystem #挂载类型
  accessModes:
    - ReadWriteMany #访问模式
  persistentVolumeReclaimPolicy: Retain #回收策略
  storageClassName: nfs-slow #类名
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp #存储路径
    server: 172.17.0.2 #nfs地址
    
kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv0001   2Gi        RWX            Retain           Available           nfs-slow                4s

#PV状态：
#Available：空闲的pv，没有被任何pvc绑定
#Bound：已经被pvc绑定
#Released：PVC被删除，但是资源未被重新使用
#Failed：自动回收失败
```

回收策略`persistentVolumeReclaimPolicy`：

- Recycle：删掉PVC，会rm -rf /thevolume/*将**PV里的内容删除**
- Retain：保留，删掉PVC后PV由Bound状态变为Released，此状态下的PV不能和别的PVC绑定，**删除PV后PV存储的内容还在**，可以重新创建PV和PVC绑定
- Delete：删除PVC后，对应的PV和存储内容也会被删掉

访问模式`accessModes`：

- ReadWriteOnce（RWO），该卷可以被单个节点以读写方式挂载 
- ReadOnlyMany（ROX），该卷可以被许多节点以只读方式挂载
- ReadWriteMany（RWX），该卷可以被多个节点以读写方式挂载

PV类名`storageClassName`：**PVC和PV的类名一样，才能被绑定**

创建PVC：

```yaml
vim nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim #pvc名称
# namespace: 
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-slow
  
kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myclaim   Bound    pv0001   2Gi        RWX            nfs-slow       4s
```

把pvc挂载到pod

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /tmp/pvc #把pv卷挂载到容器的该目录
      name: pvc-test #pv卷名称
  volumes:
  - name: pvc-test #卷名称
    persistentVolumeClaim: #申明使用静态PVC永久化存储
      claimName: myclaim #pvc名称
```

创建PVC后一直绑定不上PV（pending）的原因：

- PVC的空间申请大小大于PV的容量
- PVC的StorageClassName和PV不一致
- PVC的accessModes和PV不一致

创建挂载了PVC的Pod后，一直处于pending状态的原因：

- PVC没有创建成功
- PVC和Pod不在同一个namespace

### 动态PV

尽管PVC使得用户可以以抽象的方式访问存储资源，但很多时候还是会涉及PV的不少属性，例如，由于不同场景时设置的性能参数等。为此，集群管理员不得不通过多种方式提供多种不同的PV以满不同用户不同的使用需求，两者衔接上的偏差必然会导致用户的需求无法全部及时有效地得到满足。Kubernetes从1.4版起引入了一个新的资源对象StorageClass，可用于**将存储资源定义为具有显著特性的类**（Class）而不是具体的PV，例如“fast”“slow”或“glod”“silver”“bronze”等。**用户通过PVC直接向意向的类别发出申请**，匹配由管理员事先创建的PV，或者由其按需为用户动态创建PV，这样做甚至**免去了需要先创建PV的过程**。
PV对存储系统的支持可通过其插件来实现，目前，Kubernetes支持如下类型的插件：

https://kubernetes.io/docs/concepts/storage/storage-classes/

定义一个storage

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false"
```

因为storage自动创建pv需要经过kube-apiserver,所以要进行授权

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

创建nfs相关存储服务

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.101.144
            - name: NFS_PATH
              value: /data/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.101.144
            path: /data/k8s
```

查看创建的storageclass

```shell
kubectl get storageclass
NAME                  PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  2m3s
```

查看创建的nfs

```shell
kubectl get po
NAME                                      READY   STATUS             RESTARTS   AGE
nfs-client-provisioner-8466c647bb-5mwf6   0/1     ImagePullBackOff   0          2m8s
```

部署nginx使用动态PV

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  clusterIP: None
  selector:
    app: nginx
  ports:
  - name: web
    port: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.2
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-html
      volumes:
      - name: nginx-html
        persistentVolumeClaim:
          claimName: nginx-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: managed-nfs-storage
  resources:
    requests:
      storage: 1Gi
```
