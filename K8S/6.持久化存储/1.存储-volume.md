# 持久化存储

https://kubernetes.io/docs/concepts/storage/volumes/

## 为什么要持久化存储

因为pod是有生命周期的，如果pod不挂载数据卷，那么**pod删除或重启后数据会丢失**。

pod中的pause容器会挂载绑定volume，pod中的容器会共享pause的volume，容器重启后volume还在，**pod不存在时volume也不存在**。

## 数据卷（volume）使用规则

- 一个Pod可以挂载多个数据卷（Volume）。
- 一个Pod可以挂载多种类型的数据卷（Volume）。
- 每个被Pod挂载的Volume卷，可以在不同的容器间共享。
- Kubernetes环境推荐使用PVC和PV方式挂载数据卷（Volume）。
- 虽然单Pod可以挂载多个数据卷（Volume），但是并不建议给一个Pod挂载过多数据卷。

## 本地存储

### hostpath

hostPath Volume是指**Pod挂载宿主机上的目录或文件**。 hostPath Volume使得容器可以使用宿主机的文件系统进行存储，hostpath（宿主机路径）：节点级别的存储卷，在pod被删除，这个存储卷还是存在的，不会被删除，所以只要同一个pod被调度到同一个节点上来，在pod被删除重新被调度到这个节点之后，对应的数据依然是存在的。

### emptyDir

emptyDir类型的Volume是在Pod分配到Node上时被创建，Kubernetes会在Node上自动分配一个目录，因此无需指定宿主机Node上对应的目录文件。 这个目录的初始内容为空，**如果删除pod，emptyDir中的数据会被永久删除。**emptyDir Volume主要用于某些应用程序无需永久保存的临时目录，多个容器的共享目录等。



容器崩溃不会从节点移除pod，因此emptydir卷的数据在容器崩溃时是安全的

示例：

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache #挂载到容器的/cache目录
      name: cache-volume
  volumes:
  - name: cache-volume #定义了一个emptyDir卷
    emptyDir: {}
```



查看本机临时目录的位置：

```shell
kubectl get po xxx -oyaml |grep uid

tree /var/lib/kubelet/pods/<uid>

临时目录在本地的/var/lib/kubelet/pods/<uid>/volumes/kubernetes.io~empty-dir/cache-volume/下
```



## PV&PVC

上面说的数据卷没有持久化特征，为此Kubernetes引入了PV和PVC两个资源对象



PV：

PersistentVolume (PV) ，持久化存储卷，代表一个具体存储类型的卷，其对象中定义了具体存储类型和卷参数，**PV是一个集群级别的概念，生命周期独立于pod**

可以通过两种方式配置PV：静态或动态

PVC：

PersistentVolumeClaim (PVC) ，存储声明，PVC只需要申明自己需要的存储size、access mode等业务真正需要关心的需求，不用关心存储细节

**PV没有namespace概念，PVC有namespace隔离**



**pv与pvc使用说明**

- PVC和PV的绑定

  PVC与PV是**一一对应关系**，不能一个PVC挂载多个PV，也不能一个PV挂载多个PVC。为应用配置存储时，需要声明一个存储需求声明（PVC），而Kubernetes会通过最佳匹配的方式选择一个满足PVC需求的PV，并与之绑定。所以从职责上PVC是应用所需要的存储对象，属于应用作用域。PV是存储平面的存储对象，属于整个存储域。

  PVC只有绑定了PV之后才能被Pod使用，而PVC绑定PV的过程即是消费PV的过程，这个过程是有一定规则的，以下规则都满足的PV才能被PVC绑定：

  - VolumeMode：被消费PV的VolumeMode需要和PVC一致。
  - AccessMode：被消费PV的AccessMode需要和PVC一致。
  - StorageClassName：如果PVC定义了此参数，PV必须有相关的参数定义才能进行绑定。
  - LabelSelector：通过标签（**labels**）匹配的方式从PV列表中选择合适的PV绑定。
  - Size：被消费PV的**capacity**必须大于或者等于PVC的存储容量需求才能被绑定。

- PV和PVC定义中的**size**字段

  PVC和PV里面的**size**字段作用如下：

  - PVC、PV绑定时，会根据各自的**size**进行筛选。
  - 通过PVC、StorageClass动态创建PV时，有些存储类型会参考PVC的**size**创建相应大小的PV和后端存储。
  - 对于支持Resize操作的存储类型，PVC的**size**作为扩容后PV、后端存储的容量值。

  一个PVC、PV的**size**值只是在执行一些PVC和PV管控操作的时候，作为配置参数来使用。

  真正的存储卷数据流写数据的时候，不会参考PVC和PV的**size**字段，而是依赖底层存储介质的实际容量。







回收策略：

当删除pod时，pvc和pv的绑定就会解除，解除后pv卷里的数据要怎么处理？

- Recycle：删掉PVC，会rm -rf /thevolume/*将**PV里的内容删除**
- **Retain**：保留，删掉PVC后PV由Bound状态变为Released，此状态下的PV不能和别的PVC绑定，**删除PV后PV存储的内容还在**，可以重新创建PV和PVC绑定
- Delete：删除PVC后，对应的PV和存储内容也会被删掉

### 静态PV

静态存储卷一般是指由管理员创建的PV。所有的数据卷（Volume）都支持创建静态存储卷。



nfs搭建

```
yum install nfs-utils
 
vim /etc/exports
/data/k8s/ 172.16.1.0/24(sync,rw,no_root_squash)
 
systemctl start nfs; systemctl start rpcbind 
systemctl enable nfs

测试：
yum install nfs-utils
showmount -e 172.16.1.131
```

手动创建PV：

```yaml
vim nfs-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 #pv名称
spec:
  capacity:
    storage: 2Gi #pv容量
  volumeMode: Filesystem #挂载类型
  accessModes:
    - ReadWriteMany #访问模式
  persistentVolumeReclaimPolicy: Retain #回收策略
  storageClassName: nfs-slow #类名
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp #存储路径
    server: 172.17.0.2 #nfs地址
    
kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv0001   2Gi        RWX            Retain           Available           nfs-slow                4s

#PV状态：
#Available：空闲的pv，没有被任何pvc绑定
#Bound：已经被pvc绑定
#Released：PVC被删除，但是资源未被重新使用
#Failed：自动回收失败
```



**访问模式**`accessModes`：

- ReadWriteOnce（RWO），该卷可以被单个节点以读写方式挂载 
- ReadOnlyMany（ROX），该卷可以被许多节点以只读方式挂载
- ReadWriteMany（RWX），该卷可以被多个节点以读写方式挂载

一个卷一次只能使用一种访问模式挂载，即使它支持多种访问模式



创建PVC：

```yaml
vim nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim #pvc名称
# namespace: 
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-slow #PVC和PV的类名一样，才能被绑定
  
kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myclaim   Bound    pv0001   2Gi        RWX            nfs-slow       4s
```

把pvc挂载到pod

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /tmp/pvc #把pv卷挂载到容器的该目录
      name: pvc-test
  volumes:
  - name: pvc-test #卷名称
    persistentVolumeClaim: #申明使用静态PVC永久化存储
      claimName: myclaim #pvc名称
```



创建PVC后一直绑定不上PV（pending）的原因：

- PVC的空间申请大小大于PV的容量
- PVC的StorageClassName和PV不一致
- PVC的accessModes和PV不一致

创建挂载了PVC的Pod后，一直处于pending状态的原因：

- PVC没有创建成功
- PVC和Pod不在同一个namespace

### 动态PV

动态存储卷一般是指通过存储插件自动创建的PV

动态卷一般由集群管理员配置好后端的存储池，并创建相应的模板（StorageClass），当有PVC需要消费PV的时候，根据PVC定义的需求，并参考StorageClass的存储细节，由存储插件动态创建一个PV。

**StorageClass 对象的作用，其实就是自动创建 PV 的模板**。StorageClass定义如下：

当您声明一个PVC时，如果在PVC中添加了**StorageClassName**字段，意味着当PVC在集群中找不到匹配的PV时，会根据**StorageClassName**的定义触发相应的Provisioner插件创建合适的PV供绑定，即创建动态数据卷。



定义一个storage

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
```

provisioner：供应商，storageclass需要一个供应者，用来确定我们使用什么样的存储来创建pv，常见的provisioner如下：

https://kubernetes.io/docs/concepts/storage/storage-classes/

provisioner既可以由内部供应商提供，也可以由外部供应商提供，如果是外部供应商可以参考https://github.com/kubernetes-incubator/external-storage/下提供的方法创建。

**因为storage自动创建pv需要经过kube-apiserver,所以要进行授权**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

创建nfs相关存储服务

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.101.144
            - name: NFS_PATH
              value: /data/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.101.144
            path: /data/k8s
```

查看创建的storageclass

```shell
kubectl get storageclass
NAME                  PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  2m3s
```

查看创建的nfs

```shell
kubectl get po
NAME                                      READY   STATUS             RESTARTS   AGE
nfs-client-provisioner-8466c647bb-5mwf6   0/1     ImagePullBackOff   0          2m8s
```

部署nginx使用动态PV

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  clusterIP: None
  selector:
    app: nginx
  ports:
  - name: web
    port: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.2
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-html
      volumes:
      - name: nginx-html
        persistentVolumeClaim:
          claimName: nginx-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: managed-nfs-storage
  resources:
    requests:
      storage: 1Gi
```
