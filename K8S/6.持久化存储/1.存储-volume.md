# 持久化存储

https://kubernetes.io/docs/concepts/storage/volumes/

## 为什么要持久化存储

在k8s中部署的应用都是以pod容器形式运行，如果部署mysql、redis等数据库，需要对这些数据库产生的数据做备份，因为pod是有生命周期的，如果pod不挂载数据卷，那么**pod删除或重启后数据会丢失**。







pod中的pause容器会挂载绑定volume，pod中的容器会共享pause的volume，容器重启后volume还在，**pod不存在时volume也不存在**。

<img src="D:%5Clinuxnotes%5CK8S%5C9.%E5%AD%98%E5%82%A8-volume.assets%5Cimage-20220605010447858.png" alt="image-20220605010447858" style="zoom:67%;" />



使用存储卷步骤：

- 定义pod的volume，这个volume指定它要关联到哪个存储上
- 在容器中要使用volumemounts挂载对应的存储

## hostpath

hostPath Volume是指**Pod挂载宿主机上的目录或文件**。 hostPath Volume使得容器可以使用宿主机的文件系统进行存储，hostpath（宿主机路径）：节点级别的存储卷，在pod被删除，这个存储卷还是存在的，不会被删除，所以只要同一个pod被调度到同一个节点上来，在pod被删除重新被调度到这个节点之后，对应的数据依然是存在的。

## emptyDir

emptyDir类型的Volume是在Pod分配到Node上时被创建，Kubernetes会在Node上自动分配一个目录，因此无需指定宿主机Node上对应的目录文件。 这个目录的初始内容为空，**如果删除pod，emptyDir中的数据会被永久删除。**emptyDir Volume主要用于某些应用程序无需永久保存的临时目录，多个容器的共享目录等。



容器崩溃不会从节点移除pod，因此emptydir卷的数据在容器崩溃时是安全的

示例：

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache #挂载到容器的/cache目录
      name: cache-volume
  volumes:
  - name: cache-volume #定义了一个emptyDir卷
    emptyDir: {}
```



查看本机临时目录的位置：

```shell
kubectl get po xxx -oyaml |grep uid

tree /var/lib/kubelet/pods/<uid>

临时目录在本地的/var/lib/kubelet/pods/<uid>/volumes/kubernetes.io~empty-dir/cache-volume/下
```



## PV&PVC

PV：

PersistentVolume (PV) 是集群中的一块存储，**生命周期独立于 Pod**，它是集群中的资源。是管理员设置的**外部存储系统中的一块存储空间**，它是**对底层共享存储的抽象**，PV和其对应的后端存储信息交给集群管理员运维和管理

可以通过两种方式配置PV：静态或动态

PVC：

PersistentVolumeClaim (PVC) 是一个持久化存储卷，PVC只需要申明自己需要的存储size、access mode等业务真正需要关心的需求，不用关心存储细节

**PV没有namespace概念，PVC有namespace隔离**



绑定：

创建pvc并指定需要的资源和访问模式，在找到可用的pv之前，pvc会保持未绑定状态。

**当pvc和pv绑定后，pod就可以在volume字段声明pvc，然后pvc对应的pv就会挂载到pod容器内的目录上**

![image-20220605011226538](D:%5Clinuxnotes%5CK8S%5C9.%E5%AD%98%E5%82%A8-volume.assets%5Cimage-20220605011226538.png)



使用：

- 找一个存储服务器，划分成多个存储空间
- k8s管理员把这些存储空间定义成多个pv
- 创建pvc，通过定义需要使用pv的大小和访问模式来找到合适的pv
- pvc被创建后就可以当成存储卷来使用，创建pod时就可以使用这个pvc的存储卷



回收策略：

当删除pod时，pvc和pv的绑定就会解除，解除后pv卷里的数据要怎么处理？

- Recycle：删掉PVC，会rm -rf /thevolume/*将**PV里的内容删除**
- **Retain**：保留，删掉PVC后PV由Bound状态变为Released，此状态下的PV不能和别的PVC绑定，**删除PV后PV存储的内容还在**，可以重新创建PV和PVC绑定
- Delete：删除PVC后，对应的PV和存储内容也会被删掉

### 静态PV

集群管理员创建许多PV，它们包含了要使用的实际存储的具体信息。



nfs搭建

```
yum install nfs-utils
 
vim /etc/exports
/data/k8s/ 172.16.1.0/24(sync,rw,no_root_squash)
 
systemctl start nfs; systemctl start rpcbind 
systemctl enable nfs

测试：
yum install nfs-utils
showmount -e 172.16.1.131
```

手动创建PV：

```yaml
vim nfs-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 #pv名称
spec:
  capacity:
    storage: 2Gi #pv容量
  volumeMode: Filesystem #挂载类型
  accessModes:
    - ReadWriteMany #访问模式
  persistentVolumeReclaimPolicy: Retain #回收策略
  storageClassName: nfs-slow #类名
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp #存储路径
    server: 172.17.0.2 #nfs地址
    
kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv0001   2Gi        RWX            Retain           Available           nfs-slow                4s

#PV状态：
#Available：空闲的pv，没有被任何pvc绑定
#Bound：已经被pvc绑定
#Released：PVC被删除，但是资源未被重新使用
#Failed：自动回收失败
```



**访问模式**`accessModes`：

- ReadWriteOnce（RWO），该卷可以被单个节点以读写方式挂载 
- ReadOnlyMany（ROX），该卷可以被许多节点以只读方式挂载
- ReadWriteMany（RWX），该卷可以被多个节点以读写方式挂载

一个卷一次只能使用一种访问模式挂载，即使它支持多种访问模式



创建PVC：

```yaml
vim nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim #pvc名称
# namespace: 
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-slow #PVC和PV的类名一样，才能被绑定
  
kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myclaim   Bound    pv0001   2Gi        RWX            nfs-slow       4s
```

把pvc挂载到pod

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /tmp/pvc #把pv卷挂载到容器的该目录
      name: pvc-test
  volumes:
  - name: pvc-test #卷名称
    persistentVolumeClaim: #申明使用静态PVC永久化存储
      claimName: myclaim #pvc名称
```



创建PVC后一直绑定不上PV（pending）的原因：

- PVC的空间申请大小大于PV的容量
- PVC的StorageClassName和PV不一致
- PVC的accessModes和PV不一致

创建挂载了PVC的Pod后，一直处于pending状态的原因：

- PVC没有创建成功
- PVC和Pod不在同一个namespace

### 动态PV

当管理员创建的静态PV都不匹配PersistentVolumeClaim时，集群就会尝试为PVC动态的配置卷。



前面介绍的pv和pvc模式都是需要先创建好pv，然后定义好pvc和pv进行一对一的bond，如果一个大规模集群可能有成千上万个PVC，那意味着运维要创建成千上万个PV，而 **StorageClass 对象的作用，其实就是自动创建 PV 的模板**。



StorageClass 对象会定义如下两个部分内容：

- 第一，PV 的属性。比如，存储类型、Volume 的大小等等。
- 第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。

有这两个信息后，k8s就能根据用户提交的PVC，找到对应的StorageClass，然后k8s调用该StorageClass 声明的存储插件，创建需要的PV。



k8s存储架构：

![image-20220605011416333](D:%5Clinuxnotes%5CK8S%5C9.%E5%AD%98%E5%82%A8-volume.assets%5Cimage-20220605011416333.png)

- PV Controller：负责PV和PVC的绑定、生命周期管理，并根据需求进行数据卷的Provision/Delete操作
- AD Controller：负责数据卷的Attach/Detach操作，将数据卷挂载/卸载到目标节点
- Volume Manager：管理卷的Mount/Unmount操作、卷设备的格式化
- Volume Plugins：扩展各种存储类型的卷管理能力，实现第三方存储的各种操作能力与k8s存储系统结合



k8s挂载volume过程：

![image-20220605011443868](D:%5Clinuxnotes%5CK8S%5C9.%E5%AD%98%E5%82%A8-volume.assets%5Cimage-20220605011443868.png)

1. 用户创建了一个包含PVC的Pod（使用动态存储卷）
2. PV Controller会尝试找一个合适的PV和PVC进行绑定，找不到合适的PV就调用Volume Plugin创建存储卷，并创建PV对象，并将PV绑定到PVC
3. Sechduler根据Pod配置、节点状态、PV配置等，把Pod调度到节点
4. AD Controller发现Pod和PVC都处于待挂载状态，调用Volume Plugin实现设备挂载到目标节点
5. 节点的kubelet（Volume Manager）等设备挂载完，通过Volume Plugin将设备挂载到Pod的指定目录
6. kubelet得知挂载完毕后启动pod中的containers，用docekr -v将已经挂载到本地的卷映射到容器中



定义一个storage

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner
parameters:
  archiveOnDelete: "false"
```

provisioner：供应商，storageclass需要一个供应者，用来确定我们使用什么样的存储来创建pv，常见的provisioner如下：

https://kubernetes.io/docs/concepts/storage/storage-classes/

provisioner既可以由内部供应商提供，也可以由外部供应商提供，如果是外部供应商可以参考https://github.com/kubernetes-incubator/external-storage/下提供的方法创建。

**因为storage自动创建pv需要经过kube-apiserver,所以要进行授权**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

创建nfs相关存储服务

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.101.144
            - name: NFS_PATH
              value: /data/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.101.144
            path: /data/k8s
```

查看创建的storageclass

```shell
kubectl get storageclass
NAME                  PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  2m3s
```

查看创建的nfs

```shell
kubectl get po
NAME                                      READY   STATUS             RESTARTS   AGE
nfs-client-provisioner-8466c647bb-5mwf6   0/1     ImagePullBackOff   0          2m8s
```

部署nginx使用动态PV

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  clusterIP: None
  selector:
    app: nginx
  ports:
  - name: web
    port: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.2
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-html
      volumes:
      - name: nginx-html
        persistentVolumeClaim:
          claimName: nginx-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: managed-nfs-storage
  resources:
    requests:
      storage: 1Gi
```
