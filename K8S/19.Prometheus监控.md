# Prometheus

由于服务pod会被调度到任何机器上运行，且pod挂掉后会被自动重启，并且我们也需要有更好的自动服务发现功能来实现服务报警的自动接入，实现更高效的运维报警，这里我们需要用到K8s的监控实现Prometheus，它是基于Google内部监控系统的开源实现。

## 安装

用Prometheus-Operator来进行Prometheus监控服务的安装，这也是我们生产中常用的安装方式。

Prometheus Operator的架构示意图：

<img src="D:%5Clinuxnotes%5CK8S%5C19.Prometheus%E7%9B%91%E6%8E%A7.assets%5Cimage-20220525210828281.png" alt="image-20220525210828281" style="zoom: 50%;" />

地址：https://github.com/prometheus-operator/kube-prometheus

下载：

```shell
git clone -b release-0.5 --single-branch https://github.com/prometheus-operator/kube-prometheus.git
```

安装operator：

```shell
cd kube-prometheus/manifests/setup/
kubectl apply -f .

[root@k8s-master01 manifests]# kubectl get po -n monitoring
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-6478d8fc6d-mqlf9   2/2     Running   0          2m43s

kubectl get svc -n monitoring
```

安装Prometheus：

```shell
cd ..
kubectl apply -f .

[root@k8s-master01 manifests]# kubectl get po -n monitoring
NAME                                   READY   STATUS    RESTARTS   AGE
alertmanager-main-0                    2/2     Running   0          28m
alertmanager-main-1                    2/2     Running   0          28m
alertmanager-main-2                    2/2     Running   0          28m
grafana-5d9d5f67c4-wrblp               1/1     Running   0          28m
kube-state-metrics-7fddf8779f-pxmsh    3/3     Running   0          28m
node-exporter-84l65                    2/2     Running   0          28m
node-exporter-9zpq8                    2/2     Running   0          28m
node-exporter-flppr                    2/2     Running   0          28m
node-exporter-pdrvv                    2/2     Running   0          28m
node-exporter-qrm4h                    2/2     Running   0          28m
prometheus-adapter-cb548cdbf-s7dcj     1/1     Running   0          28m
prometheus-k8s-0                       3/3     Running   1          28m
prometheus-k8s-1                       3/3     Running   0          28m
prometheus-operator-6478d8fc6d-mqlf9   2/2     Running   0          126m
```

创建ingress（之前已经部署了ingress-nginx）

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: prom-ingresses
  namespace: monitoring
spec:
  ingressClassName: nginx
  rules:
  - host: alert.test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: alertmanager-main
            port: 
              number: 9093
  - host: grafana.test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port: 
              number: 3000
  - host: prom.test.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus-k8s
            port: 
              number: 9090
```

把域名解析到ingress-nginx部署的节点的ip地址

## 监控有metrics接口的应用

这种应用监控起来比较简单，可以先创建endpoint和service（如果没有的话），然后通过servicemonitor来监控到应用

Prometheus Operator 引入 ServiceMonitor 对象，它发现 Endpoints 对象并配置 Prometheus 去监控这些 Pods

### 监控ingress-nginx

前面ingress-nginx服务是以daemonset形式部署的，并且映射了自己的端口到宿主机上，那么我可以直接用pod运行NODE上的IP来看下metrics

```shell
kubectl get service nginx-ingress-lb -n ingress-nginx --show-labels
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                    AGE   LABELS
nginx-ingress-lb   ClusterIP   10.98.244.237   <none>        80/TCP,443/TCP,10254/TCP   18d   app=ingress-nginx
kubectl get ep nginx-ingress-lb -n ingress-nginx
NAME               ENDPOINTS                                                      AGE
nginx-ingress-lb   192.168.101.149:10254,192.168.101.149:443,192.168.101.149:80   18d

curl 192.168.101.149:10254/metrics
```

创建servicemonitor配置让Prometheus发现ingress-nginx的metrics

```yaml
cd k8s-apps/kube-prometheus/manifests
cp prometheus-serviceMonitor.yaml prometheus-serviceMonitoringress.yaml

# vim prometheus-serviceMonitoringress.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: ingress-nginx
  name: nginx-ingress-scraping
  namespace: ingress-nginx
spec:
  endpoints:
  - interval: 30s
    path: /metrics
    port: metrics
  jobLabel: app
  namespaceSelector:
    matchNames:
    - ingress-nginx #目标服务的namespaces
  selector:
    matchLabels:
      app: ingress-nginx #目标服务的labels
      
kubectl apply -f prometheus-serviceMonitoringress.yaml 

# kubectl -n ingress-nginx get servicemonitors
NAME                     AGE
nginx-ingress-scraping   8s
```

但是指标一直没收集上来，看看proemtheus服务的日志，发现报错如下：

```shell
kubectl -n monitoring logs prometheus-k8s-0 -c prometheus |grep error
level=error ts=2022-05-14T07:29:26.404Z caller=klog.go:94 component=k8s_client_runtime func=ErrorDepth msg="/app/discovery/kubernetes/kubernetes.go:261: Failed to list *v1.Endpoints: endpoints is forbidden: User \"system:serviceaccount:monitoring:prometheus-k8s\" cannot list resource \"endpoints\" in API group \"\" in the namespace \"ingress-nginx\""

#因为我们用servicemonitor资源进行监听发现，该资源属于prometheus-operator的，所以我们变相使用的是prometheus-operator的用户和权限，通过报错可以发现prometheus-k8s用户权限不够
```

需要修改prometheus-k8s的clusterrole

```
#   kubectl edit clusterrole prometheus-k8s
#------ 原始的rules -------
rules:
- apiGroups:
  - ""
  resources:
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
#---------------------------

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus-k8s
rules:
- apiGroups:
  - ""
  resources:
  - nodes
  - services
  - endpoints
  - pods
  - nodes/proxy
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - nodes/metrics
  verbs:
  - get
- nonResourceURLs:
  - /metrics
  verbs:
  - get
```

再到prometheus UI上看下，发现已经有了

```
ingress-nginx/nginx-ingress-scraping/0 (1/1 up) 
```

### 监控二进制部署的ETCD集群

案例目的：监控非K8s集群服务

查看etcd暴露的哪些指标：

```
curl --cacert /etc/kubernetes/ssl/ca.pem --cert /etc/etcd/ssl/etcd.pem  --key /etc/etcd/ssl/etcd-key.pem https://10.0.1.201:2379/metrics
```

开始进行配置使ETCD能被prometheus发现并监控

```shell
# 首先把ETCD的证书创建为secret
kubectl -n monitoring create secret generic etcd-certs --from-file=/etc/etcd/ssl/etcd.pem   --from-file=/etc/etcd/ssl/etcd-key.pem   --from-file=/etc/kubernetes/ssl/ca.pem

# 接着在prometheus里面引用这个secrets
kubectl -n monitoring edit prometheus k8s
spec:
...
  secrets:
  - etcd-certs
  
# 保存退出后，prometheus会自动重启服务pod以加载这个secret配置，过一会，我们进pod来查看下是不是已经加载到ETCD的证书了
kubectl -n monitoring exec -it prometheus-k8s-0 -c prometheus  -- sh 
/prometheus $ ls /etc/prometheus/secrets/etcd-certs/
ca.pem        etcd-key.pem  etcd.pem
```

创建service、endpoints以及ServiceMonitor的yaml配置

```yaml
# vim prometheus-etcd.yaml 
apiVersion: v1
kind: Service
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: api
    port: 2379
    protocol: TCP
---
apiVersion: v1
kind: Endpoints
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd
subsets:
- addresses:
  - ip: 10.0.1.201
  - ip: 10.0.1.202
  - ip: 10.0.1.203
  ports:
  - name: api
    port: 2379
    protocol: TCP
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: etcd-k8s
  namespace: monitoring
  labels:
    k8s-app: etcd-k8s
spec:
  jobLabel: k8s-app
  endpoints:
  - port: api
    interval: 30s
    scheme: https
    tlsConfig:
      caFile: /etc/prometheus/secrets/etcd-certs/ca.pem
      certFile: /etc/prometheus/secrets/etcd-certs/etcd.pem
      keyFile: /etc/prometheus/secrets/etcd-certs/etcd-key.pem
      #use insecureSkipVerify only if you cannot use a Subject Alternative Name
      insecureSkipVerify: true 
  selector:
    matchLabels:
      k8s-app: etcd
  namespaceSelector:
    matchNames:
    - monitoring
```

过一会，就可以在prometheus UI上面看到ETCD集群被监控了

```
monitoring/etcd-k8s/0 (3/3 up) 
```

接下来我们用grafana来展示被监控的ETCD指标

```
1. 在grafana官网模板中心搜索etcd，下载这个json格式的模板文件
https://grafana.com/dashboards/3070

2.然后打开自己先部署的grafana首页，
点击左边菜单栏四个小正方形方块HOME --- Manage
再点击右边 Import dashboard --- 
点击Upload .json File 按钮，上传上面下载好的json文件 etcd_rev3.json，
然后在prometheus选择数据来源
点击Import，即可显示etcd集群的图形监控信息
```

## Exporter

像mysql、redis这种应用没有metrics接口该怎么监控，可以通过exporter暴露metrics接口，然后再通过servicemonitor监控发现

### 监控redis

redis应用

```shell
kubectl get po -n text --show-labels
NAME                                 READY   STATUS    RESTARTS   AGE    LABELS
redis-single-node-754c46848c-xg6zg   1/1     Running   0          101m   app=redis-single-node,pod-template-hash=754c46848c
```

创建redis-exporter deploy

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: redis-ec-exporter
    k8s.kuboard.cn/name: redis-ec-exporter
  name: redis-ec-exporter
  namespace: monitoring
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: redis-ec-exporter
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: redis-ec-exporter
    spec:
      containers:
      - args:
        - -redis.addr
        - redis://redis-single-node:6379
        image: oliver006/redis_exporter:latest
        imagePullPolicy: ifNotPresent
        name: redis-ec-exporter
        ports:
        - containerPort: 9121
          name: http
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```

创建redis-exporter service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-ec-exporter
  namespace: monitoring
  labels:
    app: redis-ec-exporter
    k8s.kuboard.cn/name: redis-ec-exporter
spec:
  type: ClusterIP
  ports:
  - port: 9121
    protocol: TCP
    name: http
  selector:
    app: redis-ec-exporter
```

创建servicemonitor

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: redis-ec-exporter
  name: redis-ec-exporter
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: http
  jobLabel: app
  namespaceSelector:
    matchNames:
    - monitoring
  selector:
    matchLabels:
      app: redis-ec-exporter
```

再到prometheus UI上看下，发现已经有了

```shell
monitoring/redis-ec-exporter/0 (1/1 up)
```

## 黑盒监控

 黑盒监控，主要**关注的现象**，一般都是正在发生的东西，例如出现一个告警，某文件系统不可写入，那么这种监控就是**站在用户的角度**能看到的监控，重点在于能对正在发生的故障进行告警。

白盒监控，主要**关注的是原因**，也就是**系统内部暴露的一些指标**，例如redis的info中显示redis slave down，这个就是redis info显示的一个内部的指标，重点在于原因，可能是在黑盒监控中看到redis down，而查看内部信息的时候，显示redis port is refused connection。



Blackbox Exporter 是 Prometheus 社区提供的官方黑盒监控解决方案，其允许用户通过：HTTP、HTTPS、DNS、TCP 以及 ICMP 的方式对网络进行探测。



下载blackbox.yaml:

https://github.com/prometheus/blackbox_exporter

创建configmap

```yaml
kubectl create cm blackbox-conf --from-file=blackbox.yaml -n monitoring
```

创建blackbox-exporter这个deployment

```yaml
apiVersion: v1
kind: Service
metadata:
  name: blackbox-exporter
  namespace: monitoring
spec:
  ports:
  - name: web
    port: 9115
    protocol: TCP
    targetPort: 9115
  selector:
    app: blackbox-exporter
  type: ClusterIP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: blackbox-exporter
  name: blackbox-exporter
  namespace: monitoring
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: blackbox-exporter
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blackbox-exporter
    spec:
      containers:
      - image: prom/blackbox-exporter:master
        args:
        - --config.file=/mnt/blackbox.yaml
        env:
        - name: TZ
          value: Asia/Shanghai
        - name: LANG
          value: C.UTF-8
        imagePullPolicy: IfNotPresent
        name: blackbox-exporter
        ports:
        - containerPort: 9115
          name: web
          protocol: TCP
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /usr/share/zoneinfo/Asia/Shanghai
          name: tz-config
        - mountPath: /etc/localtime
          name: tz-config
        - mountPath: /etc/timezone
          name: timezone
        - mountPath: /mnt
          name: config
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - hostPath:
          path: /usr/share/zoneinfo/Asia/Shanghai
        name: tz-config
      - hostPath:
          path: /etc/timezone
        name: timezone
      - configMap:
          defaultMode: 420
          name: blackbox-conf
        name: configs
```

返回针对 baidu.com 的 HTTP 探测的指标

```shell
kubectl get svc blackbox-exporter -n monitoring
NAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
blackbox-exporter   ClusterIP   10.96.47.27   <none>        9115/TCP   7d2h

curl "http://10.96.47.27:9115/probe?target=baidu.com&module=http_2xx"
```

由于集群是用的Prometheus Operator方式部署的，所以就以additional的形式添加配置。

用传统方式写一个配置文件

```yaml
vim prometheus-additional.yaml

- job_name: 'blackbox'
  metrics_path: /probe
  params:
    module: [http_2xx]  # Look for a HTTP 200 response.
  static_configs:
    - targets:
      - http://prometheus.io    # Target to probe with http.
      - https://prometheus.io   # Target to probe with https.
      - https://www.baidu.com # Target to probe with http on port 8080.
  relabel_configs:
    - source_labels: [__address__]
      target_label: __param_target
    - source_labels: [__param_target]
      target_label: instance
    - target_label: __address__
      replacement: blackbox-exporter:9115  # The blackbox exporter's real hostname:port.
```

生成一个secret文件并创建secret

```shell
kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml --dry-run -oyaml > additional-scrape-configs.yaml

kubectl apply -f additional-scrape-configs.yaml -n monitoring
```

修改prometheus-prometheus.yaml文件

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  labels:
    prometheus: k8s
  name: k8s
  namespace: monitoring
spec:
  alerting:
    alertmanagers:
    - name: alertmanager-main
      namespace: monitoring
      port: web
  image: quay.io/prometheus/prometheus:v2.15.2
  nodeSelector:
    kubernetes.io/os: linux
  podMonitorNamespaceSelector: {}
  podMonitorSelector: {}
  replicas: 2
  resources:
    requests:
      memory: 400Mi
  ruleSelector:
    matchLabels:
      prometheus: k8s
      role: alert-rules
  securityContext:
    fsGroup: 2000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccountName: prometheus-k8s
  serviceMonitorNamespaceSelector: {}
  serviceMonitorSelector: {}
  version: v2.15.2
  #添加这几行内容
  additionalScrapeConfigs:
    name: additional-scrape-configs
    key: prometheus-additional.yaml
```

reload Prometheus

```shell
kubectl get svc -n monitoring -l prometheus=k8s
NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
prometheus-k8s   ClusterIP   10.107.186.102   <none>        9090/TCP   56d

curl -X POST "http://10.107.186.102:9090/-/reload"
```

访问http://prom.test.com/config，搜索blackbox可以看到配置文件已经加载过来了

到grafana官网下载模板：https://grafana.com/api/dashboards/5345/revisions/3/download

把模板导入grafana

![image-20220522205933440](https://gitee.com/c_honghui/picture/raw/master/img/20220522205940.png)

![image-20220522210105075](https://gitee.com/c_honghui/picture/raw/master/img/20220522210105.png)

## 告警

### 配置文件

a'lertmanager.yaml

```yaml
# global块配置下的配置选项在本配置文件内的所有配置项下可见
global:
  # 在Alertmanager内管理的每一条告警均有两种状态: "resolved"或者"firing". 在altermanager首次发送告警通知后, 该告警会一直处于firing状态,设置resolve_timeout可以指定处于firing状态的告警间隔多长时间会被设置为resolved状态, 在设置为resolved状态的告警后,altermanager不会再发送firing的告警通知.
  resolve_timeout: 1h

  # 邮件告警配置
  smtp_smarthost: 'smtp.exmail.qq.com:25'
  smtp_from: 'dukuan@xxx.com'
  smtp_auth_username: 'dukuan@xxx.com'
  smtp_auth_password: 'DKxxx'
  # HipChat告警配置
  # hipchat_auth_token: '123456789'
  # hipchat_auth_url: 'https://hipchat.foobar.org/'
  # wechat
  wechat_api_url: 'https://qyapi.weixin.qq.com/cgi-bin/'
  wechat_api_secret: 'JJ'
  wechat_api_corp_id: 'ww'

  # 告警通知模板
templates:
- '/etc/alertmanager/config/*.tmpl'

# route: 根路由,该模块用于该根路由下的节点及子路由routes的定义. 子树节点如果不对相关配置进行配置，则默认会从父路由树继承该配置选项。每一条告警都要进入route，即要求配置选项group_by的值能够匹配到每一条告警的至少一个labelkey(即通过POST请求向altermanager服务接口所发送告警的labels项所携带的<labelname>)，告警进入到route后，将会根据子路由routes节点中的配置项match_re或者match来确定能进入该子路由节点的告警(由在match_re或者match下配置的labelkey: labelvalue是否为告警labels的子集决定，是的话则会进入该子路由节点，否则不能接收进入该子路由节点).
route:
  # 例如所有labelkey:labelvalue含cluster=A及altertname=LatencyHigh labelkey的告警都会被归入单一组中
  group_by: ['job', 'altername', 'cluster', 'service','severity']
  # 若一组新的告警产生，则会等group_wait后再发送通知，该功能主要用于当告警在很短时间内接连产生时，在group_wait内合并为单一的告警后再发送
  group_wait: 30s
  # 再次告警时间间隔
  group_interval: 5m
  # 如果一条告警通知已成功发送，且在间隔repeat_interval后，该告警仍然未被设置为resolved，则会再次发送该告警通知
  repeat_interval: 12h
  # 默认告警通知接收者，凡未被匹配进入各子路由节点的告警均被发送到此接收者
  receiver: 'wechat'
  # 上述route的配置会被传递给子路由节点，子路由节点进行重新配置才会被覆盖

  # 子路由树
  routes:
  # 该配置选项使用正则表达式来匹配告警的labels，以确定能否进入该子路由树
  # match_re和match均用于匹配labelkey为service,labelvalue分别为指定值的告警，被匹配到的告警会将通知发送到对应的receiver
  - match_re:
      service: ^(foo1|foo2|baz)$
    receiver: 'wechat'
    # 在带有service标签的告警同时有severity标签时，他可以有自己的子路由，同时具有severity != critical的告警则被发送给接收者team-ops-mails,对severity == critical的告警则被发送到对应的接收者即team-ops-pager
    routes:
    - match:
        severity: critical
      receiver: 'wechat'
  # 比如关于数据库服务的告警，如果子路由没有匹配到相应的owner标签，则都默认由team-DB-pager接收
  - match:
      service: database
    receiver: 'wechat'
  # 我们也可以先根据标签service:database将数据库服务告警过滤出来，然后进一步将所有同时带labelkey为database
  - match:
      severity: critical
    receiver: 'wechat'
# 抑制规则，当出现critical告警时 忽略warning
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  # Apply inhibition if the alertname is the same.
  #   equal: ['alertname', 'cluster', 'service']
  #
# 收件人配置
receivers:
- name: 'team-ops-mails'
  email_configs:
  - to: 'dukuan@xxx.com'
- name: 'wechat'
  wechat_configs:
  - send_resolved: true
    corp_id: 'ww'
    api_secret: 'JJ'
    to_tag: '1'
    agent_id: '1000002'
    api_url: 'https://qyapi.weixin.qq.com/cgi-bin/'
    message: '{{ template "wechat.default.message" . }}'
#- name: 'team-X-pager'
#  email_configs:
#  - to: 'team-X+alerts-critical@example.org'
#  pagerduty_configs:
#  - service_key: <team-X-key>
#
#- name: 'team-Y-mails'
#  email_configs:
#  - to: 'team-Y+alerts@example.org'
#
#- name: 'team-Y-pager'
#  pagerduty_configs:
#  - service_key: <team-Y-key>
#
#- name: 'team-DB-pager'
#  pagerduty_configs:
#  - service_key: <team-DB-key>
#  
#- name: 'team-X-hipchat'
#  hipchat_configs:
#  - auth_token: <auth_token>
#    room_id: 85
#    message_format: html
#    notify: true 
```

### 使用邮箱告警

https://prometheus.io/docs/alerting/latest/configuration/#email_config

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-main
  namespace: monitoring
stringData:
  alertmanager.yaml: |-
    "global":
      "resolve_timeout": "5m"
      smtp_smarthost: 'smtp@163.com:465'
      smtp_from: 'kubernetes_guide@163.com'
      smtp_auth_username: 'kubernetes_guide@163.com'
      smtp_auth_password: 'DKxxx'
    "inhibit_rules":
    - "equal":
      - "namespace"
      - "alertname"
      "source_match":
        "severity": "critical"
      "target_match_re":
        "severity": "warning|info"
    - "equal":
      - "namespace"
      - "alertname"
      "source_match":
        "severity": "warning"
      "target_match_re":
        "severity": "info"
    "receivers":
    - "name": "Default"
      "email_configs":
      - to: 'kubernetes_guide@163.com'
        send_resolved: true
    - "name": "Watchdog"
      "email_configs":
      - to: 'kubernetes_guide@163.com'
        send_resolved: true
    - "name": "Critical"
      "email_configs":
      - to: 'kubernetes_guide@163.com'
        send_resolved: true
    "route":
      "group_by":
      - "namespace"
      "group_interval": "5m"
      "group_wait": "30s"
      "receiver": "Default"
      "repeat_interval": "12h"
      "routes":
      - "match":
          "alertname": "Watchdog"
        "receiver": "Watchdog"
      - "match":
          "severity": "critical"
        "receiver": "Critical"
type: Opaque
```

### 使用微信告警

在企业微信管理平台创建一个应用给Prometheus调用

修改配置文件

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-main
  namespace: monitoring
stringData:
  alertmanager.yaml: |-
    "global":
      "resolve_timeout": "5m"
      #email
      smtp_smarthost: 'smtp@163.com:465'
      smtp_from: 'kubernetes_guide@163.com'
      smtp_auth_username: 'kubernetes_guide@163.com'
      smtp_auth_password: 'DKxxx'
      #wechat
      wechat_api_url: 'https://qyapi.weixin.qq.com/cgi-bin/'
      wechat_api_secret: 'xxxxxxxxx' #应用的secret，在企业微信管理界面有
      wechat_api_corp_id: 'xxxxxx' #企业id
    "inhibit_rules":
    - "equal":
      - "namespace"
      - "alertname"
      "source_match":
        "severity": "critical"
      "target_match_re":
        "severity": "warning|info"
    - "equal":
      - "namespace"
      - "alertname"
      "source_match":
        "severity": "warning"
      "target_match_re":
        "severity": "info"
    "receivers":
    - "name": "Default"
      "email_configs":
      - to: 'kubernetes_guide@163.com'
        send_resolved: true
    - "name": "Watchdog"
      "email_configs":
      - to: 'kubernetes_guide@163.com'
        send_resolved: true
    - "name": "Critical"
      "email_configs":
      - to: 'kubernetes_guide@163.com'
        send_resolved: true
    - name: 'wechat'
      wechat_configs:
      - send_resolved: true
        to_tag: '1' #部门id
        agent_id: '1000002' #应用id
    "route":
      "group_by":
      - "namespace"
      "group_interval": "5m"
      "group_wait": "30s"
      "receiver": "Default"
      "repeat_interval": "12h"
      "routes":
      - "match":
          "alertname": "Watchdog"
        "receiver": "wechat"
      - "match":
          "severity": "critical"
        "receiver": "Critical"
type: Opaque
```

自定义告警模板：

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-main
  namespace: monitoring
stringData:
  alertmanager.yaml: |-
    "global":
    ...
    templates:
    - '/etc/alertmanager/config/*.tmpl'
    ...
    "receivers":
    - name: 'wechat'
      wechat_configs:
      - send_resolved: true
        to_tag: '1'
        agent_id: '1000002'
        message: '{{ template "wechat.default.message" . }}' #跟下面定义的告警模板名称一致
  wechat.tmpl: |-
    {{ define "wechat.default.message" }} #跟上面的message参数内容一致
    {{ if gt (len .Alerts.Firing) 0 -}}
    Alerts Firing:
    {{ range .Alerts }}
    告警级别: {{ .Labels.severity }}
    告警类型: {{ .Labels.alertname }}
    故障主机: {{ .Labels.instance }}
    Job名称: {{ .Labels.job }}
    告警主题: {{ .Annotations.summary }}
    告警详情: {{ .Annotations.description }}
    触发时间: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
    {{- end }}
    {{- end }}
    {{ if gt (len .Alerts.Resolved) 0 -}}
    Alerts Resolved:
    {{ range .Alerts }}
    告警级别: {{ .Labels.severity }}
    告警类型: {{ .Labels.alertname }}
    故障主机: {{ .Labels.instance }}
    Job名称: {{ .Labels.job }}
    告警主题: {{ .Annotations.summary }}
    触发时间: {{ .StartsAt.Format "2006-01-02 15:04:05" }}
    恢复时间: {{ .EndsAt.Format "2006-01-02 15:04:05" }}
    {{- end }}
    {{- end }}
    告警链接:
    {{ template "__alertmanagerURL" . }}
    {{- end }}
type: Opaque
```

## 自动发现





