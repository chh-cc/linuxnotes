# Pod

## 为什么用Pod

一个应用不可能由单个容器支撑的，肯定有多个微服务，如果用k8s直接去控制裸容器的话，不能保证俩容器在同一宿主机上，不能保证俩容器能共享一个目录

k8s不是只编排docker容器

## 什么是Pod

Pod是k8s可以创建和部署的最小的单元，Pod包含一个或多个容器

每个Pod还包含一个Pause容器，Pause容器是Pod的父容器，负责僵尸进程的回收管理，其他容器通过Pause容器共享volume、网络、PID、IPC等。

**Pod有独立的IP，用namespace进行资源隔离，pod内部容器之间通过localhost进行访问**

<img src="https://gitee.com/c_honghui/picture/raw/master/img/20211114000209.png" alt="image-20211114000209381" style="zoom:67%;" />

一个pod对应一个pause容器：

![image-20211113235653751](https://gitee.com/c_honghui/picture/raw/master/img/20211113235653.png)

## 创建一个Pod

定义一个pod（生产环境很少这样创建pod）

```yaml
vim pod.yaml

apiVersion: v2 #指定api版本，此值必须在kubectl apiversion中
kind: Pod #必选，类型pod
metadata: #必选，元数据
  name: nginx #必选，资源的名字，在同一个namespace中必须唯一
# namespace: default #可选，pod所在命名空间，不指定默认default，可使用-n指定namespace
  labels: #设定资源的标签，可写多个
    app: nginx
    role: frontend
# annotations:        #可选，自定义注解列表
#   app: nginx        #自定义注解名字
spec: #必选，定义容器详细信息
# nodeSelector:     #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1
#   zone: node1
  containers:  #必选，容器列表
    - name: nginx #容器的名字
      image: nginx:1.15.2 #容器使用的镜像
      imagePullPolicy: IfNotPresent #可选，镜像拉取策略
                                    # Always，每次都检查
                                    # Never，每次都不检查（不管本地是否有）
                                    # IfNotPresent，如果本地有就不检查，如果没有就拉取
      command:  #容器启动执行的命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT
      - nginx
      - -g
      - "daemon off;"
      workingDir: /usr/share/nginx/html #可选，容器的工作目录
      volumeMounts:  #可选，容器存储配置，可配置多个
      - name: webroot #存储卷名称
        mountPath: /usr/share/nginx/html #挂载目录
        readOnly: True #只读
      ports:  #可选，要暴露的端口号列表
      - containerPort: 80 #端口号
        name: http  #端口名称
        protocol: TCP #端口协议，默认tcp
      env: #可选，环境变量配置列表
      - name: TZ #变量的名字  
        value: Asia/Shanghai #变量的值 
      - name: LANG
        value: en_US.utf8
#     resources: #可选，资源限制和资源请求限制
#       requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行
#         cpu: 100m #1000m=1核
#         memory: 512Mi
#       limits: #最大限制设置
#         cpu: 1000m
#         memory: 1024Mi
#     startupProbe: #可选，检测容器内进程是否完成启动
#       httpGet:
#         path: /api/successStart
#         port: 80
#     readinessProbe:
#       httpGet:
#         path: /
#          port: 80
      livenessProbe: #可选，健康检查
        httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常
          path: / #URI地址
          port: 80
        initialDelaySeconds: 60 #初始化时间
        timeoutSeconds: 2 #检测的超时时间
        periodSeconds: 5  #检查间隔时间
        successThreshold: 1 #检查成功1次表示就绪
        failureThreshold: 2 #检查失败2次表示未就绪
#       exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常
#         command:
#           - cat
#           - /tmp/health
      lifecycle: #生命周期管理
        postStart: #容器创建完成后执行的命令
          exec:
            command:
              - sh
              - -c
              - 'mkdir /data'
        preStop: #容器关闭之前运行的任务
          httpGet:
               path: /
               port: 80
  restartPolicy: Always #可选，默认Always，容器故障或没启动成功，就自动重启该容器，Onfailure：容器以不为0的状态终止，自动重启容器，Never：不重启
  imagePullSecrets: #可选，拉取镜像使用的secret，可配置多个
  - name: default-dockercfg-86258
  volumes: #共享存储卷列表
  - name: webroot
    emptyDir: {} #挂载目录
```

## Pod相关命令

kubectl自动补齐

```shell
yum -y install bash-completion
bash
source <(kubectl completion bash)
```

创建一个pod

```shell
kubectl create -f pod.yaml
#kubectl apply -f pod.yaml #如果存在则更新，不存在则创建
```

查看pod状态

```shell
kubectl get po [-owide]

NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-74cdcc745-c5ctf   2/2     Running   0          38m   172.171.14.193   k8s-node02   <none>           <none>
nginx-74cdcc745-ch9zd   2/2     Running   0          38m   172.161.125.24   k8s-node01   <none>           <none>
nginx-74cdcc745-kr9qh   2/2     Running   0          38m   172.171.14.194   k8s-node02   <none>           <none>

#Pending：API Server 已经创建该 Pod，且 Pod 内还有一个或多个容器的镜像没有创建，包括正在下载镜像的过程。
#Failed：Pod 内有容器启动失败。
#Unknown：由于某种原因无法获得pod的当前状态，通常是由于与pod所在的node节点通信错误。
#Succeeded：Pod 内所有容器均成功执行退出，且不会重启
#Running：Pod 内所有容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态。
```

进入pod

```shell
kubectl exec -it kubia-66c8b6d4fc-cdzzg -- sh
```

进入pod中的某个容器

```shell
kubectl exec -it <pod-name> -c <container-name> -- bash
```

查看pod标签

```shell
kubectl get po --show-labels

#查看所有namespace
kubectl get po -A --show-labels
```

通过标签筛选pod

```shell
kubectl get po -l app=nginx -w

# -w是动态查看
```

将pod http-label-7cf498876f-rhqxf的标签env由prod更改为debug

```shell
kubectl label pod http-label-7cf498876f-rhqxf env=debug  --overwrite
```

查看命名空间

```shell
kubectl get ns
```

查看指定命名空间的pod

```shell
kubectl get po -n kube-system
#不指定的话默认是default
```

查看pod详细信息，比如**pod状态不是running的时候查看报错**

```shell
kubectl describe po web-2

Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  4m                     default-scheduler  Successfully assigned default/nginx-5cf8b44c6d-hk4l7 to k8s-node02
  Normal   Pulled     2m32s (x5 over 3m59s)  kubelet            Container image "nginx:1.15.2" already present on machine
  Normal   Created    2m32s (x5 over 3m59s)  kubelet            Created container nginx2
  Warning  Failed     2m32s (x5 over 3m59s)  kubelet            Error: failed to start container "nginx2": Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: "sleep 3600": executable file not found in $PATH: unknown
  Warning  BackOff    114s (x10 over 3m57s)  kubelet            Back-off restarting failed container
```

以yaml格式查看pod信息

```shell
kubectl get po web-2 -oyaml
```

查看容器日志，比如**pod状态不是running的时候查看报错**

```shell
kubectl logs -f <pod-name> [<container-name>]
```

删除pod

```shell
kubectl delete po web-0
```

## Pod探针

### 探针种类

StartupProbe：k8s1.16版本新加的探测方式，用于判断**容器内的应用程序是否已经启动**，如果配置了StartupProbe，就会先禁止其他的探针，直到它成功为止，成功后将不再进行探测。（**程序启动慢可以配这个**）

LivenessProbe：用于探测**容器是否运行（running状态）**，如果探测失败，kubelet会根据配置的重启策略进行相应处理，若没有配置该探针，默认就是success。

ReadinessProbe：判断容器是否准备好服务请求。如果 ReadinessProbe 探针探测到失败，则 Pod 的状态将被修改。Endpoint  Controller 将从与 Pod 匹配的所有 Service 的端点中删除该 Pod 的 IP 地址

### 探针的检测方式

ExecAction：在容器内执行一个命令，如果返回值为0，则认为容器健康（类似linux的echo $?命令）

TCPSocketAction：通过TCP连接检查容器内的端口是否是通的，如果是通的就认为容器健康。

**HttpGetAction**：通过应用程序暴露的API地址来检查程序是否正常，如果状态码在200~400之间则认为容器健康。**生产环境建议使用**

### 探针检查参数

```yaml
initialDelay: 10 #初始化时间
timeoutSeconds: 2 #超时时间
periodSeconds: 10 #检测间隔
successThreshold: 1 #检查成功1次表示就绪
failureThreshold: 1 #检查失败1次表示未就绪
```

### 为什么用StartupProbe

startupProbe 和 livenessProbe 最大的区别就是**startupProbe在探测成功之后就不会继续探测了**，而livenessProbe在pod的生命周期中一直在探测。

假如配置了如下LivenessProbe

```yaml
livenessProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 1
  initialDelay: 10
  periodSeconds: 10
```

上面配置的意思是容器启动10s后每10s检查一次，允许失败的次数是1次。如果失败次数超过1则会触发restartPolicy。

但如果服务启动很慢的话比如60s，这个时候如果还是用上面的探针就会进入死循环，因为上面的探针10s后就开始探测，这时候我们服务并没有起来，发现探测失败就会触发restartPolicy。

如果改成如下配置

```yaml
livenessProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 4
  initialDelay: 30
  periodSeconds: 10
```

确实这样pod能够启动起来了，但在后期的探测中，你需要10*4=40s才能发现这个pod不可用。

在这时候我们把`startupProbe`和`livenessProbe`结合起来使用就可以很大程度上解决我们的问题。

```yaml
livenessProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 1
  initialDelay: 10
  periodSeconds: 10

startupProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 10
  initialDelay: 10
  periodSeconds: 10
```

`startupProbe`配置的是10s\*10+10s，也就是说只要应用在110s内启动都是OK的，一旦启动探针探测成功之后，就会被livenessProbe接管，这样应用挂掉了10s就会发现问题。

## Pod退出流程

用户执行删除操作，发送delete pod命令

执行`pod get` 命令显示pod状态变为Terminating，Endpoint删除该Pod的IP地址

如果pod中的一个容器定义了 preStop hook，就在容器中调用它。如果过了宽限期（terminationGracePeriodSeconds）preStop还在运行，则延长一个短的宽限期（2秒）。如果要延长preStop，你要修改terminationGracePeriodSeconds

宽限期过期时，pod中所有运行的进程被SIGKILL杀死

## HPA自动扩缩Pod

Horizontal Pod Autoscaler：Pod的水平自动伸缩器

观察**Pod的CPU、内存使用率**来自动扩缩Pod数量

不适用无法缩放的对象，比如DaemonSet

必须定义requests参数，必须安装metrics-server

运行hpa资源，名称为demo-nginx，当deployment资源对象的cpu使用率达到百分之20就进行扩容，最小2个，最多5个

```shell
kubectl autoscale deploy demo-nginx --cpu-percent=20 --min=2 --max=5
```

模拟并发请求

```shell
[root@node01 ~]# while true; do wget -q -O- 10.97.45.108; done              #一直返回ok属于正常现象
```

查看hpa资源对cpu的占用情况

```shell
kubectl get hpa
kubectl get top
```

