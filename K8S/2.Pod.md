# Pod

## Namespace

讲pod之前先说下namespace

在K8s上面，大部分资源都受ns的限制，来做资源的隔离，少部分如pv，clusterRole等不受ns控制

```shell

# 查看目前集群上有哪些ns
# kubectl get ns
NAME              STATUS        AGE
default           Active        5d3h
kube-system       Active        5d3h

# 通过kubectl 接上 -n namespaceName 来查看对应ns上面的资源信息
# kubectl -n kube-system get pod
NAME                                       READY   STATUS    RESTARTS   AGE
calico-kube-controllers-7fdc86d8ff-2mcm9   1/1     Running   1          29h
calico-node-dlt57                          1/1     Running   1          29h
calico-node-tvzqj                          1/1     Running   1          29h
calico-node-vh6sk                          1/1     Running   1          29h
calico-node-wpsfh                          1/1     Running   1          29h
coredns-d9b6857b5-tt7j2                    1/1     Running   1          29h
metrics-server-869ffc99cd-n2dc4            1/1     Running   2          29h

# 我们通过不接-n 的情况下，都是在默认命令空间default下进行操作，在生产中，通过测试一些资源就在这里进行
[root@node-1 ~]# kubectl get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-867c95f465-njv78   1/1     Running   0          12m
[root@node-1 ~]# kubectl -n default get pod
NAME                     READY   STATUS    RESTARTS   AGE
nginx-867c95f465-njv78   1/1     Running   0          12m

# 创建也很简单
[root@node-1 ~]# kubectl create ns test
namespace/test created
[root@node-1 ~]# kubectl get ns|grep test
test  

# 删除ns
# kubectl delete ns test 
namespace "test" deleted
```

生产中的小技巧：k8s删除namespaces状态一直为terminating问题处理

```shell

# kubectl get ns
NAME              STATUS        AGE
default           Active        5d4h
kubevirt          Terminating   2d2h   # <------ here

1、新开一个窗口运行命令  kubectl proxy
> 此命令启动了一个代理服务来接收来自你本机的HTTP连接并转发至API服务器，同时处理身份认证

2、新开一个终端窗口，将下面shell脚本整理到文本内`1.sh`并执行，$1参数即为删除不了的ns名称
vim 1.sh
#!/bin/bash

set -eo pipefail

die() { echo "$*" 1>&2 ; exit 1; }

need() {
        which "$1" &>/dev/null || die "Binary '$1' is missing but required"
}

# checking pre-reqs

need "jq"
need "curl"
need "kubectl"

PROJECT="$1"
shift

test -n "$PROJECT" || die "Missing arguments: kill-ns <namespace>"

kubectl proxy &>/dev/null &
PROXY_PID=$!
killproxy () {
        kill $PROXY_PID
}
trap killproxy EXIT

sleep 1 # give the proxy a second

kubectl get namespace "$PROJECT" -o json | jq 'del(.spec.finalizers[] | select("kubernetes"))' | curl -s -k -H "Content-Type: application/json" -X PUT -o /dev/null --data-binary @- http://localhost:8001/api/v1/namespaces/$PROJECT/finalize && echo "Killed namespace: $PROJECT"

3. 执行脚本删除
# bash 1.sh kubevirt
Killed namespace: kubevirt
1.sh: line 23: kill: (9098) - No such process
```

## 为什么要用Pod

容器的本质是进程。

在linux系统执行pstree -g，可以看到当前系统正在运行的进程树状结构：

```shell

systemd(1)-+-accounts-daemon(1984)-+-{gdbus}(1984)
           | `-{gmain}(1984)
           |-acpid(2044)
          ...      
           |-lxcfs(1936)-+-{lxcfs}(1936)
           | `-{lxcfs}(1936)
           |-mdadm(2135)
           |-ntpd(2358)
           |-polkitd(2128)-+-{gdbus}(2128)
           | `-{gmain}(2128)
           |-rsyslogd(1632)-+-{in:imklog}(1632)
           |  |-{in:imuxsock) S 1(1632)
           | `-{rs:main Q:Reg}(1632)
           |-snapd(1942)-+-{snapd}(1942)
           |  |-{snapd}(1942)
           |  |-{snapd}(1942)
           |  |-{snapd}(1942)
           |  |-{snapd}(1942)
```

比如rsyslogd这个程序，由三个进程组成：一个 imklog 模块，一个 imuxsock 模块，一个 rsyslogd 自己的 main 函数主进程，它们同属于1632进程组。这些进程互相协作，共同完成rsyslogd这个程序的职责。这三个进程必须在同一台机器上运行，否则它们之间的 Socket 通信和文件交换会出现问题。

而**Pod便是一个“进程组”**，解决具有“超亲密关系”的容器的调度问题。

## 什么是Pod

简单点说，Pod是k8s可以创建和部署的最小的单元，Pod包含一个或多个容器

pod其实**是一个逻辑概念**，k8s真正处理的还是宿主机上容器的namespace和cgroups，并不存在一个所谓pod的边界或隔离环境。



pod其实是一组共享了某些资源的容器，这些容器**共享同一个Network Namespace和同一个volume**，它们**借助一个中间容器Infra容器**，Infra 容器一定要占用

极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也

只有 100~200 KB 左右。

这也就意味着，对于 Pod 里的容器 A 和容器 B 来说（Pod的特性）：

- 它们可以直接使用 **localhost 进行通信**；
- 它们看到的网络设备跟 Infra 容器看到的完全一样；
- 一个 Pod **只有一个 IP 地址**，也就是这个 Pod 的 Network Namespace 对应的 IP 地址；当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；
- Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。

<img src="https://gitee.com/c_honghui/picture/raw/master/img/20211114000209.png" alt="image-20211114000209381" style="zoom:67%;" />

一个pod对应一个pause容器：

![image-20211113235653751](https://gitee.com/c_honghui/picture/raw/master/img/20211113235653.png)

## Pod基本概念

把**Pod看成传统环境的“虚拟机”，容器看成运行在这个机器的“用户程序”**，很多关于 Pod 对象的设计就非常容易理解了。

比如，凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。

 **Pod 中几个重要字段的含义和用法：**

- NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段

```yaml
apiVersion: v1
kind: Pod
...
spec:
 nodeSelector:
   disktype: ssd #这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上
```

- NodeName：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。
- HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。

```yaml
apiVersion: v1
kind: Pod
...
spec:
  hostAliases:
  - ip: "10.1.2.3"
    hostnames:
    - "foo.remote"
    - "bar.remote"
...
```

凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。
 这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。

举个例子，在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  shareProcessNamespace: true #这个 Pod 里的容器要共享 PID Namespace
  containers:
  - name: nginx
    image: nginx
  - name: shell
    image: busybox
    stdin: true
    tty: true
```

在shell容器就可以看到pod里每个容器的进程：

```ruby
$ kubectl attach -it nginx -c shell
/ # ps ax
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   14 101       0:00 nginx: worker process
   15 root      0:00 sh
   21 root      0:00 ps ax
```

**除了这些属性，Pod 里最重要的字段当属“Containers”了。**

k8s对“Containers”的定义和docker没什么差别，Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开发的端口），以及 volumeMounts（容器要挂载的 Volume）都是比较重要的字段，但还有几个需要额外关注的

- ImagePullPolicy：默认是 Always，即每次创建 Pod 都重新拉取一次镜像。 如果是Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。
- Lifecycle：容器状态发生变化时触发一系列“钩子”

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart: #在容器启动后，立刻执行一个指定的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但在 postStart 启动时，ENTRYPOINT 有可能还没有结束；如果 postStart 执行超时或者错误，Kubernetes 会在该 Pod 的 Events 中报出该容器启动失败的错误信息，导致 Pod 也处于失败的状态。
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
      
      preStop: #在容器被杀死之前（比如，收到了 SIGKILL 信号）执行的操作。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。
        exec:
          command: ["/usr/sbin/nginx","-s","quit"] #在这个容器被删除之前，我们则先调用了 nginx 的退出指令（即 preStop 定义的操作），从而实现了容器的“优雅退出”
```

## 创建一个Pod

定义一个静态pod，一般不会这样直接创建pod，因为这样pod**不能自我修复**；用controller可以创建管理多个pod，还提供滚动升级、自愈能力

作为 Kubernetes 项目里最核心的编排对象，Pod 携带的信息非常丰富：

```yaml
vim pod.yaml

apiVersion: v1 #指定api版本，此值必须在kubectl apiversion中
kind: Pod #必选，资源类型
metadata: #必选，元数据
  name: nginx #必选，资源的名字，在同一个namespace中必须唯一
  namespace: default #可选，pod所在命名空间，不指定默认default，可使用-n指定namespace
  labels: #设定资源的标签，可写多个
    app: nginx
    role: frontend
# annotations:        #可选，自定义注解列表
#   app: nginx        #自定义注解名字
spec: #必选，定义容器详细信息
# nodeSelector:     #节点选择，先给主机打标签kubectl label nodes kube-node1 zone=node1
#   zone: node1
  containers:  #必选，容器列表
    - name: nginx #容器的名字
      image: nginx:1.15.2 #容器使用的镜像
      imagePullPolicy: IfNotPresent # 镜像拉取策略
                                    # Always，每次都重新拉取镜像
                                    # Never，每次都不拉取镜像（不管本地是否有）
                                    # IfNotPresent，如果本地有就不拉取，如果没有就拉取
      command:  #容器启动执行的命令，将覆盖容器中的Entrypoint,对应Dockefile中的ENTRYPOINT
      - nginx
      - -g
      - "daemon off;"
      workingDir: /usr/share/nginx/html #可选，容器的工作目录
      volumeMounts:  #可选，容器存储配置，可配置多个
      - name: webroot #存储卷名称
        mountPath: /usr/share/nginx/html #挂载目录
        readOnly: True #只读
      ports:  #可选，要暴露的端口号列表
      - containerPort: 80 #端口号
        name: http  #端口名称
        protocol: TCP #端口协议，默认tcp
      env: #可选，环境变量配置列表
      - name: TZ #变量的名字  
        value: Asia/Shanghai #变量的值 
      - name: LANG
        value: en_US.utf8
#     resources: #可选，资源限制和资源请求限制
#       requests: #容器运行时，最低资源需求，也就是说最少需要多少资源容器才能正常运行
#         cpu: 100m #1000m（微核）=1核
#         memory: 512Mi
#       limits: #最大限制设置
#         cpu: 1000m
#         memory: 1024Mi
#     startupProbe: #可选，检测容器内进程是否完成启动
#       httpGet:
#         path: /api/successStart
#         port: 80
#     readinessProbe:
#       httpGet:
#         path: /
#          port: 80
      livenessProbe: #可选，健康检查
        httpGet: #通过httpget检查健康，返回200-399之间，则认为容器正常
          path: / #URI地址
          port: 80
        initialDelaySeconds: 60 #初始化时间
        timeoutSeconds: 2 #检测的超时时间
        periodSeconds: 5  #检查间隔时间
        successThreshold: 1 #检查成功1次表示就绪
        failureThreshold: 2 #检查失败2次表示未就绪
#       exec: 执行命令的方法进行监测，如果其退出码不为0，则认为容器正常
#         command:
#           - cat
#           - /tmp/health
      lifecycle: #生命周期管理
        postStart: #容器创建完成后执行的命令
          exec:
            command:
              - sh
              - -c
              - 'mkdir /data'
        preStop: #容器关闭之前运行的任务
          httpGet:
               path: /
               port: 80
  restartPolicy: Always # 可选，默认Always，容器退出后总是重启该容器
                        # Onfailure：容器异常退出（退出状态码不为0）才会重启容器
                        # Never：不重启
  imagePullSecrets: #可选，拉取镜像使用的secret，可配置多个
  - name: default-dockercfg-86258
  volumes: #共享存储卷列表
  - name: webroot
    emptyDir: {} #挂载目录
```

## InitContainer

假如现在有个war包需要放在tomcat的webapps目录下运行

有了pod后，可以把war包和tomcat分别做成镜像，然后把它们作为一个pod里的两个容器，配置文件如下：

```yaml

apiVersion: v1
kind: Pod
metadata:
  name: javaweb-2
spec:
  initContainers:
  - image: geektime/sample:v2
    name: war
    command: ["cp", "/sample.war", "/app"]
    volumeMounts:
    - mountPath: /app
      name: app-volume
  containers:
  - image: geektime/tomcat:7.0
    name: tomcat
    command: ["sh","-c","/root/apache-tomcat-7.0.42-v2/bin/start.sh"]
    volumeMounts:
    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps
      name: app-volume
    ports:
    - containerPort: 8080
      hostPort: 8001 
  volumes:
  - name: app-volume
    emptyDir: {}
```

这里的WAR 包容器的类型不再是一个普通容器，而是一个 Init Container 类型的容器。这个 Init Container 类型的 WAR 包容器启动后，我执行了一句"cp /sample.war /app"，把应用的 WAR 包拷贝到 /app 目录下，然后退出。



**init容器与普通容器的区别：**

- init容器会先于普通容器启动执行，直到所有init容器执行成功后，普通容器才会被启动
- 只有前一个init容器运行成功后才能运行下一个init容器
- init容器执行成功后就**退出了**



上面例子的tomcat才是我们的主容器，而war包容器只是为了给tomcat容器提供一个war包，所以用init容器优先运行war包容器。



若init容器运行失败，那么k8s需要重启它直到成功完成。如果Pod的spec.restartPolicy字段值为”Never“，那么运行失败的init容器不会被重启。

如果pod重启，所有init容器都要重新执行

修改init容器的image字段才会使init容器重新执行，也相当于重启pod

init容器不能有探针

## Pod相关命令

kubectl自动补齐

```shell
yum -y install bash-completion
bash
source <(kubectl completion bash)
```

创建一个pod

```shell
kubectl apply -f pod.yaml #如果存在则更新，不存在则创建
```

查看pod状态

```shell
kubectl get po [-owide]

NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-74cdcc745-c5ctf   2/2     Running   0          38m   172.171.14.193   k8s-node02   <none>           <none>
nginx-74cdcc745-ch9zd   2/2     Running   0          38m   172.161.125.24   k8s-node01   <none>           <none>
nginx-74cdcc745-kr9qh   2/2     Running   0          38m   172.171.14.194   k8s-node02   <none>           <none>

#Pending：Pod的yaml文件已经交给了k8s，API对象已经被创建并保存在etcd中，但这个pod里有些容器因为某些原因不能被顺利创建，比如调度失败。
#Failed：Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。
#Unknown：意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。
#Succeeded：Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。
#Running：这个Pod已经被调度成功，它包含的容器都被成功创建，并且至少有一个在运行。

CrashLoopBackOff： 容器退出，kubelet正在将它重启

InvalidImageName： 无法解析镜像名称

ImageInspectError： 无法校验镜像

ErrImageNeverPull： 策略禁止拉取镜像

ImagePullBackOff： 正在重试拉取

RegistryUnavailable： 连接不到镜像中心

ErrImagePull： 通用的拉取镜像出错

CreateContainerConfigError： 不能创建kubelet使用的容器配置

CreateContainerError： 创建容器失败

m.internalLifecycle.PreStartContainer  执行hook报错

RunContainerError： 启动容器失败

PostStartHookError： 执行hook报错

ContainersNotInitialized： 容器没有初始化完毕

ContainersNotReady： 容器没有准备完毕

ContainerCreating：容器创建中

PodInitializing：pod 初始化中

DockerDaemonNotReady：docker还没有完全启动

NetworkPluginNotReady： 网络插件还没有完全启动

Evicted：即驱赶的意思，意思是当节点出现异常时，kubernetes将有相应的机制驱赶该节点上的Pod。 多见于资源不足时导致的驱赶。
```

进入容器

```shell
kubectl exec -it kubia-66c8b6d4fc-cdzzg -- sh
```

进入pod中指定的容器

```shell
kubectl exec -it <pod-name> -c <container-name> -- bash
```

查看pod标签

```shell
kubectl get po --show-labels

#查看所有namespace
kubectl get po -A --show-labels
```

通过标签筛选pod

```shell
kubectl get po -l app=nginx -w

# -w是动态查看
```

将pod http-label-7cf498876f-rhqxf的标签env由prod更改为debug

```shell
kubectl label pod http-label-7cf498876f-rhqxf env=debug  --overwrite
```

查看命名空间

```shell
kubectl get ns
```

查看指定命名空间的pod

```shell
kubectl get po -n kube-system
#不指定的话默认是default
```

查看pod详细信息，比如**pod状态不是running的时候查看报错**

```shell
kubectl describe po web-2

Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  4m                     default-scheduler  Successfully assigned default/nginx-5cf8b44c6d-hk4l7 to k8s-node02
  Normal   Pulled     2m32s (x5 over 3m59s)  kubelet            Container image "nginx:1.15.2" already present on machine
  Normal   Created    2m32s (x5 over 3m59s)  kubelet            Created container nginx2
  Warning  Failed     2m32s (x5 over 3m59s)  kubelet            Error: failed to start container "nginx2": Error response from daemon: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: "sleep 3600": executable file not found in $PATH: unknown
  Warning  BackOff    114s (x10 over 3m57s)  kubelet            Back-off restarting failed container
```

以yaml格式查看pod信息

```shell
kubectl get po web-2 -oyaml
```

查看容器日志，比如**pod状态不是running的时候查看报错**

```shell
kubectl logs -f <pod-name> [<container-name>]
```

如果pod状态为running但没有正常工作，可能是拼写错误，可以通过校验排查

```shell
kubectl apply -validate -f pod.yaml
```

删除pod

```shell
kubectl delete po web-0
```

## 容器的健康检查和恢复机制

### k8s的默认健康检查机制

当容器内进程退出时返回状态码为非零，则会认为容器发生了故障，K8s就会根据restartPolicy来重启这个容器，以达到自愈的效果

模拟一个容器发生故障时的场景 :

```yaml

# vim testHealthz.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  name: busybox
spec:
  containers:
  - image: busybox:latest
    imagePullPolicy: IfNotPresent
    name: busybox
    resources: {}
    args:
    - /bin/sh
    - -c
    - sleep 10; exit 1       # 并添加pod运行指定脚本命令，模拟容器启动10秒后发生故障，退出状态码为1
  dnsPolicy: ClusterFirst
  restartPolicy: OnFailure # 将默认的Always修改为OnFailure
status: {}
```
创建容器

```shell
# kubectl apply -f testHealthz.yaml 
pod/busybox created

# 观察几分钟，利用-w 参数来持续监听pod的状态变化
# kubectl  get pod -w
NAME                     READY   STATUS              RESTARTS   AGE
busybox                  0/1     ContainerCreating   0          4s
busybox                  1/1     Running             0          6s
busybox                  0/1     Error               0          16s
busybox                  1/1     Running             1          22s
busybox                  0/1     Error               1          34s
busybox                  0/1     CrashLoopBackOff    1          47s
busybox                  1/1     Running             2          63s
busybox                  0/1     Error               2          73s
busybox                  0/1     CrashLoopBackOff    2          86s
busybox                  1/1     Running             3          109s
busybox                  0/1     Error               3          2m
busybox                  0/1     CrashLoopBackOff    3          2m15s
busybox                  1/1     Running             4          3m2s
busybox                  0/1     Error               4          3m12s
busybox                  0/1     CrashLoopBackOff    4          3m23s
busybox                  1/1     Running             5          4m52s
busybox                  0/1     Error               5          5m2s
busybox                  0/1     CrashLoopBackOff    5          5m14s

上面可以看到这个测试pod被重启了5次，然而服务始终正常不了，就会保持在CrashLoopBackOff了，等待运维人员来进行下一步错误排查
注：kubelet会以指数级的退避延迟（10s，20s，40s等）重新启动它们，上限为5分钟
这里我们是人为模拟服务故障来进行的测试，在实际生产工作中，对于业务服务，我们如何利用这种重启容器来恢复的机制来配置业务服务呢，答案是`liveness`检测
```

### 探针种类

StartupProbe：k8s1.16版本新加的探测方式，用于判断容器内的应用程序是否已经启动，如果配置了StartupProbe，就**会先禁止其他的探针，直到它成功为止，成功后将不再进行探测**。（**程序启动慢可以配这个**）

LivenessProbe（存活检查）：可以自定义条件来判断容器是否健康，**如果探测失败，则kubelet会杀死容器并重启容器（restartPolicy: Always）**

ReadinessProbe（就绪检查）：通过Readiness检测来告诉K8s什么时候可以将pod加入到服务Service的负载均衡池中，对外提供服务，这个在生产场景服务发布新版本时非常重要，当我们上线的新版本发生程序错误时，Readiness会检测失败，Endpoint  Controller 会**把pod从service endpoint中剔除**，从而不导入流量到pod内，将服务的故障控制在内部，在生产场景中，建议这个是必加的，Liveness不加都可以，因为有时候我们需要保留服务出错的现场来查询日志，定位问题，告之开发来修复程序。

PS：

**livenessprobe和readinessprobe的区别：**

两种检测的配置方法完全一样，支持的配置参数也一样。不同之处在于检测失败后的行为：Liveness 检测是重启容器；Readiness 检测则是将容器设置为不可用，不接收 Service 转发的请求。

Liveness 检测和 Readiness 检测是独立执行的，**二者之间没有依赖**，所以可以单独使用，也可以同时使用。

**生产滚动更新应用场景：**

如果 Readiness 检测一直没通过，所以新版本的pod都是Not ready状态的，这样就保证了错误的业务代码不会被外界请求到

### 探针的检测方式

ExecAction：在容器内执行一个命令，如果返回值为0，则认为容器健康（生产环境中保证应用健康存活的重要手段）

```yaml
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - "redis-cli ping"
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          livenessProbe:
            exec:
              command:
              - sh
              - -c
              - "redis-cli ping"
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
```

TCPSocketAction：对指定端口的容器IP进行TCP检查，如果端口打开则认为容器健康（Web 服务类的应用中非常常用）

```yaml
        readinessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 15
          periodSeconds: 10
        livenessProbe:
          tcpSocket:
            port: 9092
          initialDelaySeconds: 15
          periodSeconds: 10
```

HttpGetAction：对指定端口和路径的容器IP进行HTTP Get请求，如果状态码在200~400之间则认为容器健康。（Web 服务类的应用中非常常用）

```yaml
          readinessProbe:  # 定义只有http检测容器6222端口请求返回是 200-400，则接收下面的Service web-svc 的请求
            httpGet:
              scheme: HTTP
              path: /check
              port: 6222
            initialDelaySeconds: 10   # 容器启动 10 秒之后开始探测，注意观察下g1的启动成功时间
            periodSeconds: 5          # 每隔 5 秒再探测一次
            timeoutSeconds: 5         # http检测请求的超时时间
            successThreshold: 1       # 检测到有1次成功则认为服务是`就绪`
            failureThreshold: 3       # 检测到有3次失败则认为服务是`未就绪`
          livenessProbe:  # 定义只有http检测容器6222端口请求返回是 200-400，否则就重启pod
            httpGet:
              scheme: HTTP
              path: /check
              port: 6222
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 3
```

### 探针检查参数

```yaml
initialDelaySeconds: 10 #容器启动几秒后开始检测，一般会根据应用启动的准备时间来设置。比如某个应用正常启动要花 30 秒，那么 initialDelaySeconds 的值就应该大于 30。
timeoutSeconds: 2 #超时时间
periodSeconds: 10 #每几秒检测一次，K8s 如果连续执行 3 次 Liveness 检测均失败，则会杀掉并重启容器。
successThreshold: 1 #检查成功1次表示就绪
failureThreshold: 1 #检查失败1次表示未就绪
```

### 注意事项

选择合适的探针可以防止被误操作：

- 调大判断的超时阈值，防止容器在高压的情况出现偶发超时
- 调整判断次数的阈值，3次的默认值在短周期下不一定是最佳实践

### ExecAction例子

```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: test-liveness-exec
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 #创建一个文件，30秒后再删掉
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5 #健康检查在容器启动 5 s 后开始执行
      periodSeconds: 5 #5秒执行一次
```

30 s 之后，我们再查看一下 Pod 的 Events：

```shell
kubectl describe pod test-liveness-exec
 
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Warning  Unhealthy  47s (x3 over 56s)  kubelet            Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    47s                kubelet            Container liveness failed liveness probe, will be restarted
```

再次查看一下这个 Pod 的状态：

```shell
$ kubectl get pod test-liveness-exec
NAME           READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
```

Pod 并没有进入 Failed 状态，而是保持了 Running 状态。从RESTARTS 字段 0 到 1 的变化，就能明白原因：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。

**需要注意的是：Kubernetes 中并没有 Docker 的 Stop 语义。所以虽然是 Restart（重启），但实际却是重新创建了容器。**

这个功能就是 Kubernetes 里的**Pod 恢复机制**，也叫 **restartPolicy**。它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），**默认值是 Always**，即：任何时候这个容器发生了异常，它一定会被重新创建。

所以，假如一个 Pod 里只有一个容器，然后这个容器异常退出了。那么，只有当 restartPolicy=Never 时，这个 Pod 才会进入 Failed 状态。而其他情况下，由于 Kubernetes 都可以重启这个容器，所以 Pod 的状态保持 Running 不变。

而如果这个 Pod 有多个容器，仅有一个容器异常退出，它就始终保持 Running 状态，哪怕即使 restartPolicy=Never。只有当**所有容器也异常退出之后，这个 Pod 才会进入 Failed 状态。**

### 为什么用StartupProbe

startupProbe 和 livenessProbe 最大的区别就是**startupProbe在探测成功之后就不会继续探测了**，而**livenessProbe在pod的生命周期中一直在探测。**

假如配置了如下LivenessProbe

```yaml
livenessProbe:
  httpGet:
    path: /test
    prot: 80
#    tcpSocket:
#      port: 80
  failureThreshold: 1
  initialDelay: 10
  periodSeconds: 10
```

上面配置的意思是容器启动10s后每10s检查一次，允许失败的次数是1次。如果失败次数超过1则会触发restartPolicy。

但如果服务启动很慢的话比如60s，这个时候如果还是用上面的探针就会进入死循环，因为上面的探针10s后就开始探测，这时候我们服务并没有起来，发现探测失败就会触发restartPolicy。

如果改成如下配置

```yaml
livenessProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 4
  initialDelay: 30
  periodSeconds: 10
```

确实这样pod能够启动起来了，但在后期的探测中，你需要10*4=40s才能发现这个pod不可用。

在这时候我们把`startupProbe`和`livenessProbe`结合起来使用就可以很大程度上解决我们的问题。

```yaml
livenessProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 1
  initialDelay: 10
  periodSeconds: 10

startupProbe:
  httpGet:
    path: /test
    prot: 80
  failureThreshold: 10
  initialDelay: 10
  periodSeconds: 10
```

`startupProbe`配置的是10s\*10+10s，也就是说只要应用在110s内启动都是OK的，一旦启动探针探测成功之后，就会被livenessProbe接管，这样应用挂掉了10s就会发现问题。

## Pod退出流程

用户执行删除操作，发送delete pod命令

执行`pod get` 命令显示pod状态变为Terminating，Endpoint删除该Pod的IP地址

如果pod中的一个容器定义了 preStop hook，就在容器中调用它。如果过了宽限期（terminationGracePeriodSeconds）preStop还在运行，则延长一个短的宽限期（2秒）。如果要延长preStop，你要修改terminationGracePeriodSeconds

宽限期过期时，pod中所有运行的进程被SIGKILL杀死

## HPA自动扩缩Pod

Horizontal Pod Autoscaler：Pod的水平自动伸缩器

观察**Pod的CPU、内存使用率**来自动扩缩Pod数量

不适用无法缩放的对象，比如DaemonSet

必须定义requests参数，必须安装metrics-server

运行hpa资源，名称为demo-nginx，当deployment资源对象的cpu使用率达到百分之20就进行扩容，最小2个，最多5个

```shell
kubectl autoscale deploy demo-nginx --cpu-percent=20 --min=2 --max=5
```

模拟并发请求

```shell
[root@node01 ~]# while true; do wget -q -O- 10.97.45.108; done              #一直返回ok属于正常现象
```

查看hpa资源对cpu的占用情况

```shell
kubectl get hpa
kubectl get top
```

## Pod报错

节点性能不足：

```yaml
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  13s (x8 over 7m1s)  default-scheduler  0/5 nodes are available: 2 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn't tolerate, 3 Insufficient memory.
```

