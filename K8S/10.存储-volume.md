# Volume

https://kubernetes.io/docs/concepts/storage/volumes/

容器磁盘的文件的生命周期是短暂的，kubelet重启容器后容器的文件会丢失；一个pod中运行的多个容器可能需要共享一些文件。volume就是为了解决这些问题。



pod中的pause容器会挂载绑定volume，pod中的容器会共享pause的volume，容器重启后volume还在，**pod不存在时volume也不存在**。

![image-20220315231614348](https://gitee.com/c_honghui/picture/raw/master/img/20220315231621.png)



k8s支持的volume类型：

-    本地存储：emptyDIR/hostPath
-    网络存储
-    PVC和PV体系

## hostpath

不推荐使用，将节点上的文件或目录挂载到pod上

## emptyDir

emptyDir 是最基础的 Volume 类型。正如其名字所示，一个 emptyDir Volume 是 Host 上的一个空目录。

**如果删除pod，emptydir卷中的数据也会被删除**，一般用于pod中不同容器**临时共享数据**

容器崩溃不会从节点移除pod，因此emptydir卷的数据在容器崩溃时是安全的

示例：

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache #挂载到容器的/cache目录
      name: cache-volume
  volumes:
  - name: cache-volume #定义了一个emptyDir卷
    emptyDir: {}
```

## PV&PVC

PersistentVolume (PV) 是持久化存储卷，**生命周期独立于 Pod**，它是集群中的资源。是管理员设置的外部存储系统中的一块存储空间，它是**对底层共享存储的抽象**，PV和其对应的后端存储信息交给集群管理员运维和管理

PersistentVolumeClaim (PVC) 是**用户对PV的申请的接口**，PVC只需要申明自己需要的存储size、access mode等业务真正需要关心的需求，不用关心存储细节

**PV没有namespace概念，PVC有namespace隔离**

pod需要做的，就是在volume字段声明pvc，然后pvc对应的pv就会挂载到pod容器内的目录上

![img](https://gitee.com/c_honghui/picture/raw/master/img/20220122111121.png)

### 静态PV

集群管理员手动创建一些PV，它们带有可供集群用户使用的实际存储的细节。



nfs搭建

```
yum install nfs-utils
 
vim /etc/exports
/data/k8s/ 172.16.1.0/24(sync,rw,no_root_squash)
 
systemctl start nfs; systemctl start rpcbind 
systemctl enable nfs

测试：
yum install nfs-utils
showmount -e 172.16.1.131
```

创建PV：

```yaml
vim nfs-pv.yaml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 #pv名称
spec:
  capacity:
    storage: 2Gi #pv容量
  volumeMode: Filesystem #挂载类型
  accessModes:
    - ReadWriteMany #访问模式
  persistentVolumeReclaimPolicy: Retain #回收策略
  storageClassName: nfs-slow #类名
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp #存储路径
    server: 172.17.0.2 #nfs地址
    
kubectl get pv
NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv0001   2Gi        RWX            Retain           Available           nfs-slow                4s

#PV状态：
#Available：空闲的pv，没有被任何pvc绑定
#Bound：已经被pvc绑定
#Released：PVC被删除，但是资源未被重新使用
#Failed：自动回收失败
```



**回收策略**`persistentVolumeReclaimPolicy`：

- Recycle：删掉PVC，会rm -rf /thevolume/*将**PV里的内容删除**
- Retain：保留，删掉PVC后PV由Bound状态变为Released，此状态下的PV不能和别的PVC绑定，**删除PV后PV存储的内容还在**，可以重新创建PV和PVC绑定
- Delete：删除PVC后，对应的PV和存储内容也会被删掉



**访问模式**`accessModes`：

- ReadWriteOnce（RWO），该卷可以被单个节点以读写方式挂载 
- ReadOnlyMany（ROX），该卷可以被许多节点以只读方式挂载
- ReadWriteMany（RWX），该卷可以被多个节点以读写方式挂载

一个卷一次只能使用一种访问模式挂载，即使它支持多种访问模式



创建PVC：

```yaml
vim nfs-pvc.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim #pvc名称
# namespace: 
spec:
  accessModes:
    - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: nfs-slow #PVC和PV的类名一样，才能被绑定
  
kubectl get pvc
NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
myclaim   Bound    pv0001   2Gi        RWX            nfs-slow       4s
```

把pvc挂载到pod

```yaml
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /tmp/pvc #把pv卷挂载到容器的该目录
      name: pvc-test #pv卷名称
  volumes:
  - name: pvc-test #卷名称
    persistentVolumeClaim: #申明使用静态PVC永久化存储
      claimName: myclaim #pvc名称
```



创建PVC后一直绑定不上PV（pending）的原因：

- PVC的空间申请大小大于PV的容量
- PVC的StorageClassName和PV不一致
- PVC的accessModes和PV不一致

创建挂载了PVC的Pod后，一直处于pending状态的原因：

- PVC没有创建成功
- PVC和Pod不在同一个namespace

### 动态PV

如果一个大规模集群可能有成千上万个PVC，那意味着运维要创建成千上万个PV，而 StorageClass 对象的作用，其实就是自动创建 PV 的模板。



StorageClass 对象会定义如下两个部分内容：

- 第一，PV 的属性。比如，存储类型、Volume 的大小等等。
- 第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。

有这两个信息后，k8s就能根据用户提交的PVC，找到对应的StorageClass，然后k8s调用该StorageClass 声明的存储插件，创建需要的PV。



k8s存储架构：

![image-20220317163620511](https://gitee.com/c_honghui/picture/raw/master/img/20220317163620.png)

- PV Controller：负责PV和PVC的绑定、生命周期管理，并根据需求进行数据卷的Provision/Delete操作
- AD Controller：负责数据卷的Attach/Detach操作，将数据卷挂载/卸载到目标节点
- Volume Manager：管理卷的Mount/Unmount操作、卷设备的格式化
- Volume Plugins：扩展各种存储类型的卷管理能力，实现第三方存储的各种操作能力与k8s存储系统结合



k8s挂载volume过程：

![image-20220317162512151](https://gitee.com/c_honghui/picture/raw/master/img/20220317162519.png)

1. 用户创建了一个包含PVC的Pod（使用动态存储卷）
2. PV Controller会尝试找一个合适的PV和PVC进行绑定，找不到合适的PV就调用Volume Plugin创建存储卷，并创建PV对象，并将PV绑定到PVC
3. Sechduler根据Pod配置、节点状态、PV配置等，把Pod调度到节点
4. AD Controller发现Pod和PVC都处于待挂载状态，调用Volume Plugin实现设备挂载到目标节点
5. 节点的kubelet（Volume Manager）等设备挂载完，通过Volume Plugin将设备挂载到Pod的指定目录
6. kubelet得知挂载完毕后启动pod中的containers，用docekr -v将已经挂载到本地的卷映射到容器中



PV对存储系统的支持可通过其插件来实现，目前，Kubernetes支持如下类型的插件：

https://kubernetes.io/docs/concepts/storage/storage-classes/



定义一个storage

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false"
```

因为storage自动创建pv需要经过kube-apiserver,所以要进行授权

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

创建nfs相关存储服务

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: k8s.gcr.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: k8s-sigs.io/nfs-subdir-external-provisioner
            - name: NFS_SERVER
              value: 192.168.101.144
            - name: NFS_PATH
              value: /data/k8s
      volumes:
        - name: nfs-client-root
          nfs:
            server: 192.168.101.144
            path: /data/k8s
```

查看创建的storageclass

```shell
kubectl get storageclass
NAME                  PROVISIONER                                   RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage   k8s-sigs.io/nfs-subdir-external-provisioner   Delete          Immediate           false                  2m3s
```

查看创建的nfs

```shell
kubectl get po
NAME                                      READY   STATUS             RESTARTS   AGE
nfs-client-provisioner-8466c647bb-5mwf6   0/1     ImagePullBackOff   0          2m8s
```

部署nginx使用动态PV

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  clusterIP: None
  selector:
    app: nginx
  ports:
  - name: web
    port: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.2
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: nginx-html
      volumes:
      - name: nginx-html
        persistentVolumeClaim:
          claimName: nginx-pvc
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nginx-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: managed-nfs-storage
  resources:
    requests:
      storage: 1Gi
```
