# 安全机制

 在开启了 TLS 的集群中，**每当与集群交互的时候少不了的是身份认证，使用 kubeconfig（即证书） 和 token 两种认证方式是最简单也最通用的认证方式。**

kubeconfig就是为访问集群所作的配置。

生成的 kubeconfig 被保存到 `~/.kube/config` 文件；配置文件描述了集群、用户和上下文

## 概述

k8s安全框架主要指的是apiserver，执行的kubectl命令都是向apiserver发送的请求，apiserver将这些请求持久化到etcd当中，所以apiserver是各个组件的协调者，也是集群的访问入口。

针对安全也是针对集群入口来做的，也就是apiserver，kubectl请求apiserver其实也是以http显示（http://ip:port/api/pods......）kubectl将请求给封装了，只提供给我们一些参数。 

K8S安全控制框架主要由下面3个阶段进行控制，每一个阶段都 支持插件方式，通过API Server配置来启用插件。
1. **Authentication（认证）**
2. **Authorization（授权）**
3. **Admission Control（准入控制）**
客户端要想访问K8s集群API Server，一般需要证书、Token；如果Pod访问，需要ServiceAccount

![image-20220605011602521](assets/image-20220605011602521.png)

## 认证

对用户身份进行验证。认证方式现共有8种，可以启用一种或多种认证方式，只要有一种认证方式通过，就不再进行其它方式的认证。通常启用**Client 证书验证（https双向认证）**和**jwt token（用于service account）**两种认证方式。



kubeadm启动的k8s集群，apiserver的初始设置中默认支持client验证和service account两种验证方式：

```yaml
cat /etc/kubernetes/manifests/kube-apiserver.yaml

...
spec:
  containers:
  - command:
    - kube-apiserver
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
```



在这个环节中，apiserver会通过client证书或jwt token来识别出请求的用户身份，包括“user”、“group”等，这些信息将在后面的鉴权环节使用

### X509 Client Certs

X509 Client Certs（客户端证书认证），启用此认证方式是通过设置apiserver参数--client-ca-file=/etc/kubernetes/pki/ca.crt。当用户访问API时，apiserver使用/etc/kubernetes/pki/ca.crt（CA证书），通过TLS双向认证验证客户端证书是否由它签发。

```shell
cat /etc/kubernetes/manifests/kube-apiserver.yaml

...
spec:
  containers:
  - command:
    - kube-apiserver
    - --client-ca-file=/etc/kubernetes/pki/ca.crt #集群组件间通讯用的证书都由集群根CA签发，如controller-manager、scheduler、kube-proxy、kubelet等组件
```

证书中有两个身份凭证相关的重要字段：

- CN：apiserver在认证过程中将其作为用户（user）
- O：apiserver在认证过程中将其作为组（group）

![image-20220605011620063](D:%5Clinuxnotes%5CK8S%5C13.%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6.assets%5Cimage-20220605011620063.png)



### Service Accout Tokens

上面讲的是k8s各种组件访问apiserver，那么pod中的容器访问api server，因为pod的创建、销毁都是动态的，所以无法为它手动颁发证书，k8s使用**serviceaccount解决pod访问api server的认证问题**

当创建一个namespace时，会同时在该namespace下生成名为default的sa和对应的secret

sa包含三个部分：**token、ca.crt、namespace**

```shell
#进入容器
kubectl exec  kube-proxy-cmzp6 -n=kube-system -it -- /bin/sh
cd /run/secrets/kubernates.io/serviceaccount/
ls #里面有ca.crt(根的证书)   namespace  token 3个文件

#token是使用 API Server 私钥签名的 JWT（json web token）。用于访问API Server时，Server端认证
#ca.crt，根证书(是k8s中私有的)。用于Client端验证API Server发送的证书
#namespace, 标识这个service-account-token的作用域名空间

service Account密钥对 sa.key sa.pub
提供给 kube-controller-manager使用，kube-controller-manager通过 sa.key 对 token 进行签名,
master 节点通过公钥 sa.pub 进行签名的验证 如 kube-proxy 是以 pod 形式运行的, 在 pod 中, 
直接使用 service account 与 kube-apiserver 进行认证, 此时就不需要再单独为 kube-proxy 创建证书了,
会直接使用token校验。
```



总结：

![image-20220605011636265](D:%5Clinuxnotes%5CK8S%5C13.%E5%AE%89%E5%85%A8%E6%9C%BA%E5%88%B6.assets%5Cimage-20220605011636265.png)

## 鉴权

上面的认证过程只是确认了双方可以通信，而鉴权是确定请求方可以访问哪些资源。现在一般用的授权方式是RBAC。

优点：

- 整个RBAC由几个API对象（role、clusterrole、rolebinding、clusterrolebinding）完成，同其他API对象一样可以用kubectl或API进行操作
- 可以在运行时进行调整，不用重启API Server

### RBAC

从1.6版本起，Kubernetes 默认启用RBAC访问控制策略。

#### 角色

- Role、ClusterRole：是一组权限的集合，这里的权限都是许可形式的，**不存在拒绝的规则**，**Role只作用于命名空间内，ClusterRole作用于整个集群**


角色绑定：

- RoleBinding、ClusterRoleBinding：把角色绑定到目标上，绑定目标可以是**User、Group或者Service Account**，从而让这些目标拥有该角色的权限


##### Role

允许“被作用者”，对pod-reader这个namespace下的pod做"get", "watch", "list"操作：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
    namespace: default
    name: pod-reader
rules: #定义权限
- apiGroups: [""]  # 空字符串表示核心API群
  resource: ["pods"] #资源类型："services", "endpoints", "pods","secrets","configmaps","crontabs","deployments","jobs","nodes","rolebindings","clusterroles","daemonsets","replicasets","statefulsets","horizontalpodautoscalers","replicationcontrollers","cronjobs"
  verbs: ["get", "watch", "list"] #对资源对象的操作："get", "list", "watch", "create", "update", "patch", "delete", "exec"
```

那么这个“被作用者”要如何指定？这就需要通过RoleBinding实现

##### ClusterRole

想要跨namespace可以用clusterrole

因其集群级别的范围，还可以用于以下**特殊元素**的授权：

- 集群范围的资源，例如Node
- 非资源型的路径，例如/healthz
- 包含全部命名空间的资源，例如pods

允许“被作用者”，对所有secrets做"get", "watch", "list"操作：

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: secret-reader
  # ClusterRole不受限于命名空间，所以省略了namespace name的定义
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]
```

##### k8s内置的角色

k8s预定好了四个集群角色供用户使用，使用kubectl get clusterrole查看，其中systemd:开头的为系统内部使用。（以system开头的是绝对不能删除的，删除了可能导致集群异常）

- **cluster-admin**  超级管理员，对集群所有权限（在部署dashboard的时候，先创建sa，然后将sa绑定到角色cluster-admin，最后获取到token，这就使用了内置的cluster-admin ）
- **admin**   主要用于授权命名空间所有读写权限
- **edit**   允许对命名空间大多数对象读写操作，不允许查看或者修改角色、角色绑定。
- **view** 允许对命名空间大多数对象只读权限，不允许查看角色、角色绑定和Secret

```shell
kubectl get clusterrole
NAME                                                                   CREATED AT
admin                                                                  2021-05-20T07:04:01Z
edit                                                                   2021-05-20T07:04:01Z
cluster-admin                                                          2021-05-20T07:04:01Z
view                                                                   2021-05-20T07:04:01Z
```

##### 角色绑定(RoleBinding)和集群角色绑定(ClusterRoleBinding)

在default命名空间中把pod-reader角色授予用户jane，可以让jane用户读取default命名空间的Pod

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects: #授权的目标，即“被作用者”
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef: #引用之前定义好的Role，这样Role和“被作用者”就绑定在一起
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

允许manager组的用户读取任意namespace中的secret

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: read-secrets-global
subjects:
- kind: Group
  name: manager
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: secret-reader
  apiGroup: rbac.authorization.k8s.io
```

**rolebinding同样可以引用clusterrole（但clusterrolebinding只能绑定clusterrole）**，当绑定clusterrole时，subjects的权限会限定于rolebinding定义的namespace，想跨namespace需要使用clusterrolebinding

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: lol
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

##### resources

k8s的一些资源一般以其名称字符串来表示，这些字符串一般在API的URL出现，同时这些资源也会包含子资源，例如logs资源就是pod的子资源，API的URL如下：

```shell
GET /api/v1/namespaces/{namespace}/pods/{name}/log
```

如果要在RBAC授权控制这些子资源的访问权限，可用斜线/来分割资源和下级资源，以下是授权让某个主体同时能够读取Pod和Pod log

```yaml
rules:
- apiGroups: [""]
  resources: ["pods", "pods/log"]
  verbs: ["get", "list"]
```

资源还可以通过名字(ResourceName)进行引用,资源还可以通过名字(ResourceName)进行引用

```yaml
rules:
- apiGroups: [""]
  resources: ["configmap"]
  resourceNames: ["my-configmap"]
  verbs: ["update", "get"]
```

#### 账号

前面rolebinding定义了“被作用者”subjects字段，它的类型是user，那么这个user是从哪来的？

其实k8s本身不提供用户管理能力，大多数时候不太使用用户这个功能，而是直接使用 Kubernetes 里的“内置用户”——ServiceAccount。

**ServiceAccount 分配权限的过程：**

创建sa、role、rolebinding

查看一下这个 ServiceAccount 的详细信息：

```shell

$ kubectl get sa -n mynamespace -o yaml
- apiVersion: v1
  kind: ServiceAccount
  metadata:
    creationTimestamp: 2018-09-08T12:59:17Z
    name: example-sa
    namespace: mynamespace
    resourceVersion: "409327"
    ...
  secrets:
  - name: example-sa-token-vmfg6
```

可以看到k8s会自动给sa创建一个secret，这个secret就是sa用来跟apiserver进行交互的授权文件，我们一般称为Token，

当pod引用这个sa，这个sa的secret就会自动挂载到容器的/var/run/secrets/kubernetes.io/serviceaccount 目录下

```shell

$ kubectl describe pod sa-token-test -n mynamespace
Name:               sa-token-test
Namespace:          mynamespace
...
Containers:
  nginx:
    ...
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from example-sa-token-vmfg6 (ro)
```

进入容器查看这个目录的文件

```shell

$ kubectl exec -it sa-token-test -n mynamespace -- /bin/bash
root@sa-token-test:/# ls /var/run/secrets/kubernetes.io/serviceaccount
ca.crt namespace  token
```

容器里的应用，就可以使用这个 ca.crt 来访问 APIServer 了

如果一个pod没有声明sa，k8s会在它的namespace下创建一个名为default的sa，然后分配给这个pod。

```shell
kubectl get sa
NAME      SECRETS   AGE
default   1         62d

kubectl get po nginx-7975c75659-f2fqd -oyaml|grep "serviceAccount"
  serviceAccount: default
  serviceAccountName: default
```

但在这种情况下，这个默认 ServiceAccount 并没有关联任何 Role。也就是说，此时它有访问 APIServer 的绝大多数权限。

所以，在生产环境中，我强烈建议你为所有 Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。



Kubernetes 还拥有“用户组”（Group）的概念，实际上，一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字是：

```shell
system:serviceaccount:<Namespace名字>:<ServiceAccount名字>
```

而它对应的内置“用户组”的名字，就是：

```shell
system:serviceaccounts:<Namespace名字>
```

比如，role授权给mynamespace 里的所有 ServiceAccount，就用到了“用户组”概念

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts:mynamespace
  apiGroup: rbac.authorization.k8s.io
```

role授权给整个系统 里的所有 ServiceAccount

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts
  apiGroup: rbac.authorization.k8s.io
```

**在 Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。**

除此之外，Kubernetes 还提供了四个预先定义好的 ClusterRole 来供用户直接使用：cluster-admin；admin；edit；view。



**ServiceAccount 主要是用于解决Pod在集群中的身份认证问题，一般用于集群内部 Pod 进程使用，和 api-server 交互，而普通用户一般用于 kubectl 或者 REST 请求使用**



k8s并不会提供用户管理，那么user指定的用户从哪里来？k8s组件（kubectl、kube-proxy）或是其他自定义用户在向ca申请证书时，需要提供一个证书请求文件：

```yaml
{
  "CN": "admin",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "CN",
      "ST": "Beijing",
      "L": "Beijing",
      "O": "system:masters",
      "OU": "Kubernetes-manual"
    }
  ]
}
```

api server会把客户端的证书的`CN`字段作为user，把`name.O`作为group

kubelet使用TLS Bootstaping认证时，k8s会为token绑定一个默认的user和group

pod使用sa认证时，service-account-token中的JWT会保存user信息

有了用户信息，就可以授权

#### 常用权限

允许读取核心API组中Pod的资源：

```yaml
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
```

允许读写"extensions"和"apps"两个API组中的deployment资源

```yaml
rules:
- apiGroups: ["extensions", "apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

允许读写pods及读写jobs

```yaml
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["batch", "extensions"]
  resources: ["jobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
```

允许读取一个名为my-config的ConfigMap(必须绑定到一个RoleBinding来限制到一个namespace下的ConfigMap)：

```yaml
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  resourceNames: ["my-config"]
  verbs: ["get"]
```

读取核心组的node资源(Node属于集群级别的资源，必须放在ClusterRole中，并使用ClusterRoleBinding进行绑定)：

```yaml
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "list", "watch"]
```

允许对非资源端点/healthz及其所有子路径进行GET/POST操作(必须使用ClusterRole和ClusterRoleBinding)：

```yaml
rules:
- nonResourceURLs: ["/healthz", "/healthz/*"]
  verbs: ["get", "post"]
```

### kubectl认证授权

kubeadm初始化完后，会提示以下内容：

```shell
To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf
```

这些信息告诉我们怎么配置kubeconfig文件，配置完后kubectl就可以直接使用$HOME/.kube/config的信息访问k8s集群

看一下config内容：

```yaml
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1URXhNekEzTkRBek0xb1hEVE14TVRFeE1UQTNOREF6TTFvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBUG4xCnZqcXI5dFdvckxxSDZROVc2UmVvT1QzQXh6eENjTVkzVmE2c2RoNXgwLzEyckdUMm53dlNyNWxvNFF2ZEZRWUIKajB2bXBic1oxM0NJdEJvNHlnaUVMV0hEdjJ1K2lEYytSaFdhU1NBbDVIbGtxNndVVjd3TlkvY3ZYUEN0eWNrZApjMi8xSEM2OS85bVduQnNldmdoNHIrTDdyS0pOY3A0UVBQZ1pvYVgvZUNFZ2NBZEJ3T3RrUGZQTG01Lys3SVpEClVNeUpHSWRtekl6V1RGeVMwNkJGa3lGVFpYeDA4YlF0eVNyUW4wbG55RHNrSWhiZGZTaGVodmVnbHJ0S1NyV20KanVEeE1vZ0FSbFBIZi84TWZ4T2FoVmtBNy8yeFFPUVlzVmdaSEovc3JjU3BzVU9uNVUySjRmV0wvS3hLa1V3dwpJeUhhVWFUZGhCRzdDeDdRWVljQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZMay91NExzN1pYWkhYVDk1MkJBS2RNV2tZOHJNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBb0JrRDJJZUlQcXp6OS8wRDJ1d2VaOUlYVm1vM3ZvNFFkaDB1dEtTaFBOaCtHQ2pFVgpKT1U5MWRMVHZ3VURKd3l3c3VkSmFxUVU3aHl0WUtYQUo4QndSVTVZMmRudzJ4c0Zqck9hT2xjemVrS0FSTnlICjRPREFuUGtpUHJwb1Fad0FiK3M1Q0VMNHpmOWZFdU95L0dXTDNMblkvSWlSRkRLOTRvWnlRSWt1SGNmZXl3TjEKY2F6dUxZd3BINkl6NFBnRmU5cTRxWWNmTEpqT3pBUWN4OW45OXhsZTRPWWVKNHNtREdOKzNtRnNrM3FmMWpsMAptajhJSjNUQmpDdSt1dEMrc2dYdENucmMrcTJneEpPMnVoYWxhTmtQcnU2SXZRUUNkb2h5c1hMbWx2TDlManN2Ck50cVhQb1FpNU5GVk5Nb2VYVzl5Yi9aNmpFSkJmRWp4OUlzRwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    server: https://192.168.101.200:16443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    namespace: ingress-nginx
    user: chh
  name: kubernetes
current-context: kubernetes
kind: Config
preferences: {}
users:
- name: chh
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURsakNDQW42Z0F3SUJBZ0lVWHBsbnllRmdnd3lZN0hMMzlWVkhhc04rYTBvd0RRWUpLb1pJaHZjTkFRRUwKQlFBd0ZURVRNQkVHQTFVRUF4TUthM1ZpWlhKdVpYUmxjekFlRncweU1qQXpNRFF4TlRVNU1EQmFGdzB5TXpBegpNRFF4TlRVNU1EQmFNSFF4Q3pBSkJnTlZCQVlUQWtOT01SQXdEZ1lEVlFRSUV3ZENaV2xxYVc1bk1SQXdEZ1lEClZRUUhFd2RDWldscWFXNW5NUmN3RlFZRFZRUUtFdzV6ZVhOMFpXMDZiV0Z6ZEdWeWN6RWFNQmdHQTFVRUN4TVIKUzNWaVpYSnVaWFJsY3kxdFlXNTFZV3d4RERBS0JnTlZCQU1UQTJOb2FEQ0NBU0l3RFFZSktvWklodmNOQVFFQgpCUUFEZ2dFUEFEQ0NBUW9DZ2dFQkFMeTNBeWQ0SlEySVZyUW53THh2N0g0S1hiSThkWjIvQ1VUanVqdWVRcmxkCnNXSjR1d1QwcWdkWkVDK3YyaHZERE5LVjZDRFhmSm4rdlM5WVdQSDZUV2VUN0hndVIxbkk5YTRtVHBJbVFjMGQKT2ZGTEYvWjR5MGEyWjlYU3lDbUJ6Z2tEOUtPdmhoTUM0a2R2c0Q4Z0JoOW9wekloUHRsOUp3SkZydzlsaXdqMAprbTgxVlhNTEhwSHNiQnM4dXRqMVI1TTZ5N0lNSVZrSGcyVzRYK2FYYkV5ZWFZV3ExaEFOMHFJcXMvUFFXbzZOCjhXTUFkeGJyS2ZVSldBMnJ0dTBOcXEvcklNc2Z5TldCa2JLdXcybjdrajluOHNSU0tUOXZhcTZQSHpMbGVOL1kKQWpFRGRhaTF2L1NmOXFnTndtenF5alU4UGVpbC83Yk9XeWVNZE9YWGVVc0NBd0VBQWFOL01IMHdEZ1lEVlIwUApBUUgvQkFRREFnV2dNQjBHQTFVZEpRUVdNQlFHQ0NzR0FRVUZCd01CQmdnckJnRUZCUWNEQWpBTUJnTlZIUk1CCkFmOEVBakFBTUIwR0ExVWREZ1FXQkJSTHlhNTBuU2hCUXFJS0Q5RUlBVVRicFBSVGxEQWZCZ05WSFNNRUdEQVcKZ0JTNVA3dUM3TzJWMlIxMC9lZGdRQ25URnBHUEt6QU5CZ2txaGtpRzl3MEJBUXNGQUFPQ0FRRUFMWFg4T0ZPNgpJNlBjU3RrNmxxcWtLQU5LQSttb2tjcUREVFRHemdWSHRVWlZaaXVUdjZGbGd3NWxmdlhQa1lVU3NpdmszKzFtCkRXeFhOUm9Xb2ZsYlNSd0xIbGdtU2RIS1EvVmdndUcwOUM1QkFNQUlieVE0c2NRbDExN1R6OVFzMlRZRUEzMjUKMS9GMUh6N2ljc2FIcU9TZk4rL2pJVDdsNFF1bERUVXB4SEU1WjAwVWJYQUY2Rjdwcm1ySFFWSkM4VzFtaktRcwozQmNuaVZ0K3FhR2R2SE1lNW5aVGZ5R1RLcFNya0x2NlAyQkUwaG1GNzJoOFlZM2xpbGhMeGJDMGxWK2lZaGd6Ck1qU1NnUk5GUFFqaEpJRCtuK0hGY2k4VWdtT1RkOGNoWnNJb1Q1MWhzZWdjYTUxRld0aml6QjNJZVlDRDRYQ1QKb2RVOERtcS9tOUw5T0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: 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
```

从信息可以看出kubectl在请求中使用了client证书验证的方式

apiserver首先会使用--client-ca-file配置的ca证书去验证kubectl提供的证书的有效性：

```shell
echo 证书内容|base64 -d > kubectl.crt
openssl verify -CAfile /etc/kubernetes/pki/ca.crt kubectl.crt 
kubectl.crt: OK
```

除了认证，还会取出必要的信息来给鉴权阶段使用：

```shell
openssl x509 -in kubectl.crt -text
Certificate:
    Data:
        ...
        Subject: C=CN, ST=Beijing, L=Beijing, O=system:masters, OU=Kubernetes-manual, CN=chh
        ...
-----END CERTIFICATE-----
```

- CN：apiserver在认证过程中将其作为用户（user）
- O：apiserver在认证过程中将其作为组（group）

查找绑定了system:masters组的clusterrolebinding

```yaml
kubectl get clusterrolebinding -oyaml|grep -B38 system:masters

- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    ...
    name: cluster-admin
    ...
  roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: cluster-admin
  subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: Group
    name: system:masters
```

查看cluster-admin这个clusterrole

```yaml
kubectl get clusterrole cluster-admin -oyaml

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  ...
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
```



### 实践

#### 创建只能访问某个namespace的用户

我们来创建一个 User Account，只能访问 kube-system 这个命名空间：

- username: haimaxy
- group: youdianzhishi

**第1步：创建用户凭证**

`Kubernetes`没有 User Account 的 API 对象。这里我们来使用`OpenSSL`证书来创建一个 User：

- 给用户 haimaxy 创建一个私钥，命名成：haimaxy.key：

  ```shell
  $ openssl genrsa -out haimaxy.key 2048
  ```

- 使用我们刚刚创建的私钥创建一个证书签名请求文件：haimaxy.csr，要注意需要确保在`-subj`参数中指定用户名和组(CN表示用户名，O表示组)：

  ```shell
  $ openssl req -new -key haimaxy.key -out haimaxy.csr -subj "/CN=haimaxy/O=youdianzhis"
  ```

- 然后找到我们的`Kubernetes`集群的`CA`，我们使用的是`kubeadm`安装的集群，`CA`相关证书位于`/etc/kubernetes/pki/`目录下面，如果你是二进制方式搭建的，你应该在最开始搭建集群的时候就已经指定好了`CA`的目录，我们会利用该目录下面的`ca.crt`和`ca.key`两个文件来批准上面的证书请求

- 生成最终的证书文件，我们这里设置证书的有效期为500天：

  ```shell
  $ openssl x509 -req -in haimaxy.csr -CA /etc/kubernetes/pki/ca.crt -CAkey /etc/kubernetes/pki/ca.key -CAcreateserial -out haimaxy.crt -days 500
  ```

  现在查看我们当前文件夹下面是否生成了一个证书文件：

  ```shell
  $ ls
  haimaxy.csr haimaxy.key haimaxy.crt
  ```

- 现在我们可以使用刚刚创建的证书文件和私钥文件在集群中创建新的凭证和上下文(Context):

  ```shell
  kubectl config set-credentials haimaxy --client-certificate=haimaxy.crt  --client-key=haimaxy.key
  ```

我们可以看到一个用户`haimaxy`创建了，然后为这个用户设置新的 Context:

```shell
$ kubectl config set-context haimaxy-context --cluster=kubernetes --namespace=kube-system --user=haimaxy
```

**第2步：创建角色**

创建一个允许用户操作 Deployment、Pod、ReplicaSets 的角色，如下定义：(haimaxy-role.yaml)

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: haimaxy-role
  namespace: kube-system
rules:
- apiGroups:
  - ""
  - extensions
  - apps
  resources:
  - deployments
  - replicasets
  - pods
  verbs: ['*']
```

**第3步：创建角色权限绑定**

将上面的 haimaxy-role 角色和用户 haimaxy 进行绑定:(haimaxy-rolebinding.yaml)

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: haimaxy-rolebinding
  namespace: kube-system
subjects:
- kind: User
  name: haimaxy
  apiGroup: ""
roleRef:
  kind: Role
  name: haimaxy-role
  apiGroup: ""
```

先切换到haimaxy-context

```shell
kubectl config use-context haimaxy-context
```

或直接用--context参数

```shell
kubectl get pods --context=haimaxy-context
```

#### 创建一个只能访问某个 namespace 的ServiceAccount

有两种登陆认证方式可供选择：Kubeconfig 和 Token 方式（其实两种方式都需要服务账号的 Token）

在default空间创建一个sa

```shell
kubectl create sa kube-dashboard-reader
```

将系统自带的 ClusterRole：view 角色绑定到上一步创建的服务账号，授予集群范围的资源只读权限

```shell
kubectl create clusterrolebinding kube-dashboard-reader --clusterrole=view --serviceaccount=default:kube-dashboard-reader
```

获取服务账号的 token

```shell
kubectl describe secret kube-dashboard-reader
```

kubeconfig方式登录：

```yaml
在 Kubeconfig 的 Users 下 User 部分添加:
...
users:
- name: kubernetes-admin
  user:
    client-certificate-data: ...
    client-key-data: ...
    token: <这里为上面获取的 Token...>
```

token方式登录：

将获取的token填入

#### 授权用户连接ack托管版

需求：



创建RAM用户并授权：

由于用户最终只能通过控制台才能获取`config`文件，因此需要创建一个可以登录控制台，并对`ack`具有只读权限的用户

<img src="assets/image-20220716230156991.png" alt="image-20220716230156991" style="zoom:67%;" />

对用户进行授权

<img src="assets/image-20220716230228529.png" alt="image-20220716230228529" style="zoom:67%;" />

ack集群中创建clusterrole

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: project-rbac-role
rules:
  - apiGroups: ["extensions", "apps"]
    resources: ["deployments"]
    verbs: ["create", "delete"]
  - apiGroups: [""]
    resources: ["service"]
    verbs: ["create", "delete"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingress"]
    verbs: ["create", "delete"]
 
kubectl apply -f clusterrole.yaml
```

控制台授权绑定：

在`ack`的控制台对`ram`用户授权，点击到某个集群——>安全管理——>授权，选择上面创建的`ram`子账号，点击“管理权限”，选择集群（这里虽然已经进入到了特定集群，但依然可以选择其他集群，或许是`ack`产品设计上有点不合理），选择对应要授权的命名空间，访问权限选择“自定义”，然后下拉就能找到上面创建的名为`project-rbac-role`的`ClusterRole`

![img](assets/a58afaf19576ef5d9343b665422256db.webp)

实际上这里的操作是对应的在`ack`集群中创建了一个`Rolebinding`绑定集群的`ClusterRole`

获取连接集群的kubeconfig：

用上面创建的`readonly`这个`ram`用户登录到`ack`的每个集群控制台，获取这个`ram`用户的集群连接信息即`config`内容，保存成文件即可

## 准入控制

一个控制链，用于拦截请求的一种方式。