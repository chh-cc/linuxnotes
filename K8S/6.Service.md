# Service

## 为什么用service

每个pod都有自己的ip地址，也可以被访问，可是pod生命周期短暂，一旦被更新、被删除后重新创建，**ip地址就改变了**，而且一组pod实例之间总会有负载均衡的需求。

Service是一种可以**访问 （逻辑上的）一组Pod的策略**， Service**通过 Label Selector**选取合适的pod，构建出一个**endpoint，即负载均衡列表**。实际运用中一般会为同一个微服务的pod实例都打上类似`app=xxx`的标签，同时为该微服务创建一个标签为`app=xxx`的service。

![image-20220604231311906](D:%5Clinuxnotes%5CK8S%5C6.Service.assets%5Cimage-20220604231311906.png)

service只提供四层负载功能，没有7层负载功能。

## 定义一个service

以kuboard的service为例：

```yaml
vim kuboard-svc.yaml
  
apiVersion: v1
kind: Service
metadata:
  name: kuboard
  namespace: kube-system
spec:
  clusterIP: 10.111.246.18
  externalTrafficPolicy: Cluster
  ports:
    - name: http
      nodePort: 32567
      port: 80 #service的端口
      protocol: TCP
      targetPort: 80 #代理的pod端口
  selector: #只代理携带了以下标签的 Pod
    k8s.kuboard.cn/layer: monitor
    k8s.kuboard.cn/name: kuboard
  sessionAffinity: None
  type: NodePort
```

创建svc

```shell
kubectl create -f kuboard-svc.yaml
```

查看自动关联生成的 **Endpoints**

```shell
kubectl get ep kuboard -n kube-system
NAME      ENDPOINTS           AGE
kuboard   172.171.14.233:80   62d

#创建一个svc的同时会创建一个同名的endpoint，endpoint记录了svc关联的pod的ip地址；pod更新后ip地址改变，endpoint的地址也会刷新
```

通过该 Service 的 VIP ，就可以访问到它所代理的 Pod 了： 

```shell
#service port为80，nodeport为32567
kubectl get svc kuboard -n kube-system -owide
NAME      TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE   SELECTOR
kuboard   NodePort   10.111.246.18   <none>        80:32567/TCP   62d   k8s.kuboard.cn/layer=monitor,k8s.kuboard.cn/name=kuboard

#在集群内部访问clusterip
[root@k8s-master01 ~]# curl 10.111.246.18

#pod之间访问
curl http://kuboard
```

## service原理

Service 是由 kube-proxy 组件，加上 iptables 来共同实现的。

假如创建了名为hostname、clusterip为10.0.1.175、端口为80的service，kube-proxy会感知到这个service对象的添加，就会在宿主机创建一条iptables规则（可以通过 iptables-save 看到它）

```shell
-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
```

这条 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。

KUBE-SVC-NWV5X2332I4OT4T3 规则：

```shell
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
 
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
 
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR
```

这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。

当你的宿主机上有大量 Pod 的时候，**成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源**，甚至会让宿主机“卡”在这个过程中。而 **IPVS 模式的 Service**就解决了这个问题。



IPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上**创建一个虚拟网卡（叫作：kube-ipvs0）**，并为它分配 Service VIP 作为 IP 地址，如下所示： 

```shell
# ip addr
  ...
  73：kube-ipvs0：<BROADCAST,NOARP>  mtu 1500 qdisc noop state DOWN qlen 1000
  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
  inet 10.0.1.175/32  scope global kube-ipvs0
  valid_lft forever  preferred_lft forever
```

kube-proxy 会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 **IPVS 虚拟主机**，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。

```shell
# ipvsadm -ln
 IP Virtual Server version 1.2.1 (size=4096)
  Prot LocalAddress:Port Scheduler Flags
    ->  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     
  TCP  10.102.128.4:80 rr
    ->  10.244.3.6:9376    Masq    1       0          0         
    ->  10.244.1.7:9376    Masq    1       0          0
    ->  10.244.2.3:9376    Masq    1       0          0
```

这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。 

IPVS 不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。

不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。 

## service类型

东西向的流量调用，即服务间的调用，使用ClusterIP

南北向的流量调用，即外部请求访问k8s集群，使用Nodeport、LoadBalancer

### ClusterIP

ClusterIP：默认值，在**集群内部使用**的IP，其他pod可以通过这个ip访问到该pod组，集群外部访问不了

service只要创建完，就会在CoreDNS中添加一个解析记录，格式为**SVC_NAME.NS_NAME.DOMAIN.LTD**，默认的集群service 的A记录：**svc.cluster.local**，也可以用DNS名访问service

![image-20220604234108788](D:%5Clinuxnotes%5CK8S%5C6.Service.assets%5Cimage-20220604234108788.png)

配置文件：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: service-python #service名称
spec:
  #clusterIP: 10.97.97.97 #不指定会自动分配
  ports:
  - port: 3000 #service端口
    protocol: TCP
    targetPort: 443 #后端pod端口
  selector:
    run: pod-python
  type: ClusterIP #service类型
```

#### Headless Service（结合statefulset使用）

无头服务，是一种特殊的ClusterIP。它的 clusterIP 字段的值是：None，即：这个 Service，没有一个 VIP 作为“头”。

所以这个service被创建后**不会被分配一个VIP**，而是以DNS记录的形式暴露它所代理的Pod。它所代理的所有 Pod 的 IP 地址，都会被 绑定一个这样格式的 **DNS 记录：**

```shell
<pod-name>.<svc-name>.<namespace>.svc.cluster.local
# 如
web-1.nginx.default.svc.cluster.local
```

有了这个记录，只要知道pod的名字，和它对应的service名字，就能通过该记录访问到pod的ip地址



定义headless service

```yaml
vim nginx-sts.yaml

apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  clusterIP: None #此参数表示headless service
  selector: #定义了selector，Endpoint控制器在API中创建了Endpoints记录，并且修改DNS配置返回A记录（地址），通过这个地址直接到达 Service 的后端 Pod 上
    app: nginx
  ports:
  - name: web
    port: 80
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:  
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.15.2
        ports:
        - containerPort: 80
          name: web
```

查看svc

```shell
kubectl get svc
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.0.0.1     <none>        443/TCP   3d10h
nginx        ClusterIP   None         <none>        80/TCP    2m48s
```

dns解析

```shell
nslookup web-01.nginx.default.svc.cluster.local
```

### NodePort

NodePort：在所有安装了kube-proxy的节点打开一个端口来暴露服务，**集群外部**通过NodeIP:NodePort访问pod的服务，**nodeport端口号默认范围是30000~32767**

访问流程：

```shell
client--->nodeip:port--->service ip:port--->podip:port
```

![image-20220604235547232](D:%5Clinuxnotes%5CK8S%5C6.Service.assets%5Cimage-20220604235547232.png)

查看svc

```shell
kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)         AGE
kubernetes-dashboard        NodePort    10.111.4.67     <none>        443:30543/TCP   21d
```

编辑nginx-svc

```shell
kubectl edit svc nginx-svc
```

修改配置

```yaml
apiVersion: v1
kind: Service
metadata:
  name: service-python
spec:
  ports:
  - port: 3000 #service端口
    protocol: TCP
    targetPort: 443 #后端pod端口
    nodePort: 30080 #node节点的端口，不指定会自动分配
  selector:
    run: pod-python
  type: NodePort
```

查看svc

```shell
kubectl get svc
```

### LoadBalancer

LoadBalancer：使用云供应商的负载均衡器公开服务，把每个NodeIP:NodePort作为后端添加进去

<img src="D:%5Clinuxnotes%5CK8S%5C6.Service.assets%5Cimage-20220604235728542.png" alt="image-20220604235728542" style="zoom: 50%;" />

配置文件：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: service-python
spec:
  ports:
  - port: 3000
    protocol: TCP
    targetPort: 443
    nodePort: 30080
  selector:
    run: pod-python
  type: LoadBalancer
```

### ExternalName

把集群外部的服务引入到集群内部来，在集群内部直接使用。

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  type: ExternalName
  externalName: www.baidu.com
```

my-service.default.svc.cluster.local会解析为www.baidu.com

## service生产小技巧 通过svc来访问非K8s上的服务

我们创建svc时，如果没有定义这个selector，那么系统是不会自动创建endpoint的，我们可不可以手动来创建这个endpoint呢？答案是可以的，在生产中，我们可以**通过创建不带selector的Service，然后创建同样名称的endpoint，来关联K8s集群以外的服务**，这个具体能带给我们运维人员什么好处呢，就是我们可以直接复用K8s上的ingress，来访问K8s集群以外的服务，省去了自己搭建前面Nginx代理服务器的麻烦

node2节点运行一个简单的web服务器

```shell
[root@k8s-node02 ~]# python -m SimpleHTTPServer 9999
Serving HTTP on 0.0.0.0 port 9999 ...

```

创建svc和endpoint（Service和Endpoints的名称必须一致）

```yaml
# 注意我这里把两个资源的yaml写在一个文件内，在实际生产中，我们经常会这么做，方便对一个服务的所有资源进行统一管理，不同资源之间用"---"来分隔
apiVersion: v1
kind: Service
metadata:
  name: mysvc
  namespace: default
spec:
  type: ClusterIP
  ports:
  - port: 80
    protocol: TCP

---

apiVersion: v1
kind: Endpoints
metadata:
  name: mysvc
  namespace: default
subsets:
- addresses:
  - ip: 192.168.101.149
    nodeName: 192.168.101.149
  ports:
  - port: 9999
    protocol: TCP
```

创建并测试

```shell
kubectl  apply -f mysvc.yaml 

kubectl get svc,ep|grep mysvc
service/mysvc        ClusterIP   10.105.149.120   <none>        80/TCP           77s
endpoints/mysvc            192.168.101.149:9999                                             77s

curl 10.105.149.120
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 3.2 Final//EN"><html>
<title>Directory listing for /</title>
<body>
<h2>Directory listing for /</h2>
<hr>
<ul>
<li><a href=".bash_history">.bash_history</a>
<li><a href=".bash_logout">.bash_logout</a>
<li><a href=".bash_profile">.bash_profile</a>
<li><a href=".bashrc">.bashrc</a>
<li><a href=".cache/">.cache/</a>
<li><a href=".config/">.config/</a>
<li><a href=".cshrc">.cshrc</a>
<li><a href=".dbus/">.dbus/</a>
<li><a href=".esd_auth">.esd_auth</a>
<li><a href=".ICEauthority">.ICEauthority</a>
<li><a href=".local/">.local/</a>
<li><a href=".pki/">.pki/</a>
<li><a href=".ssh/">.ssh/</a>
<li><a href=".tcshrc">.tcshrc</a>
<li><a href=".viminfo">.viminfo</a>
<li><a href="anaconda-ks.cfg">anaconda-ks.cfg</a>
<li><a href="Desktop/">Desktop/</a>
<li><a href="Documents/">Documents/</a>
<li><a href="Downloads/">Downloads/</a>
<li><a href="k8s-ha-install/">k8s-ha-install/</a>
<li><a href="kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm">kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm</a>
<li><a href="kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm">kernel-ml-devel-4.19.12-1.el7.elrepo.x86_64.rpm</a>
<li><a href="Music/">Music/</a>
<li><a href="new.yaml">new.yaml</a>
<li><a href="original-ks.cfg">original-ks.cfg</a>
<li><a href="Pictures/">Pictures/</a>
<li><a href="Public/">Public/</a>
<li><a href="Templates/">Templates/</a>
<li><a href="Videos/">Videos/</a>
</ul>
<hr>
</body>
</html>
```

在node02终端可以看到有访问日志

```shell
192.168.101.145 - - [24/Apr/2022 21:15:44] "GET / HTTP/1.1" 200 -
```

## 生产中service的调优

对于`Service`, 如果指定类型为 `NodePort`, 那么这个端口会在集群的所有 `Node` 上打开，即使这个`Node` 上面没有这个pod

当某个节点上没有`pod`的时候，又去访问ta的这个`NodePort`,能访问到吗？答案是能



调优前

```shell
#查看nginx-pod的信息
kubectl  get pod -o wide
NAME                     READY   STATUS    RESTARTS   AGE   IP                NODE           NOMINATED NODE   READINESS GATES
nginx-66bbc9fdc5-nckkj   1/1     Running   0          94s   172.169.244.248   k8s-master01   <none>           <none>
#pod的svc
kubectl get svc
NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
nginx-svc    NodePort    10.105.84.116    <none>        3000:30000/TCP   141d
#在node02通过pod的nodeip来访问
curl 192.168.101.145:30000
#可以看到日志显示请求的源ip是master01节点的ip，不是node02的ip
kubectl logs --tail=1 nginx-66bbc9fdc5-nckkj
192.168.101.145 - - [24/Apr/2022:15:56:22 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.29.0" "-"
#再通过没有该pod的nodeip访问，可以发现也能访问
curl 192.168.101.146:30000
#可以看到源ip并非node节点ip
172.169.92.64 - - [24/Apr/2022:16:00:50 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.29.0" "-"
```

流程：

```ruby
          client
             \ ^
              \ \
               v \
 master01 <--- master02
    | ^   SNAT
    | |   --->
    v |
 endpoint


客户端发送 tcp 包 到 master02:nodePort

master02 把客户端源ip地址替换为master02 的ip地址

master02 把请求的目的地ip替换为 pod ip

tcp包被路由到 master01， 接着达到 endpoint(如service)

pod的响应被路由到 master02

master02把pod的响应发送给客户端

可以发现，在这个过程中，客户端的源IP地址丢失了
```

调优

为了解决这个问题， `k8s` 提供了一个功能，通过设置 `externalTrafficPolicy=Local` 可以保留源IP地址。如果没有本地端点，则丢弃发送到节点的数据包，因此您可以在任何数据包处理规则中依赖正确的客户端IP。

```shell
# 设置 service.spec.externalTrafficPolicy 字段如下：
kubectl patch svc nginx-svc -p '{"spec":{"externalTrafficPolicy":"Local"}}'

# 现在通过非pod所在node节点的IP来访问是不通了
curl 192.168.101.146:30000
curl: (7) Failed connect to 192.168.101.146:30000; Connection refused

# 通过所在node的IP发起请求正常
[root@k8s-node02 ~]# curl 192.168.101.145:30000

# 可以看到日志显示的来源IP就是149，这才是我们想要的结果
[root@k8s-master01 ~]# kubectl logs --tail=1 nginx-66bbc9fdc5-nckkj
192.168.101.149 - - [25/Apr/2022:09:17:15 +0000] "GET / HTTP/1.1" 200 612 "-" "curl/7.29.0" "-"

# 去掉这个优化配置也很简单
kubectl patch svc nginx -p '{"spec":{"externalTrafficPolicy":""}}' 
```

