# 集群调度

scheduler是k8s的调度器，把pod分配到集群的节点上，要考虑以下问题：

- 公平：如何保证每个节点都被分配到资源
- 资源高效利用：集群所有资源最大化利用
- 效率：调度的性能好，尽快对大批量的pod完成调度
- 灵活：允许用户根据需求控制调度规则

## 调度过程

predicate：首先是过滤掉不满足条件的节点

```shell
节点资源是否够用？
节点名称是否和pod指定节点名称匹配？
节点port是否和pod的port冲突？
节点是否和pod指定的label匹配？
```

如果predicate过程中没有合适的节点，pod就会一直pending状态，不断重试调度直到找到合适的节点

如果有多个满足的节点，按优先级对节点排序：

```shell
cpu和memory的使用率来决定权重，使用率越低权重越高
cpu和memory的使用率越接近，权重越高，要结合上面的一起使用
节点可以使用镜像的总大小越大，权重越高
```

## 自定义调度

可以编写自己的调度器，通过`spec.schedulername`为pod选择某个调度器

## 亲和性

Affinity分类：

- NodeAffinity（节点亲和力）
  - requiredDuringSchedulingIgnoredDuringExecution：硬亲和力
  - preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量部署或不部署在满足条件的节点上
- PodAffinity（Pod亲和力）
  - requiredDuringSchedulingIgnoredDuringExecution：硬亲和力
  - preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量将a应用和b应用部署在一块
- PodAntiAffinity（Pod反亲和力）
  - requiredDuringSchedulingIgnoredDuringExecution：硬亲和力
  - preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量不要将a应用和b应用部署在一块

### 节点亲和力

节点默认的标签`kubernetes.io/hostname`

pod配置affinity，让pod部署在node01或node02，否则尽量部署在master01

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: kubernetes.io/hostname #master01标签的key
                operator: In
                values:
                - k8s-master01 #master01标签的value
            weight: 1
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname #node01和node02标签的key
                operator: In # In：部署在符合条件的节点；NotIn：不部署；Exists：部署在有这个key的节点上，不需要写values；DoesNotExists：和Exists相反；Gt：大于指定条件，条件为number；Lt：小于指定条件
                values: 
                - k8s-node01 #node01标签的value
                - k8s-node01 #node02标签的value
```

可以看到pod都部署在node01

```shell
[root@k8s-master01 k8s-file]# kubectl get po -owide
NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-f588989b4-5rplt   1/1     Running   0          28s   172.161.125.53   k8s-node01   <none>           <none>
nginx-f588989b4-l5cvk   1/1     Running   0          28s   172.161.125.52   k8s-node01   <none>           <none>
nginx-f588989b4-mhlqx   1/1     Running   0          28s   172.161.125.51   k8s-node01   <none>           <none>
```

编辑depolyment `kubectl edit deploy nginx` 把node01的 `operator: In` 改成 `operator: NotIn` ，可以看到pod变成部署在master01上

```shell
[root@k8s-master01 k8s-file]# kubectl get po -owide
NAME                   READY   STATUS    RESTARTS   AGE   IP                NODE           NOMINATED NODE   READINESS GATES
nginx-dbc5bcff-672x7   1/1     Running   0          15s   172.169.244.193   k8s-master01   <none>           <none>
nginx-dbc5bcff-gr5f2   1/1     Running   0          15s   172.169.244.195   k8s-master01   <none>           <none>
nginx-dbc5bcff-t88pv   1/1     Running   0          15s   172.169.244.194   k8s-master01   <none>           <none>
```

### Pod亲和力

pod配置affinity，和namespace为 `kube-system` 且label为 `k8s-app=calico-kube-controllers` 的pod部署在同一个节点（严谨说是部署在同一拓扑域）

```yaml
spec:
  template:
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app #pod标签的key
                operator: In
                values:
                - calico-kube-controllers #pod标签的value
             namespaces:
               - kube-system #pod的namespace
             topologyKey: kubernetes.io/hostname #拓扑域
```

可以看到pod部署到k8s-node02（calico-kube-controllers这个pod也在该节点）

### Pod反亲和力

不部署在符合条件的节点

```yaml
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - calico-kube-controllers
             namespaces:
               - kube-system
             topologyKey: kubernetes.io/hostname
```

### 亲和性调度策略对比

| 调度策略        | 匹配标签 | 操作符                              | 拓扑域支持 | 调度目标            |
| --------------- | -------- | ----------------------------------- | ---------- | ------------------- |
| nodeaffinity    | 主机     | In,NotIn,Exists,DoesNotExists,Gt,Lt | 否         | 指定主机            |
| podaffinity     | pod      | In,NotIn,Exists,DoesNotExists       | 是         | 与指定pod同一拓扑域 |
| podanitaffinity | pod      | In,NotIn,Exists,DoesNotExists       | 是         | 与指定pod同一拓扑域 |

拓扑域概念：

 K8s 集群中为管理的 nodes 划分的一种“位置”关系，意思为：可以通过在 node 的 labels 信息里面填写某一个 node 属于某一个拓扑。

常见的有三种，这三种在使用时经常会遇到：

第一种，在使用云存储服务的时候，经常会遇到 region，也就是地区的概念，在 K8s 中常通过label ` failure-domain.beta.kubernetes.io/region`来标识。这个是为了标识单个 K8s 集群管理的跨 region 的 nodes 到底属于哪个地区；
第二种，比较常用的是可用区，也就是 available zone，在 K8s 中常通过label ``failure-domain.beta.kubernetes.io/zone` 来标识。这个是为了标识单个 K8s 集群管理的跨 zone 的 nodes 到底属于哪个可用区；
第三种，是 hostname，就是单机维度，是拓扑域为 node 范围，在 K8s 中常通过 label `kubernetes.io/hostname` 来标识，这个在文章最后讲 local pv 的时候，会详细描述。

## Taint和Toleration

### Taint污点

Node被设置上污点之后就和Pod之间存在了一种**相斥的关系**，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去。

一个节点可以打多个污点。



**每个污点的组成如下：**
`key=value:effect`

effect支持如下三个选项：

- NoSchedule：表示k8s不会将Pod调度到具有该污点的Node上
- PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上
- NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去



给master01节点打一个污点，这样master01节点将不会调度运行Pod，即不运行工作负载

```shell
kubectl taint node k8s-master01 master-test=test:NoSchedule
```

查看k8s-master01节点的污点

```shell
kubectl describe node k8s-master01

...
Taints:             master-test=test:NoSchedule
                    node-role.kubernetes.io/master:NoSchedule
...
```

删除taint

```shell
#删除指定key所有的effect
kubectl taint node k8s-master01 master-test-
#删除指定key的指定effect
kubectl taint node node1 key1:NoExecute-
```

### Toleration容忍

要想pod可以调度到打了污点的节点，就要在pod配置容忍

在pod配置文件添加一个容忍

```yaml
spec:
  tolerations:
  - key: "master-test" #污点的key名称
    value: "test" #污点的value
    effect: "NoSchedule" #污点的effect
    operator: "Equal"
    
或
spec:
  tolerations:
  - key: "master-test"
    effect: "NoSchedule"
    operator: "Exists"

或
spec:
  tolerations:
  - key: "master-test"
    value: "test"
    effect: "NoExecute"
    operator: "Equal"
    tolerationSeconds: 60 #多少秒后驱逐pod
```

节点有多个taint的话，每个污点都要容忍才能部署

## 固定节点调度

spec.nodeName：将pod直接调度到指定的node节点，会跳过Scheduler的调度策略，该匹配规则是强制匹配

```yaml
spec:
  template:
    spec:
      nodeName: k8s-node01
```

sepc.nodeSelector：通过k8s的label-selector机制选择节点，由调度器调度策略匹配label，而后调度到目标节点，该匹配规则属于强制约束

```yaml
spec:
  template:
    spec:
      nodeSelector:
        key: value
```

