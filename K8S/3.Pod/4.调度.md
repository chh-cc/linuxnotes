nodeName

指定pod运行在哪个具体node

```yaml
spec:
  nodeName: k8s-node01
```



nodeSelector

指定pod运行在哪个标签的node

给node打标签：

```shell
kubectl label no k8s-node01 disk=ceph
```

```yaml
spec:
  nodeSelector:
    disk: ceph
```



## 亲和性

Affinity分类：

- NodeAffinity（节点亲和力）
  - requiredDuringSchedulingIgnoredDuringExecution：硬亲和力
  - preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量部署或不部署在满足条件的节点上
- PodAffinity（Pod亲和力）
  - requiredDuringSchedulingIgnoredDuringExecution：硬亲和力
  - preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量将a应用和b应用部署在一块
- PodAntiAffinity（Pod反亲和力）
  - requiredDuringSchedulingIgnoredDuringExecution：硬亲和力
  - preferredDuringSchedulingIgnoredDuringExecution：软亲和力，尽量不要将a应用和b应用部署在一块

### 节点亲和力

节点默认的标签`kubernetes.io/hostname`

pod配置affinity，让pod部署在node01或node02，否则尽量部署在master01

```yaml
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: kubernetes.io/hostname #master01标签的key
                operator: In
                values:
                - k8s-master01 #master01标签的value
            weight: 1
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname #node01和node02标签的key
                operator: In # In：部署在符合条件的节点；NotIn：不部署；Exists：部署在有这个key的节点上，不需要写values；DoesNotExists：和Exists相反；Gt：大于指定条件，条件为number；Lt：小于指定条件
                values: 
                - k8s-node01 #node01标签的value
                - k8s-node01 #node02标签的value
```

可以看到pod都部署在node01

```shell
[root@k8s-master01 k8s-file]# kubectl get po -owide
NAME                    READY   STATUS    RESTARTS   AGE   IP               NODE         NOMINATED NODE   READINESS GATES
nginx-f588989b4-5rplt   1/1     Running   0          28s   172.161.125.53   k8s-node01   <none>           <none>
nginx-f588989b4-l5cvk   1/1     Running   0          28s   172.161.125.52   k8s-node01   <none>           <none>
nginx-f588989b4-mhlqx   1/1     Running   0          28s   172.161.125.51   k8s-node01   <none>           <none>
```

编辑depolyment `kubectl edit deploy nginx` 把node01的 `operator: In` 改成 `operator: NotIn` ，可以看到pod变成部署在master01上

```shell
[root@k8s-master01 k8s-file]# kubectl get po -owide
NAME                   READY   STATUS    RESTARTS   AGE   IP                NODE           NOMINATED NODE   READINESS GATES
nginx-dbc5bcff-672x7   1/1     Running   0          15s   172.169.244.193   k8s-master01   <none>           <none>
nginx-dbc5bcff-gr5f2   1/1     Running   0          15s   172.169.244.195   k8s-master01   <none>           <none>
nginx-dbc5bcff-t88pv   1/1     Running   0          15s   172.169.244.194   k8s-master01   <none>           <none>
```

### Pod亲和力

pod配置affinity，和namespace为 `kube-system` 且label为 `k8s-app=calico-kube-controllers` 的pod部署在同一个节点（严谨说是部署在同一拓扑域）

```yaml
spec:
  template:
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app #pod标签的key
                operator: In
                values:
                - calico-kube-controllers #pod标签的value
             namespaces:
               - kube-system #pod的namespace
             topologyKey: kubernetes.io/hostname #拓扑域
```

可以看到pod部署到k8s-node02（calico-kube-controllers这个pod也在该节点）

### Pod反亲和力

不与指定的pod部署在同一个节点，在一些多副本的应用可以让每个副本分布在不同节点

```yaml
spec:
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: k8s-app
                operator: In
                values:
                - calico-kube-controllers
             namespaces:
               - kube-system
             topologyKey: kubernetes.io/hostname
```



## 污点和容忍

这里介绍高级调度的两个特性是节点污点，以及pod对于污点的容忍度，这些特性被用于**限制哪些pod可以被调度到某一个节点**。只有当一个pod容忍某个节点的污点，这个pod才能被调度到该节点。

### Taint污点

Node被设置上污点之后就和Pod之间存在了一种**相斥的关系**，可以让Node拒绝Pod的调度执行，甚至将Node已经存在的Pod驱逐出去。

一个节点可以打多个污点。



**每个污点的组成如下：**
`key=value:effect`

effect支持如下三个选项：

- NoSchedule：表示k8s不会将Pod调度到具有该污点的Node上
- PreferNoSchedule：表示k8s将尽量避免将Pod调度到具有该污点的Node上
- NoExecute：表示k8s将不会将Pod调度到具有该污点的Node上，同时会将Node上已经存在的Pod驱逐出去



给master01节点打一个污点，这样master01节点将不会调度运行Pod，即不运行工作负载

```shell
kubectl taint node k8s-master01 master-test=test:NoSchedule
```

查看k8s-master01节点的污点

```shell
kubectl describe node k8s-master01

...
Taints:             master-test=test:NoSchedule
                    node-role.kubernetes.io/master:NoSchedule
...
```

删除taint

```shell
#删除指定key所有的effect
kubectl taint node k8s-master01 master-test-
#删除指定key的指定effect
kubectl taint node node1 key1:NoExecute-
```

### Toleration容忍

要想pod可以调度到打了污点的节点，就要在pod配置容忍

在pod配置文件添加一个容忍

```yaml
spec:
  tolerations:
  - key: "master-test" #污点的key名称
    value: "test" #污点的value
    effect: "NoSchedule" #污点的effect
    operator: "Equal"
    
或
spec:
  tolerations:
  - key: "master-test"
    effect: "NoSchedule"
    operator: "Exists"

或
spec:
  tolerations:
  - key: "master-test"
    value: "test"
    effect: "NoExecute"
    operator: "Equal"
    tolerationSeconds: 60 #多少秒后驱逐pod
```

节点有多个taint的话，每个污点都要容忍才能部署