# DaemonSet

守护进程集，在所有节点或匹配的节点上运行一个pod，当有node加入集群时，也会为它们新增一个pod；当有node移除，这些pod也会被回收

场景：

- 运行集群存储daemon，比如ceph或glusterd；

- 节点的CNI网络插件，calico

- 每个节点运行日志收集daemon：fluentd或filebeat

- 节点的监控daemon：node exporter


## 创建DaemonSet

定义一个ds

```yaml
cat nginx-ds.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: nginx
  name: nginx
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: nginx
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx:1.15.2
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```

创建一个ds

```shell
kubectl create -f nginx-ds.yaml
```

查看pod部署的节点

```shell
kubectl get po -owide

#能看到master节点也部署了pod是因为master节点没有设置污点
```

给node打标签

```shell
kubectl label node k8s-node01 k8s-node02 ds=true
kubectl get node --show-labels
```

修改配置文件

```yaml
...
spec:
  nodeSelector:
    ds: "true"
  containers:
  - image: nginx:1.15.2
...
```

重新加载配置文件，可以看到master节点的pod被删除了

```shell
kubectl replace -f nginx-ds.yaml
```

给master03节点打上标签，master03就会开始部署pod

```shell
kubectl label node k8s-master03 ds=true
```

## 更新和回滚

**OnDelete** ： **建议使用ondelete**，默认升级策略，在创建好新的DaemonSet配置之后，新的Pod不会被自动创建，**用户需要手动删除旧版本的Pod**，才触发新建操作。

RollingUpdate： 旧版本的POD 将被自动杀掉，然后自动创建新版的DaemonSet Pod。**与Deployment 不同为不支持查看和管理DaemonSet的更新记录**

更新镜像：

```shell
kubectl set image ds nginx nginx=nginx:1.15.3 --record
```

回滚和deploy、sts一样

