# 性能分析

## 平均负载

平均负载：**单位时间内，系统处于可运行和不可中断的进程数**。它和我们传统意义上理解的CPU使用率并没有直接关系。

其中不可中断进程是正处于内核态关键流程中的进程（如常见的等待设备的I/O响应，也就是我们在ps命令中看到的D状态的状态）。**不可中断状态实际上是系统对进程和硬件设备的一种保护机制。**



实际生产环境中将系统的平均负载监控起来，根据历史数据判断负载的变化趋势。当负载存在**明显升高趋势时**，及时进行分析和调查。当然也可以当设置阈值（如当**平均负载高于CPU数量的70%时**）



现实工作中我们会经常混淆平均负载和CPU使用率的概念，其实两者并不完全对等：

- CPU密集型进程，大量CPU使用会导致平均负载升高，此时两者一致
- I/O密集型进程，等待I/O也会导致平均负载升高，此时CPU使用率并不一定高
- 大量等待CPU的进程调度会导致平均负载升高，此时CPU使用率也会比较高



负载高的时候可以使用 **mpstat、pidstat**等工具分析：

1、CPU 密集型进程

```shell
#-P ALL 表示监控所有 CPU，后面数字 5 表示间隔 5 秒后输出一组数据
[root@luoahong ~]# mpstat -P ALL 5
03:47:20 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
03:47:25 PM  all   25.29    0.00    0.05    0.05    0.00    0.05    0.00    0.00    0.00   74.55
03:47:25 PM    0   99.80    0.00    0.00    0.00    0.00    0.20    0.00    0.00    0.00    0.00
03:47:25 PM    1    0.00    0.00    0.20    0.20    0.00    0.00    0.00    0.00    0.00   99.59
03:47:25 PM    2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
03:47:25 PM    3    0.21    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   99.79

#正好有一个 CPU 的使用率为 100%，但其它的只有 0。这说明，平均负载的升高正是由于 CPU 使用率为 100% 。

#查看哪个进程导致了 CPU 使用率为 100%
[root@luoahong ~]# pidstat -u 5 1
Linux 3.10.0-693.el7.x86_64 (luoahong) 02/05/2019 _x86_64_  (4 CPU)
 
03:51:51 PM UID PID %usr %system %guest %wait %CPU CPU Command
03:51:56 PM 0 79 0.00 0.59 0.00 0.20 0.59 2 kworker/2:2
03:51:56 PM 0 309 0.00 0.20 0.00 0.00 0.20 0 xfsaild/sda2
03:51:56 PM 0 738 0.40 0.00 0.00 0.20 0.40 1 vmtoolsd
03:51:56 PM 0 1308 99.80 0.20 0.00 0.00 100.00 3 stress #stress 进程的 CPU 使用率为 99.80
03:51:56 PM 0 1501 0.20 0.20 0.00 0.00 0.40 0 watch
03:51:56 PM 0 1752 0.00 0.40 0.00 0.00 0.40 1 pidstat
 
Average: UID PID %usr %system %guest %wait %CPU CPU Command
Average: 0 79 0.00 0.59 0.00 0.20 0.59 - kworker/2:2
Average: 0 309 0.00 0.20 0.00 0.00 0.20 - xfsaild/sda2
Average: 0 738 0.40 0.00 0.00 0.20 0.40 - vmtoolsd
Average: 0 1308 99.80 0.20 0.00 0.00 100.00 - stress
Average: 0 1501 0.20 0.20 0.00 0.00 0.40 - watch
Average: 0 1752 0.00 0.40 0.00 0.00 0.40 - pidstat
```

2、I/O 密集型进程

```shell
[root@luoahong ~]# mpstat -P ALL 5 1
Linux 3.10.0-693.el7.x86_64 (luoahong)  02/05/2019  _x86_64_    (2 CPU)
 
08:58:00 PM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
08:58:05 PM  all    0.32    0.00   54.64   40.51    0.00    1.79    0.00    0.00    0.00    2.74
08:58:05 PM    0    0.43    0.00   27.55   66.59    0.00    3.47    0.00    0.00    0.00    1.95
08:58:05 PM    1    0.21    0.00   80.29   15.81    0.00    0.21    0.00    0.00    0.00    3.49
 
Average:     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
Average:     all    0.32    0.00   54.64   40.51    0.00    1.79    0.00    0.00    0.00    2.74
Average:       0    0.43    0.00   27.55   66.59    0.00    3.47    0.00    0.00    0.00    1.95
Average:       1    0.21    0.00   80.29   15.81    0.00    0.21    0.00    0.00    0.00    3.49

#其中一个 CPU 的系统 CPU 使用率升高到了 27.55，而 iowait 高达 66.59%。这说明，平均负载的升高是由于 iowait 的升高。
```

3、当系统中运行进程超出CPU运行能力时，就会出现等待CPU的进程

```shell
[root@luoahong ~]# pidstat -u 5 1
Linux 3.10.0-693.el7.x86_64 (luoahong)  02/05/2019  _x86_64_    (2 CPU)
 
09:15:30 PM   UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
09:15:35 PM     0       683    0.20    0.00    0.00    1.37    0.20     0  vmtoolsd
09:15:35 PM     0      1049    0.00    0.20    0.00    0.00    0.20     0  tuned
09:15:35 PM     0      4622    0.00    0.39    0.00    0.39    0.39     1  kworker/1:0
09:15:35 PM     0      4624    0.20    0.20    0.00    0.59    0.39     0  watch
09:15:35 PM     0      5271   24.31    0.00    0.00   74.31   24.31     1  stress
09:15:35 PM     0      5272   24.51    0.00    0.00   74.12   24.51     0  stress
09:15:35 PM     0      5273   24.31    0.00    0.00   73.92   24.31     1  stress
09:15:35 PM     0      5274   24.12    0.00    0.00   74.12   24.12     0  stress
09:15:35 PM     0      5275   24.31    0.00    0.00   74.12   24.31     1  stress
09:15:35 PM     0      5276   24.31    0.20    0.00   73.73   24.51     0  stress
09:15:35 PM     0      5277   24.31    0.20    0.00   74.31   24.51     1  stress
09:15:35 PM     0      5278   24.31    0.20    0.00   74.71   24.51     0  stress
09:15:35 PM     0      5326    0.00    0.20    0.00    0.39    0.20     0  pidstat
 
Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
Average:        0       683    0.20    0.00    0.00    1.37    0.20     -  vmtoolsd
Average:        0      1049    0.00    0.20    0.00    0.00    0.20     -  tuned
Average:        0      4622    0.00    0.39    0.00    0.39    0.39     -  kworker/1:0
Average:        0      4624    0.20    0.20    0.00    0.59    0.39     -  watch
Average:        0      5271   24.31    0.00    0.00   74.31   24.31     -  stress
Average:        0      5272   24.51    0.00    0.00   74.12   24.51     -  stress
Average:        0      5273   24.31    0.00    0.00   73.92   24.31     -  stress
Average:        0      5274   24.12    0.00    0.00   74.12   24.12     -  stress
Average:        0      5275   24.31    0.00    0.00   74.12   24.31     -  stress
Average:        0      5276   24.31    0.20    0.00   73.73   24.51     -  stress
Average:        0      5277   24.31    0.20    0.00   74.31   24.51     -  stress
Average:        0      5278   24.31    0.20    0.00   74.71   24.51     -  stress
Average:        0      5326    0.00    0.20    0.00    0.39    0.20     -  pidstat

#可以看出，8 个进程在争抢 2 个 CPU，每个进程等待CPU 的时间（也就是代码块中的 %wait 列）高达 75%这些超出 CPU 计算能力的进程，最终导致 CPU 过载。
```



## 上下文切换

**CPU上下文切换**，就是把前一个任务的CPU上下文（CPU寄存器和PC）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的位置，运行新任务。其中，保存下来的上下文会存储在系统内核中，待任务重新调度执行时再加载，保证原来的任务状态不受影响。

按照任务类型，CPU上下文切换分为：

- 进程上下文切换
- 线程上下文切换
- 中断上下文切换



## CPU使用率高

cpu使用率：cpu非空闲态下运行的时间占比，反应的是cpu的繁忙程度

CPU使用率可以通过top 或 ps来查看。分析进程的CPU问题可以通过 jstack（java应用）、perf



先用top找出导致cpu使用率高的进程

然后用perf分析进程哪个函数导致该问题

```shell
perf top -g -p XXXX #对某一个进程进行分析
```

![image-20220728100944151](assets/image-20220728100944151.png)

## 不可中断和僵尸进程

#### 进程状态

- R Running/Runnable，表示进程在CPU的就绪队列中，正在运行或者等待运行；
- D Disk Sleep，不可中断状态睡眠，一般表示进程正在跟硬件交互，并且交互过程中不允许被其他进程中断；
- Z Zombie，僵尸进程，表示进程实际上已经结束，但是父进程还没有回收它的资源；
- S Interruptible Sleep，可中断睡眠状态，表示进程因为等待某个事件而被系统挂起，当等待事件发生则会被唤醒并进入R状态；
- I Idle，空闲状态，用在不可中断睡眠的内核线程上。该状态不会导致平均负载升高；
- T Stop/Traced，表示进程处于暂停或跟踪状态（SIGSTOP/SIGCONT， GDB调试）；
- X Dead，进程已经消亡，不会在top/ps中看到。



对于不可中断状态，一般都是在很短时间内结束，可忽略。但是如果系统或硬件发生故障，进程可能会保持不可中断状态很久，甚至系统中出现大量不可中断状态，此时需注意是否出现了I/O性能问题。

僵尸进程一般多进程应用容易遇到，父进程来不及处理子进程状态时子进程就提前退出，此时子进程就变成了僵尸进程。大量的僵尸进程会用尽PID进程号，导致新进程无法建立。



#### 磁盘O_DIRECT问题

```
sudo docker run --privileged --name=app -itd feisky/app:iowait
ps aux | grep '/app'
```

可以看到此时有多个app进程运行，状态分别时Ss+和D+。其中后面s表示进程是一个会话的领导进程，+号表示前台进程组。

其中**进程组**表示一组相互关联的进程，子进程是父进程所在组的组员。**会话**指共享同一个控制终端的一个或多个进程组。

用top查看系统资源发现：1）平均负载在逐渐增加，且1分钟内平均负载达到了CPU个数，说明系统可能已经有了性能瓶颈；2）僵尸进程比较多且在不停增加；3）us和sys CPU使用率都不高，iowait却比较高；4）每个进程CPU使用率也不高，但有两个进程处于D状态，可能在等待IO。

分析目前数据可知：iowait过高导致系统平均负载升高，僵尸进程不断增长说明有程序没能正确清理子进程资源。

用dstat来分析，因为它可以同时查看CPU和I/O两种资源的使用情况，便于对比分析。

```
dstat 1 10    #间隔1秒输出10组数据
```

可以看到当wai（iowait）升高时磁盘请求read都会很大，说明iowait的升高和磁盘的读请求有关。接下来分析到底时哪个进程在读磁盘。

之前top查看的处于D状态的进程号，用pidstat -d -p XXX 展示进程的I/O统计数据。发现处于D状态的进程都没有任何读写操作。在用pidstat -d 查看所有进程的I/O统计数据，看到app进程在进行磁盘读操作，每秒读取32MB的数据。进程访问磁盘必须使用系统调用处于内核态，接下来重点就是找到app进程的系统调用。

```
sudo strace -p XXX #对app进程调用进行跟踪
```

报错没有权限，因为已经时root权限了。所以遇到这种情况，首先要检查进程状态是否正常。ps命令查找该进程已经处于Z状态，即僵尸进程。

这种情况下top pidstat之类的工具无法给出更多的信息，此时像第5篇一样，用perf record -d和perf report进行分析，查看app进程调用栈。

看到app确实在通过系统调用sys_read()读取数据，并且从new_sync_read和blkdev_direct_IO看出进程时进行直接读操作，请求直接从磁盘读，没有通过缓存导致iowait升高。

通过层层分析后，root cause是app内部进行了磁盘的直接I/O。然后定位到具体代码位置进行优化即可。

#### 僵尸进程

上述优化后iowait显著下降，但是僵尸进程数量仍旧在增加。首先要定位僵尸进程的父进程，通过pstree -aps XXX，打印出该僵尸进程的调用树，发现父进程就是app进程。

查看app代码，看看子进程结束的处理是否正确（是否调用wait()/waitpid(),有没有注册SIGCHILD信号的处理函数等）。

**碰到iowait升高时，先用dstat pidstat等工具确认是否存在磁盘I/O问题，再找是哪些进程导致I/O，不能用strace直接分析进程调用时可以通过perf工具分析。**

**对于僵尸问题，用pstree找到父进程，然后看源码检查子进程结束的处理逻辑即可。**

## 内存问题

查看内存

```shell
[root@localhost ~]# free
             total        used         free      shared   buffers   cached
Mem:         16402432    16360492      41940        0     465404   12714880
-/+ buffers/cache:        3180208   13222224
Swap:        8193108        264      8192844
```

首先是第一行：

1. total：物理内存的总大小。 
2. used：已经使用的物理内存多小。 
3. free：空闲的物理内存值。 
4. shared：多个进程共享的内存值。
5. buffers/cached：磁盘缓存的大小。 

1．**从内核的角度来查看内存的状态    total - used = free**

就是内核目前可以直接分配到，不需要额外的操作，即为上面free命令输出中第二行Mem项的值，可以看出，此系统物理内存有16G，空闲的内存只有41940K，也就是40M多一点，我们来做一个这样的计算：

16402432－16360492＝41940  

其实就是总的物理内存减去已经使用的物理内存得到的就是空闲的物理内存大小，注意这里的可用内存值41940并不包含处于buffers和cached状态的内存大小。

如果你认为这个系统空闲内存太小，那你就错了，实际上，内核完全控制着内存的使用情况，**linux会在需要内存的时候，或在系统运行逐步推进时，将buffers和cached状态的内存变为free状态的内存，以供系统使用。** 

2．**从应用层的角度来看系统内存的使用状态   free+buffer/cached**

也就是linux上运行的应用程序可以使用的内存大小，即free命令第三行“(-/+ buffers/cached)”的输出，可以看到，此系统已经使用的内存才3180208K，而空闲的内存达到13222224K，继续做这样一个计算：

41940＋（465404＋12714880）＝13222224

通过这个等式可知，应用程序可用的物理内存值是Mem项的free值加上buffers和cached值之和，也就是说，这个free值是包括buffers和cached项大小的，

对于应用程序来说，buffers/cached占有的内存是可用的，因为buffers/cached是为了提高文件读取的性能，当应用程序需要用到内存的时候，buffers/cached会很快地被回收，以供应用程序使用。

 buffer和cache也是真实的物理内存。

 通过内核角度去查看空闲内存，如果空闲内存较多，那很明显，这台机器是内存非常空闲的，但是如果从内核层面看到的free内存小的话，并不代表内存不足，**还要综合看buffer/cache，如果这个值也非常小，这样才能够说明内存确实不足了。**

通俗一点说：buffers主要用来存放目录里面有什么内容，文件的属性以及权限等等。而cached直接用来记忆我们打开过的文件和程序。

### 内存泄漏

虚拟内存分布从低到高分别是**只读段，数据段，堆，内存映射段，栈**五部分。其中会导致内存泄漏的是：

- 堆：由应用程序自己来分配和管理，除非程序退出这些堆内存不会被系统自动释放。
- 内存映射段：包括动态链接库和共享内存，其中共享内存由程序自动分配和管理

**内存泄漏的危害比较大，这些忘记释放的内存，不仅应用程序自己不能访问，系统也不能把它们再次分配给其他应用。** 内存泄漏不断累积甚至会耗尽系统内存.



检查内存泄漏：

看到free在不断下降，buffer和cache基本保持不变。说明系统的内存一致在升高。但并不能说明存在内存泄漏。此时可以通过memleak工具来跟踪系统或进程的内存分配/释放请求

```shell
/usr/share/bcc/tools/memleak -a -p PID
```

从memleak输出可以看到，应用在不停地分配内存，并且这些分配的地址并没有被回收。通过调用栈看到是什么函数分配的内存没有释放。定位到源码后查看源码来修复增加内存释放函数即可.