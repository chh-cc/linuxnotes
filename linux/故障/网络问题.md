## 问题一:网络不通,路由不正确

route -n发现0.0.0.0指向了eth0

分析:

重启网络服务,不行

停止网络服务,通过ifup eth1发现路由正常

然后ifup eth0发现路由异常,定位在启动eth0网卡出现异常

用 strace -f -e open ifup eth0|more 追踪一下,发现调用了 /etc/sysconfig/network 文件。  

打开 /etc/sysconfig/network 文件，发现多了一行 GATEWAYDEV=eth0。注释掉并重启网络服务.

## timewait和closewati

![image-20210715152038565](https://gitee.com/c_honghui/picture/raw/master/img/20210715152054.png)

TIME_WAIT 是 TCP 连接关闭过程中的一个状态，具体是这么形成的：  

- 主动关闭host1,发送fin,进入fin_wait_1状态,并等待

- 被动关闭host2,发送ack,进入close_wait状态,并等待

- host1收到ack,进入fin_wait_2状态,并等待

- host2发送fin,进入last_ack状态,并等待

- host1发送ack,进入time_wait状态,等待2msl后结束socket

- host2收到ack后结束socket

因此，TIME_WAIT 状态是出现在**主动发起连接关闭**的一点，和是谁发起的连接无关，可以是 client 端，也可以是 server 端。  

而 从 TIME_WAIT 状 态 到 CLOSED 状 态， 有 一 个 超 时 设 置， 这 个 超 时 设 置 是2*MSL（RFC793 定义了 MSL 为 2 分钟，Linux 设置成了 30s）。  

为什么需要 TIME_WAIT ？

1. 确保两端完全关闭

   ```text
   如果没有 TIME_WAIT状态，host1 服务器发出最后一个 ACK 就进入关闭状态，如果这个 ACK 对端没有
   收到，对端就不能完成关闭。对端没有收到 ACK，会重发 FIN，此时连接关闭，这个 FIN 也得不到 ACK，而有 TIME_WAIT，则会重发这个 ACK，确保对端能正常关闭连接
   ```

2. 确保后续连接不会收到脏数据

   ```TEXT
   MSL是指（maximum segment lifetime，我们内核一般是 30s，2MSL 就是 1 分钟），网络上数据包最大的生命周期。这是为了使网络上由于重传出现的 oldduplicate segment 都消失后，才能创建参数（四元组，源 IP/PORT，目标 IP/PORT）相同的连接，如果等待时间不够长，又创建好了一样的连接，再收到old duplicate segment，数据就错乱了。
   ```

timewait会导致什么问题?

1. 新建连接失败

   ```text
   每个连接在业务结束之后，需要 60s 的时间才能完全释放。如果业务上采用的是短连接的方式，会导致非常多的 TIME_WAIT 状态的连接，会占用一些资源，主要是本地端口资源。
   
   一台服务器的本地可用端口是有限的，也就几万个端口，由这个参数控制：
   net.ipv4.ip_local_port_range = 32768 61000
   
   当服务器存在非常多的 TIME_WAIT 连接，将本地端口都占用了，就不能主动发起新的连接去连其他服务器了。
   
   这里需要注意，是主动发起连接，又是主动发起关闭的一方才会遇到这个问题。
   ```

2. time_wait条目超出限制

   ```text
   这个限制，是由一个内核参数控制的：
   net.ipv4.tcp_max_tw_buckets = 5000
   
   超出了这个限制会报一条 INFO 级别的内核日志，然后继续关闭掉连接。并没有什么特别大的影响，只是增加了刚才提到的收到脏数据的风险而已。
   
   另外的风险就是，关闭掉 TIME_WAIT 连接后，刚刚发出的 ACK 如果对端没有收到，重发 FIN 包出来时，不能正确回复 ACK，只是回复一个 RST 包，导致对端程序报错，说 connection reset。
   
   建议增加这个值，仅仅是浪费一点点内存而已。
   ```

如何解决 time_wait? 

修改系统回收参数:

```text
net.ipv4.tcp_timestamps = 1
net.ipv4.tcp_tw_recycle = 1

设置该参数会带来什么问题？
如果这两个参数同时开启，会校验源 ip 过来的包携带的 timestamp 是否递增，
如果不是递增的话，则会导致三次握手建联不成功，具体表现为抓包的时候看到
syn 发出，server 端不响应 syn ack。
通俗一些来讲就是，一个局域网有多个客户端访问您，如果有客户端的时间比别
的客户端时间慢，就会建联不成功
```

关于 net.ipv4.tcp_max_tw_buckets 到底要不要放大，目前云上 ecs 多数是设置了5000，在很多场景下可能是不够的。  

如果 tw 满了会怎样
TCP: time wait bucket table overflow。  