# 性能调优

## 高流量大并发Linux TCP性能调优

通过修改 /etc/sysctl.conf 来达到调整的目的，注意修改完以后记得使用：`sysctl -p`使修改生效。

首先针对高并发需要提高linux的默认限制：

```shell
fs.file-max = 51200 #最大可以打开的文件描述符数量;在服务器中，我们知道每创建一个连接，系统就会打开一个文件描述符，所以，文件描述符打开的最大数量也决定了我们的最大连接数

net.ipv4.tcp_syncookies = 1 #在Tcp服务器收到Tcp Syn包并返回Tcp Syn+ack包时，不专门分配一个数据区，而是根据这个Syn包计算出一个cookie值。在收到Tcp ack包时，Tcp服务器在根据那个cookie值检查这个Tcp ack包的合法性。如果合法，再分配专门的数据区进行处理未来的TCP连接。默认为0，表示关闭；修改此参数可以有效的防范syn flood攻击

net.ipv4.tcp_tw_reuse = 1 #表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭；

net.ipv4.tcp_tw_recycle = 0 #快速回收处于TIME-WAIT状态的sockets，默认为0，表示关闭；为了对NAT设备更友好，建议设置为0。

net.ipv4.tcp_fin_timeout = 30 #修改time_wait状的存在时间，默认的2MSL。修改它有一定的风险，还是根据具体的情况来分析。
　　
net.ipv4.tcp_keepalive_time = 1200 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟Tcp keepalive心跳包机制，用于检测连接是否已断开
　　
net.ipv4.ip_local_port_range = 10000 65000 #表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为10000到65000。（注意：这里不要将最低值设的太低，否则可能会占用掉正常的端口！）
　　
net.ipv4.tcp_max_syn_backlog = 8192 #Tcp syn队列的最大长度，在进行系统调用connect时会发生Tcp的三次握手，server内核会为Tcp维护两个队列，Syn队列和Accept队列，Syn队列是指存放完成第一次握手的连接，Accept队列是存放完成整个Tcp三次握手的连接，修改该参数可以接收更多的连接
#注意此参数过大可能遭遇到Syn flood攻击，即对方发送多个Syn报文端填充满Syn队列，使server无法继续接受其他连接
　　
net.ipv4.tcp_max_tw_buckets = 5000 #表示允许存在TIME_WAIT的最大数量，如果超过这个数字，TIME_WAIT将立刻被清除并打印警告信息。

#额外的，对于内核版本新于**3.7.1**的，我们可以开启tcp_fastopen：
net.ipv4.tcp_fastopen = 3
```

解决Tcp长延时

根据Tcp的理论计算，Tcp最佳状态下传输是流水并行的，传输时间等于传输数据耗时+TTL，即千兆网卡的环境下:

传输1MB数据需要： 1000ms/100MB*1MB+TTL=10ms+TTL，同机房传输1MB耗时10毫秒，跨机房理论耗时14毫秒

传输4MB数据需要： 1000ms/100MB*4MB+TTL=40ms+TTL，同机房传输4MB需要耗时40毫秒，跨机房理论耗时44毫秒 

在我的生产环境，同机房的两个机器之间ping耗时0.15毫秒；两个机器之间读1MB数据和4MB的数据延时极度不稳定，在10毫秒~300毫秒之间波动。

另外一个跨机房使用了专线的环境，两台机器之间ping耗时4毫秒，但两个机器之间读1MB数据和4MB的数据延时也极度不稳定，在40毫秒~500毫秒之间波动。

这个现象看起来就像：网卡压力小时性能差，网卡压力大时性能反而好。

我们测试的延时高，是因为没有享受Tcp高速通道阶段甚至一直处于Tcp慢启动阶段。



我做了下面5步尝试，具体过程如下：



STEP1】 最开始的测试代码：

每次请求建立一个Tcp连接，读完4MB数据后关闭连接，测试的结果：平均延时174毫秒：每次都新建连接，都要经历慢启动阶段甚至还没享受高速阶段就结束了，所以延时高。



STEP2】 改进后的测试代码：

只建立一个Tcp连接，Client每隔10秒钟从Server读4MB数据，测试结果：平均延时102毫秒。

改进后延时还非常高，经过观察拥塞窗口发现每次读的时候拥塞窗口被重置，从一个较小值增加，tcp又从慢启动阶段开始了。



STEP3】改进后的测试代码+设置
net.ipv4.tcp_slow_start_after_idle=0：

只建立一个Tcp连接，Client每隔10秒钟从Server读4MB数据，测试结果：平均延时43毫秒。


net.ipv4.tcp_slow_start_after_idle设置为0，一个tcp连接在空闲后不进入slow start阶段，即每次收发数据都直接使用高速通道，平均延时43毫秒，跟计算的理论时间一致。



STEP4】我们线上的业务使用了Sofa-Rpc网络框架，这个网络框架复用了Socket连接，每个EndPoint只打开一个Tcp连接。

我使用Sofa-Rpc写了一个简单的测试代码，Client每隔10秒钟Rpc调用从Server读4MB数据，

即：Sofa-Rpc只建立一个Tcp连接+未设置
net.ipv4.tcp_slow_start_after_idle（默认为1），测试结果：延时高，跟理论耗时差距较大：transbuf配置为32KB时，平均延时93毫秒。



STEP5】

Sofa-Rpc只建立一个Tcp连接+设置
net.ipv4.tcp_slow_start_after_idle为0，测试结果： transbuf配置为1KB时，平均延时124毫秒；transbuf配置为32KB时，平均延时61毫秒；transbuf配置为4MB时，平均延时55毫秒

使用Sofa-Rpc网络框架，在默认1KB的transbuf时延时124毫秒，不符合预期；

使用Sofa-Rpc网络框架，配置为32KB的transbuf达到较理想的延时61毫秒。32KB跟Sofa-Rpc官方最新版本推荐的transbuf值一致。



结论：

延时高是由于Tcp传输没享受高速通道阶段造成的，

1】需要禁止Tcp空闲后慢启动 ：设置
net.ipv4.tcp_slow_start_after_idle = 0

2】尽量复用Tcp socket连接，保持一直处于高速通道阶段

3】我们使用的Sofa-Rpc网络框架，需要把Transbuf设置为32KB以上



另附linux-2.6.32.71内核对tcp idle的定义：



从内核代码153行可见在idle时间icsk_rto后需要执行tcp_cwnd_restart()进入慢启动阶段，

Icsk_rto赋值为TCP_TIMEOUT_INIT，其定义为

#define TCP_TIMEOUT_INIT ((unsigned)(3*HZ)) /* RFC 1122 initial RTO value */ 

