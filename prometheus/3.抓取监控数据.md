## 指标抓取

对于 Prometheus 中的指标抓取，采用的是 **Pull 模型**，默认是一分钟去拉取一次指标，通过 Prometheus.yaml 配置文件中的`scrape_interval` 配置项配置，Prometheus 对外都是用的 Pull 模型，一个是 Pull Exporter 的暴露的指标，一个是 Pull PushGateway 暴露的指标。

被监控端一般通过主动暴露metrics端口或通过exporter暴露指标，监控服务定期去抓取指标

## 服务注册

被监控服务在 Prometheus 中是一个 Job 存在，被监控服务的所有实例在 Prometheus 中是一个 target 的存在，所以被监控服务的注册就是在 Prometheus 中注册一个 Job 和其所有的 target，这个注册分为：**静态注册和动态注册**。

**静态注册**：静态的将服务的 IP 和抓取指标的端口号配置在 Prometheus yaml 文件的`scrape_configs`配置下:

```yaml
scrape_configs:
 - job_name: "prometheus"
   static_configs:
   - targets: ["localhost:9090"]
```

以上就是注册了一个名为 prometheus 的服务，这个服务下有一个实例，暴露的抓取地址是 localhost:9090。

**动态注册**：动态注册就是在 Prometheus yaml 文件的 scrape_configs 配置下配置服务发现的地址和服务名，Prometheus 会去该地址，根据你提供的服务名动态发现实例列表，在 Prometheus 中，支持 consul，DNS，文件，K8s 等多种服务发现机制。基于 consul 的服务发现：

```yaml
- job_name: “node_export_consul”
 metrics_path: /node_metrics
 scheme: http
 consul_sd_configs:
 - server: localhost:8500
  services:
    - node_exporter
```

我们 consul 的地址就是：localhost:8500，服务名是`node_exporter`，在这个服务下有一个exporter实例

> 注意：如果是动态注册，最好加上这两配置，静态注册指标拉取的路径会默认的帮我们指定为 `metrics_path:/metrics`，所以如果暴露的指标抓取路径不同或者是动态的服务注册，最好加上这两个配置。不然会报错“INVALID“ is not a valid start token，演示下，百度了一下，这里可能是数据格式不统一导致。

```yaml
metrics_path: /node_metrics
scheme: http
```

最后可以在 webUI 中查看发现的实例

目前，Prometheus 支持多达二十多种服务发现协议：

```

<azure_sd_config>
<consul_sd_config>
<digitalocean_sd_config>
<docker_sd_config>
<dockerswarm_sd_config>
<dns_sd_config>
<ec2_sd_config>
<openstack_sd_config>
<file_sd_config>
<gce_sd_config>
<hetzner_sd_config>
<http_sd_config>
<kubernetes_sd_config>
<kuma_sd_config>
<lightsail_sd_config>
<linode_sd_config>
<marathon_sd_config>
<nerve_sd_config>
<serverset_sd_config>
<triton_sd_config>
<eureka_sd_config>
<scaleway_sd_config>
<static_config>
```

## 配置更新

在更新完 Prometheus 的配置文件后，我们需要更新我们的配置到程序内存里，这里的更新方式有两种，第一种简单粗暴，就是重启 Prometheus，第二种是动态更新的方式。如何实现动态的更新 Prometheus 配置。

**第一步**：首先要保证启动 Prometheus 的时候带上启动参数：`--web.enable-lifecycle`

```
prometheus --config.file=/usr/local/etc/prometheus.yml --web.enable-lifecycle
```

**第二步**：去更新我们的 Prometheus 配置：

```
curl -v --request POST 'http://localhost:9090/-/reload'
```

**第三步**：更新完配置后，我们可以通过 Post 请求的方式，动态更新配置：

## 全局配置文件

```yaml
vim prometheus.yml
global:
	scrape_interval:     15s	#多久采集一次数据，默认1m
	evaluation_interval: 15s	#多久检测一次告警规则，默认1m
	scrape_timeout: 15s	#采集数据的超时时间，该值不能大于scrape_interval的值，默认10s。

#配置触发告警条件的文件（比如cpu超过多少发送告警）
rule_files:
    ...
    
#定义了 Prometheus 要抓取的目标，称为target，target的集合用job_name命名，又分静态配置和服务发现
scrape_configs:
    ...
    
#关于 Alertmanager 的配置
alerting:
  alert_relabel_configs: # - 警报重新标记在发送到Alertmanager之前应用于警报,用途是确保一对具有不同外部标签的Prometheus服务器发送相同的警报。
  alertmanagers:
  - static_configs:
    - targets: ["localhost:9093"] #关联alertmanager
    
    [ timeout: <duration> | default = 10s ]     # Per-target Alertmanager timeout when pushing alerts.
    [ api_version: <string> | default = v2 ]    # The api version of Alertmanager.
    [ path_prefix: <path> | default = / ]       # Prefix for the HTTP path alerts are pushed to.
    [ scheme: <scheme> | default = http ]       # Configures the protocol scheme used for requests.
```

## scrape_configs

```yaml
#采集对象的作业名称
- job_name: <job_name>

  #默认继承全局配置global，在这指定局部配置会覆盖全局配置
  [ scrape_interval: 15s ]
  [ scrape_timeout: 15s ]

  #指标路径
  [ metrics_path: /metrics ]
  #标签
  [ honor_labels: <boolean> | default = false ]

  #目标拉取协议
  [ scheme: <scheme> | default = http ]

  #参数
  params:
    [ <string>: [<string>, ...] ]

  #基础认证
  basic_auth:
    [ username: <string> ]
    [ password: <secret> ]
    [ password_file: <string> ]
  tls_config:
    [ <tls_config> ]
      [ ca_file: <filename> ]     # 用于验证API服务器证书的CA证书。
      [ cert_file: <filename> ]   # 用于向服务器进行客户端证书身份验证的证书和密钥文件。
      [ key_file: <filename> ]
      [ server_name: <string> ]   # 用于指示服务器的名称(https://tools.ietf.org/html/rfc4366#section-3.1)
      [ insecure_skip_verify: <boolean> ] # 禁用服务器证书验证。
  [ proxy_url: <string> ]

  #设置多种服务发现，最常见有以下两种方式
  ## 基于文件的服务发现提供了一种更通用的方法来配置静态目标，并充当插入自定义服务发现机制的接口(文件可以YAML或JSON格式提供,格式样例看下面的tips)
  file_sd_configs:
    - files:
      - my/path/tg_*.json
      [ refresh_interval: <duration> | default = 5m ] # 该静态文件刷新时间间隔
  # 基于kubernetes的服务发现,允许从 Kubernetes REST API 拉取集群pod相关信息并时刻保持同步。
  kubernetes_sd_configs:
    [ - <kubernetes_sd_config> ... ]
  ...

  #指定静态目标
  static_configs:
  - target: ['localhost:9090','localhost:9191']
    labels:
      [ <labelname>: <labelvalue> ... ]
        
  #重新标记  
  relabel_configs:
    ...
  
  metric_relabel_configs:
  ...
  
  [ sample_limit: <int> | default = 0 ] ## 每次刮取将被接受的刮取样品数量限制(0 means no limit)。
```

## 静态监控配置

```yaml
    - job_name: 'kubernetes-schedule'
      scrape_interval: 5s
      static_configs:
      - targets: ['192.168.40.180:10251'] #目标主机ip和端口
```

## 动态监控配置

Prometheus已经支持多种内置的服务发现机制：

- 发现云服务商的vm虚拟机
- k8s的自动发现
- 通过服务查找

### 基于文件的服务发现

创建一个用于服务发现的目标文件，在与prometheus.yml相同目录下创建一个targets.yml文件

```yaml
- targets: ['localhost:9090']
  labels:
    job: prometheus
- targets: ['localhost:9100']
  labels:
    instance: prometheus服务器
    job: node-exporter
- targets: ['192.168.101.141:8080']
  labels:
    instance: docker服务器
    job: cadvisor
- targets: ['localhost:8080']
  labels:
    instance: prometheus服务器
    job: cadvisor
```

springboot.yml

```yaml
- targets: ['192.168.101.141:8081']
```

配置prometheus.yml的抓取配置

```yaml
scrape_configs:
  - job_name: "file-sd"
    file_sd_configs:
    - refresh_interval: 10s
      files:
      - "targets/targets.yml"
      
# Spring Boot 2.x 应用数据采集
  - job_name: 'springboot-demo'
    metrics_path: '/actuator/prometheus'
    file_sd_configs:
    - refresh_interval: 10s
      files:
      - "targets/springboot.yml"
```

后续有新的监控配置只需要修改targets.yml文件，不需要重新加载prometheus配置文件



## relabel

### relabel机制

前面讲的几种服务发现机制，可以在不重启prometheus服务的情况下动态的发现需要监控的target实例信息

那么如何加label标签，如何从服务发现返回的target实例中有选择性的采集某些监控数据



**relabel机制：**

在采集样本数据之前，对target实例的标签（Metadata）进行重写，基本场景就是基于target实例中包含的metadata标签、动态的添加或者覆盖标签

**Metadata标签:**

在Prometheus所有的Target实例中，都包含一些默认的Metadata标签信息。可以通过Prometheus UI的Targets页面中查看这些实例的Metadata标签的内容：

![image-20220707224547619](assets/image-20220707224547619.png)

默认情况下，当prometheus加载target实例完成后，这些target都会包含一些默认的标签：

- **\__address__：当前Target实例的访问地址<host>:<port>**，如果在重新标记标签时未设置标签，则默认将\_address_标签值赋值给instance

- **\__scheme__：采集目标服务访问地址的HTTP Scheme，HTTP或者HTTPS**

- **\__metrics_path__：采集目标服务访问地址的访问路径**

**使用promql是查询不到这些标签的，因为这些标签只是普罗米修斯内部去使用的，不会存储在时序数据库供我们查询**



#### k8s服务发现的metadata标签

 k8s服务发现从以下的role方式进行目标发现

- node : 为每个群集节点发现一个目标，其地址默认为Kubelet的HTTP端口，地址类型顺序: NodeInternalIP, NodeExternalIP, NodeLegacyHostIP, and NodeHostName. 

  ```shell
  # Available meta labels: 此外instance节点的标签将设置为从API服务器检索到的节点名。
  __meta_kubernetes_node_name：节点对象的名称
  __meta_kubernetes_node_label_<labelname>：节点对象中的每个标签。
  __meta_kubernetes_node_labelpresent_<labelname> :是的对于节点对象中的每个标签。
  __meta_kubernetes_node_annotation_<annotationname>：来自节点对象的每个注释。
  __meta_kubernetes_node_annotationpresent_<annotationname> :是的对于节点对象中的每个注释。
  __meta_kubernetes_node_address_<address_type>：每个节点地址类型的第一个地址（如果存在）。 
  ```

- service : 为每个服务的每个服务端口发现一个目标。这对于服务的黑盒监视通常很有用。地址将设置为服务的Kubernetes DNS名称和相应的服务端口。 

  ```shell
  __meta_kubernetes_namespace：服务对象的命名空间。
  __meta_kubernetes_service_annotation_<annotationname>：来自服务对象的每个批注。
  __meta_kubernetes_service_annotationpresent_<annotationname>：“true”表示服务对象的每个注释。
  __meta_kubernetes_service_cluster_ip：服务的群集IP地址(不适用于ExternalName类型的服务）
  __meta_kubernetes_service_external_name：服务的DNS名称(适用于ExternalName类型的服务）
  __meta_kubernetes_service_label_<labelname>：来自服务对象的每个标签。
  __meta_kubernetes_service_labelpresent_<labelname> :是的对于服务对象的每个标签。
  __meta_kubernetes_service_name：服务对象的名称
  __meta_kubernetes_service_port_name：目标的服务端口的名称。
  __meta_kubernetes_service_port_protocol：目标的服务端口的协议。
  __meta_kubernetes_service_type：服务的类型 
  ```

- pod : 发现所有pod并将其容器作为目标公开。对于容器的每个声明端口，生成一个单独的目标。如果容器没有指定的端口，则为每个容器创建一个端口空闲目标，以便通过重新标记手动添加端口。 

  ```shell
  __meta_kubernetes_namespace：pod对象的命名空间。
  __meta_kubernetes_pod_name：pod对象的名称
  __meta_kubernetes_pod_ip：pod对象的pod IP。
  __meta_kubernetes_pod_label_<labelname>：pod对象中的每个标签。
  __meta_kubernetes_pod_labelpresent_<labelname> :是的对于pod对象中的每个标签。
  __meta_kubernetes_pod_annotation_<annotationname>：pod对象中的每个注释。
  __meta_kubernetes_pod_annotationpresent_<annotationname> :是的对于pod对象的每个注释。
  __meta_kubernetes_pod_container_init :是的如果容器是 初始化容器
  __meta_kubernetes_pod_container_name：目标地址指向的容器的名称。
  __meta_kubernetes_pod_container_port_name：容器端口的名称
  __meta_kubernetes_pod_container_port_number：集装箱端口号
  __meta_kubernetes_pod_container_port_protocol：集装箱港口的协议
  __meta_kubernetes_pod_ready：设置为是的或false吊舱准备就绪
  __meta_kubernetes_pod_phase：设置为悬而未决的 ,Running ,成功 ,Failed或未知在生命周期 .
  __meta_kubernetes_pod_node_name：pod被调度到的节点的名称。
  __meta_kubernetes_pod_host_ip：pod对象的当前主机IP。
  __meta_kubernetes_pod_uid：pod对象的UID。
  __meta_kubernetes_pod_controller_kind：pod控制器的对象类型。
  __meta_kubernetes_pod_controller_name：吊舱控制器的名称 
  ```

- endpoints : 从服务的列出的终结点发现目标。对于每个endpointaddress，每个端口都会发现一个目标。如果端点由一个pod支持，那么pod的所有附加容器端口（未绑定到端点端口）也会被发现作为目标。 

  ```shell
  _meta_kubernetes_namespace：endpoints对象的命名空间。
  __meta_kubernetes_endpoints_name：endpoints对象的名称。
  对于直接从端点列表中发现的所有目标（未从底层POD中额外推断出的目标），将附加以下标签：
  __meta_kubernetes_endpoint_hostname：终结点的主机名
  __meta_kubernetes_endpoint_node_name：承载终结点的节点的名称。
  __meta_kubernetes_endpoint_ready：设置为是的或false对于端点的就绪状态
  __meta_kubernetes_endpoint_port_name：终结点端口的名称
  __meta_kubernetes_endpoint_port_protocol：终结点端口的协议
  __meta_kubernetes_endpoint_address_target_kind：终结点地址目标的类型。
  __meta_kubernetes_endpoint_address_target_name：终结点地址目标的名称。
  如果端点属于服务，则role: service发现已附加
  对于由吊舱支持的所有目标role: pod发现已附加 
  ```

- ingress : 为每个入口的每个路径发现一个目标。这通常对黑盒监控入口很有用地址将设置为入口规范中指定的主机。 

  ```shell
  __meta_kubernetes_namespace：ingress对象的命名空间。
  __meta_kubernetes_ingress_name -入口对象的名称
  __meta_kubernetes_ingress_label_<labelname>：来自ingress对象的每个标签。
  __meta_kubernetes_ingress_labelpresent_<labelname> :是的对于ingress对象中的每个标签。
  __meta_kubernetes_ingress_annotation_<annotationname>：来自ingress对象的每个注释。
  __meta_kubernetes_ingress_annotationpresent_<annotationname> :是的对于ingress对象的每个注释。
  __meta_kubernetes_ingress_scheme：入口协议方案，https如果设置了TLSconfig。默认为http .
  __meta_kubernetes_ingress_path：从入口规范的路径默认为 / . 
  ```


### relabel_configs配置

relabel规则组成：

```yaml
relabel_configs:
  #源标签，对哪些源标签进行relabel
  [ source_labels: '[' <labelname> [, ...] ']' ]
  #分隔符，用于在连接源标签source_labels时分割它们
  [ separator: <string> | default = ; ]
  #目标标签，relabel后的标签名
  [ target_label: <labelname> ]
  #正则表达式，匹配源标签的值
  [ regex: <regex> | default = (.*) ]
  #通过分组替换后标签（target_label）对应的值
  [ replacement: <string> | default = $1 ]
  #执行的 relabeling 动作，可选值包括 replace、keep、drop、hashmod、labelmap、labeldrop 或者 labelkeep，默认值为 replace
  [ action: <relabel_action> | default = replace ]
```

action（根据正则表达式匹配执行的动作）：

- replace：默认，**替换标签值**，通过regex匹配source_label的值，使用replacement来引用表达式匹配的分组，分组使用$1,$2...引用（正则匹配，提取字段重新创建新标签，注意这里是创建新的标签）
- keep：满足regex的实例进行采集，把source_labels中没有匹配到regex的target实例丢掉，即**只采集匹配成功的实例**
- drop：满足regex的实例不采集，把source_labels中没有匹配到regex的target实例丢掉，即**只采集没有匹配成功的实例**
- labeldrop：匹配regex所有标签名称，对匹配到的实例标签进行删除
- labelkeep：匹配regex所有标签名称，对匹配到的实例标签进行保留
- labelmap：匹配regex所有标签名称，然后复制匹配标签的值进行分组，通过replacement分组引用($1,$2,...)替代



**替换标签值**

例1：

```yaml
    relabel_configs:
      - source_labels: [__meta_consul_service_address]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: 192.168.101.141:9115
```

```yaml
源数据：__meta_consul_service_address="192.168.101.141"
变成：target="192.168.101.141",instance="192.168.101.141",__address__="192.168.101.141:9115"
```

例2：

```yaml
    relabel_configs:
      - source_labels: ["__meta_consul_dc"]
        target_label: "dc"
```

```yaml
源数据：__meta_consul_dc="dc1"
变成：dc="dc1"
```

**保留或丢弃实例**

例1：

```yaml
    relabel_configs:
      - source_labels: [__meta_consul_tags]
        regex: .*exporter.*
        action: keep
        
源数据：__meta_consul_tags=",exporter," 会保留。如果action: drop就相反
源数据：__meta_consul_tags=",domain," 会丢弃。如果action: drop就相反
```

**标签集映射**

把源标签的值映射到一组新的标签中去

例1：

```yaml
    relabel_configs:
      - regex: __meta_consul_service_metadata_(.+)
        action: labelmap
       
源数据：__meta_consul_service_metadata_job="test"
变成： job="test"
```

**保留或删除标签**

例1：

```yaml
    relabel_configs:
      - regex: job
        action: labeldrop

源数据：probe_success{instance="192.168.101.129:9090", job="consul_blackbox_tcp"}
变成：probe_success{instance="192.168.101.129:9090"}
```

















例子：

```yaml
#把地址http://node-ip:10250/metrics替换成http://node-ip:9100/metrics
    - job_name: 'kubernetes-node'
      kubernetes_sd_configs: ##使用的是k8s的服务发现
      - role: node
      relabel_configs:
      - action: replace
        source_labels: [__address__] #要替换的源标签，匹配地址
        regex: '(.*):10250' #匹配带有10250端口的url
        target_label: __address__ #获取__address__对应的值
        replacement: '${1}:9100' #替换_address_的值，把匹配到的ip:10250的ip保留，端口替换成9100
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+) #匹配到下面正则表达式的标签会被保留,如果不做regex正则的话，默认只是会显示instance标签
```

![image-20230227180034869](assets/image-20230227180034869.png)

匹配到的所有标签：

```shell
__address__="192.168.40.180:10250"
__meta_kubernetes_node_address_Hostname="xianchaomaster1"
__meta_kubernetes_node_address_InternalIP="192.168.40.180"
__meta_kubernetes_node_annotation_kubeadm_alpha_kubernetes_io_cri_socket="/var/run/dockershim.sock"
__meta_kubernetes_node_annotation_node_alpha_kubernetes_io_ttl="0"
__meta_kubernetes_node_annotation_projectcalico_org_IPv4Address="192.168.40.180/24"
__meta_kubernetes_node_annotation_projectcalico_org_IPv4IPIPTunnelAddr="10.244.123.64"
__meta_kubernetes_node_annotation_volumes_kubernetes_io_controller_managed_attach_detach="true"
__meta_kubernetes_node_label_beta_kubernetes_io_arch="amd64"
__meta_kubernetes_node_label_beta_kubernetes_io_os="linux"
__meta_kubernetes_node_label_kubernetes_io_arch="amd64"
__meta_kubernetes_node_label_kubernetes_io_hostname="xianchaomaster1"
__meta_kubernetes_node_label_kubernetes_io_os="linux"
__meta_kubernetes_node_label_node_role_kubernetes_io_control_plane=""
__meta_kubernetes_node_label_node_role_kubernetes_io_master=""
__meta_kubernetes_node_name="xianchaomaster1"
__metrics_path__="/metrics"
__scheme__="http"
instance="xianchaomaster1"
job="kubernetes-node"
```



例子2：

```yaml
    - job_name: 'kubernetes-node-cadvisor' #抓取cAdvisor数据，是获取kubelet上/metrics/cadvisor接口数据来获取容器的资源使用情况
      kubernetes_sd_configs:
      - role: node
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - action: labelmap #把匹配到的标签保留
        regex: __meta_kubernetes_node_label_(.+) # 保留匹配到的具有__meta_kubernetes_node_label的标签
      - target_label: __address__ # 获取到的地址：__address__="192.168.40.180:10250"
        replacement: kubernetes.default.svc:443  # 把_address_的值替换成新的地址kubernetes.default.svc:443
      - source_labels: [_meta_kubernetes_node_name]
        regex: (.+) #把原始标签中__meta_kubernetes_node_name值匹配到
        target_label: __metrics_path__ #获取__metrics_path__对应的值
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor #把metrics替换成新的值api/v1/nodes/xianchaomaster1/proxy/metrics/cadvisor
        #${1}是__meta_kubernetes_node_name获取到的值
```

新的url就是https://kubernetes.default.svc:443/api/v1/nodes/xianchaomaster1/proxy/metrics/cadvisor



例子3:

```yaml
    - job_name: 'kubernetes-apiserver' 
      kubernetes_sd_configs:
      - role: endpoints # 使用k8s中的endpoint服务发现，采集apiserver 6443端口获取到的数据
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace,__meta_kubernetes_service_name,__meta_kubernetes_endpoint_port_name]
        action: keep # 采集满足条件的实例，其他实例不采集
        regex: default;kubernetes;https # 正则匹配到的默认空间下的service名字是kubernetes，协议是https的endpoint类型保留下来
```

![image-20230228103707056](assets/image-20230228103707056.png)



例子4:

```yaml
    - job_name: 'kubernetes-service-endpoints' 
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
        action: keep
        regex: true #只保留"prometheus.io/scrape: true"的标签，意思是说如果某个service具有prometheus.io/scrape = true annotation声明则抓取，nnotation本身也是键值结构，所以这里的源标签设置为键，而regex设置值true，当值匹配到regex设定的内容时则执行keep动作也就是保留，其余则丢弃。
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
        action: replace
        target_label: __scheme_
        regex: (https?) #重新设置scheme，匹配源标签__meta_kubernetes_service_annotation_prometheus_io_scheme也就是prometheus.io/scheme annotation，如果源标签的值匹配到regex，则把值替换为__scheme__对应的值。
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
```

应用中自定义暴露的指标，也许你暴露的API接口不是/metrics这个路径，那么你可以在这个POD对应的service中做一个"prometheus.io/path = /mymetrics"声明，上面的意思就是把你声明的这个路径赋值给__metrics_path__，其实就是让prometheus来获取自定义应用暴露的metrices的具体路径，不过这里写的要和service中做好约定，如果service中这样写prometheus.io/app-metricspath: '/metrics'那么你这里就要

```yaml
      - source_labels: [__address__,__meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace
        target_label: __address__
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        #暴露自定义的应用的端口，就是把地址和你在service中定义的"prometheus.io/port =<port>"声明做一个拼接，然后赋值给__address__，这样prometheus就能获取自定义应用的端口，然后通过这个端口再结合__metrics_path__来获取指标，如果__metrics_path__值不是默认的/metrics那么就要使用上面的标签替换来获取真正暴露的具体路径。
      - action: labelmap #保留下面匹配到的标签
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace #替换__meta_kubernetes_namespace变成kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_service_name]
        action: replace
        target_label: kubernetes_name
```
