## prometheus介绍

CNCF继k8s的第二个项目，支持多种exporter采集数据，还支持pushgateway进行上报，prometheus性能足够支持上万台规模的集群。



监控系统没有绝对的好坏，最重要是适合自己的公司团队，能够利用最小的成本解决问题，实在不知道怎么选可以参考如下：

物理机、硬件设备用zabbix；**docker、k8s推荐用Prometheus**

云服务器厂商自带监控系统，有的监控不全，也可以搭配zabbix和prometheus

## prometheus特点

1. 多维度数据模型

   prometheus采集的数据都是**时间序列数据**，时间序列数据由**metric度量指标**和**labels键值对**组成。

   metrics指定监控目标系统的测量特征（如：http_requests_total-接收http请求的总数），相同的metrics通过打上不同的labels标签形成多维度数据模型（如：所有包含度量名称为/api/tracks的http请求，打上method=POST的标签，形成具体的http请求）

2. 灵活的查询语言promql

   可以对采集的metrics指标进行加法，乘法，连接等操作；

3. 通过http pull的方式采集时序数据

4. 可以通过pushgateway把时间序列数据推送到prometheus server

5. 可以通过服务发现或静态配置来发现目标服务对象（targets）

6. 每个采样数据占3.5bytes左右，300万的时间序列，30s间隔，保留60天，消耗磁盘约200G（官方数据）

## promtheus组件

<img src="assets/image-20230224102912841.png" alt="image-20230224102912841" style="zoom:50%;" />

1. prometheus server定期从活跃目标主机（targets）**拉取**监控指标数据，可通过配置静态job或动态服务发现的方式被Prometheus server采集目标主机数据；或者从pushgateway拉取监控指标数据
2. prometheus把采集的数据**保存**在本地或云存储
3. 通过配置告警规则，把触发的**告警发送**到alertmanager
4. 通过配置告警接收方，alertmanager把告警发送到邮件、微信或钉钉等
5. Prometheus自带的web ui界面提供PromQL查询语言，可查询监控数据
6. Grafana可接入prometheus数据源，把监控数据以图形化形式展示出

## 相关概念

### 理解时间序列

安装好Prometheus后会暴露一个/metrics的http服务，通过配置，Prometheus就可以采集到/metrics里面所有监控样本数据：

```shell
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 25
```

### 样本

Prometheus会将采集到的监控样本数据以时间序列的方式保存在内存数据库中，并定期存储到硬盘上。

Prometheus会将所有采集到的样本数据以时间序列（time-series）的方式保存在内存数据库中，并且定时保存到硬盘上。

```shell
 ^
  │   . . . . . . . . . . . . . . . . .   . .   node_cpu{cpu="cpu0",mode="idle"}
  │     . . . . . . . . . . . . . . . . . . .   node_cpu{cpu="cpu0",mode="system"}
  │     . . . . . . . . . .   . . . . . . . .   node_load1{}
  │     . . . . . . . . . . . . . . . .   . .  
  v
    <------------------ 时间 ---------------->
```



在时间序列中的每一个点称为样本，**样本由以下三部分组成：**

指标（metrics）：指标名称和标签集合

时间戳：精确到毫秒

样本值

表示方式：

```text
<----------------------------metric-------------------------><--timestamp--><-value->
process_open_fds{instance="localhost:9090", job="prometheus"} @1434417560938 39
process_open_fds{instance="localhost:9090", job="prometheus"} @1434417561287 33
<--metric_name--><------------------label------------------->
```

### 四种指标类型

指标格式：

```text
<metric name>{<label name>=<label value>, ...}
```

为了方便用户理解不通监控指标之间的差异，定义了四种指标类型

#### counter

Counter是计数器类型：

1、Counter用于累计值，例如记录请求次数、任务完成数、错误发生次数。

2、**一直增加，不会减少**。

3、重启进程后，会被重置。



例如：

```shell
http_response_total{method="GET",endpoint="/api/tracks"}100
http_response_total{method="GET",endpoint="/api/tracks"}160
```



Counter类型数据可以让用户方便的了解事件产生的速率的变化，在PromQL内置的相关操作函数可以提供相应的分析，比如以HTTP应用请求量来进行说明：

1、通过rate()函数获取HTTP请求量的增长率

rate(http_requests_total[5m])

2、查询当前系统中，访问量前10的HTTP地址

topk(10, http_requests_total)

#### Gauge

Gauge是测量器类型：

1、Gauge是常规数值，例如温度变化、内存使用变化。

2、**可变大，可变小。**

3、重启进程后，会被重置



例如：

```shell
memory_usage_bytes{host="master-01"} 100
memory_usage_bytes{host="master-01"} 30
memory_usage_bytes{host="master-01"} 50
memory_usage_bytes{host="master-01"} 80
```

#### histogram

**有些情况下计算出来的平均值是不能反应出少部分的特殊情况**，比如用户的响应时间，这时候就可以用histogram，可以分别统计~=0.05秒的量有多少，0~0.05秒有多少，>2秒有多少，>10秒有多少



1. 对每个采样点进行统计（**并不是一段时间的统计**），打到各个桶(bucket)中（0.05秒以下多少，2秒以下多少，10秒以下多少）
2. 对每个采样点值累计和(sum) （请求总时间）
3. 对采样点的次数累计和(count) （请求的总次数）



度量指标名称: [basename]的柱状图, 上面三类的作用度量指标名称

1. [basename]_bucket{le=“上边界”}, 这个值为小于等于上边界的所有采样点数量
2. [basename]_sum
3. [basename]_count





![Histogram 采集整理数据过程实例](assets/20190704155237697.png)

```text
如上表，设置bucket=[1,5,10]，当实际采样数据如是采样点所示, Observe表示采样点落在该bucket中的数量，即落在[-,1]的样点数为2，即落在[1,5]的样点数为3，即落在[5,10]的样点数为1，write是得到的最终结果（histogram的最终结果bucket计数是向下包含的）：
[basename]_bucket{le=“1”} = 2
[basename]_bucket{le=“5”} =5
[basename]_bucket{le=“10”} =6
[basename]_bucket{le="+Inf"} = 6
[basename]_count =6
[basename]_sum =18.8378745
```



#### summary

用于**记录某些东西的平均大小**，可能是计算所需的时间或处理的文件大小，摘要显示两个相关的信息：**`count`（事件发生的次数）和 `sum`（所有事件的总大小）**

作用：

1. 在客户端对于**一段时间内**（默认是10分钟）的每个采样点进行统计，并形成分位图。（中位数的成绩为多少，9分位数的成绩为多少）
2. 统计班上所有同学的总成绩(sum)
3. 统计班上同学的考试总人数(count)

例如：

```text
prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.5"} 0.012352463
prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.9"} 0.014458005
prometheus_tsdb_wal_fsync_duration_seconds{quantile="0.99"} 0.017316173
prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002
prometheus_tsdb_wal_fsync_duration_seconds_count 216

#当前Prometheus Server进行wal_fsync操作的总次数为216次，耗时2.888716127000002s。其中中位数（quantile=0.5）的耗时为0.012352463，9分位数（quantile=0.9）的耗时为0.014458005s。
```

经验：

1. 如果需要聚合（aggregate），选择histograms。
2. 如果比较清楚要观测的指标的范围和分布情况，选择histograms。如果需要精确的分为数选择summary。

### job和instances

**每一个暴露监控样本数据的http服务都称为一个instance**，例如当前主机上运行的node exporter可以被称为一个实例instance，**在配置文件里面表示为target**；而具有**相同采集目的的实例集合称为job**



实例的状态：

除了通过“up”表达式查询当前所有instance的状态外，还可以通过Prometheus UI中的targets页面查看当前所有采集的job，以及各个job下的所有instance状态

## 监控流程

1. 在被监控主机安装xxx_exporter来收集数据
2. 添加prometheus配置，去收集xxx_exporter提供的监控样本数据
3. 配置触发器（告警规则）
4. grafana添加dashboard

## prometheus对k8s监控

对于Kubernetes而言，我们可以把当中所有的资源分为几类：

基础设施层（Node）：集群节点，为整个集群和应用提供运行时资源

容器基础设施（Container）：为应用提供运行时环境

用户应用（Pod）：Pod中会包含一组容器，它们一起工作，并且对外提供一个（或者一组）功能

内部服务负载均衡（Service）：在集群内，通过Service在集群暴露应用功能，集群内应用和应用之间访问时提供内部的负载均衡

外部访问入口（Ingress）：通过Ingress提供集群外的访问入口，从而可以使外部客户端能够访问到部署在Kubernetes集群内的服务



因此，如果要构建一个完整的监控体系，我们应该考虑，以下5个方面：

集群节点状态监控：从集群中各节点的kubelet服务获取节点的基本运行状态；

集群节点资源用量监控：通过Daemonset的形式在集群中各个节点部署Node Exporter采集节点的资源使用情况；

节点中运行的容器监控：通过各个节点中kubelet内置的cAdvisor中获取个节点中所有容器的运行状态和资源使用情况；

如果在集群中部署的应用程序本身内置了对Prometheus的监控支持，那么我们还应该找到相应的Pod实例，并从该Pod实例中获取其内部运行状态的监控指标。

对k8s本身的组件做监控：apiserver、scheduler、controller-manager、kubelet、kube-proxy